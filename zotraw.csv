"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"78NY5WR8","report","2009","Settles, Burr","Active Learning Literature Survey","","","","","https://minds.wisconsin.edu/handle/1793/60660","The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain.    This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.","2009","2022-09-17 13:55:35","2024-07-28 20:21:19","2022-09-17 13:55:35","","","","","","","","","","","","University of Wisconsin-Madison Department of Computer Sciences","","en","","Technical Report","","","minds.wisconsin.edu","","Accepted: 2012-03-15T17:23:56Z","","C:\Users\isido\Zotero\storage\XZAZ8M6G\Settles - 2009 - Active Learning Literature Survey.pdf; C:\Users\isido\Zotero\storage\A9QXPGLH\60660.html","","active learning; machine learning; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N5L3JNZM","preprint","2020","Baier, Lucas; Kellner, Vincent; Kühl, Niklas; Satzger, Gerhard","Switching Scheme: A Novel Approach for Handling Incremental Concept Drift in Real-World Data Sets","","","","","http://arxiv.org/abs/2011.02738","Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efﬁcient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily inﬂuenced by changing demand patterns over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results.","2020-11-05","2022-09-17 15:26:45","2024-07-28 20:21:29","2022-09-17 15:26:45","","","","","","","Switching Scheme","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2011.02738 [cs]","Comment: 54th Annual Hawaii International Conference on System Sciences (HICSS-54)","C:\Users\isido\Zotero\storage\4SZGPPN7\Baier e.a. - 2020 - Switching Scheme A Novel Approach for Handling In.pdf","","machine learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2011.02738","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3AK4YYB","preprint","2022","Mayaki, Mansour Zoubeirou A.; Riveill, Michel","Autoregressive based Drift Detection Method","","","","","http://arxiv.org/abs/2203.04769","In the classic machine learning framework, models are trained on historical data and used to predict future values. It is assumed that the data distribution does not change over time (stationarity). However, in real-world scenarios, the data generation process changes over time and the model has to adapt to the new incoming data. This phenomenon is known as concept drift and leads to a decrease in the predictive model’s performance. In this study, we propose a new concept drift detection method based on autoregressive models called ADDM. This method can be integrated into any machine learning algorithm from deep neural networks to simple linear regression model. Our results show that this new concept drift detection method outperforms the state-of-the-art drift detection methods, both on synthetic data sets and real-world data sets. Our approach is theoretically guaranteed as well as empirical and effective for the detection of various concept drifts. In addition to the drift detector, we proposed a new method of concept drift adaptation based on the severity of the drift.","2022-03-09","2022-09-17 15:33:30","2024-07-28 20:21:32","2022-09-17 15:33:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2203.04769 [cs, stat]","","C:\Users\isido\Zotero\storage\54YTNIWQ\Mayaki en Riveill - 2022 - Autoregressive based Drift Detection Method.pdf","","machine learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2203.04769","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XL4WKTRT","journalArticle","2009","Sabelfeld, K.; Mozartova, N.","Sparsified Randomization Algorithms for large systems of linear equations and a new version of the Random Walk on Boundary method","Monte Carlo Methods and Applications","","0929-9629, 1569-3961","10.1515/MCMA.2009.015","https://www.degruyter.com/document/doi/10.1515/MCMA.2009.015/html","Sparsiﬁed Randomization Monte Carlo (SRMC) algorithms for solving large systems of linear algebraic equations are presented. We construct efﬁcient stochastic algorithms based on a probabilistic sampling of small size sub-matrices, or a randomized evaluation of a matrix-vector product and matrix iterations via a random sparsiﬁcation of the matrix. This approach is beyond the standard Markov chain based Neumann–Ulam method which has no universal instrument to decrease the variance. Instead, in the new method, ﬁrst, the variance can be decreased by increasing the number of the sampled columns of the matrix in play, and second, it is free of the restricted assumption of the Neumann–Ulam scheme that the Neumann series converges. We apply the developed methods to different stochastic iterative procedures. Application to boundary integral equation of the electrostatic potential theory is given where we develop a SRMC algorithm for solving the approximated system of linear algebraic equations, and compare it with the standard Random Walk on Boundary method.","2009-01","2022-09-17 17:35:35","2024-07-28 20:30:28","2022-09-17 17:35:35","","","3","15","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>Inspiration for coupled splitting (method for fredholm equations)</p> </div>","C:\Users\isido\Zotero\storage\YBMLVR26\Sabelfeld en Mozartova - 2009 - Sparsified Randomization Algorithms for large syst.pdf","","monte carlo; linear systems; random linear algebra; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EMLGC6M2","journalArticle","2017","Benzi, Michele; Evans, Thomas M.; Hamilton, Steven P.; Lupo Pasini, Massimiliano; Slattery, Stuart R.","Analysis of Monte Carlo accelerated iterative methods for sparse linear systems","Numerical Linear Algebra with Applications","","1070-5325, 1099-1506","10.1002/nla.2088","https://onlinelibrary.wiley.com/doi/10.1002/nla.2088","We consider hybrid deterministic-stochastic iterative algorithms for the solution of large, sparse linear systems. Starting from a convergent splitting of the coeﬃcient matrix, we analyze various types of Monte Carlo acceleration schemes applied to the original preconditioned Richardson (stationary) iteration. These methods are expected to have considerable potential for resiliency to faults when implemented on massively parallel machines. We establish suﬃcient conditions for the convergence of the hybrid schemes, and we investigate diﬀerent types of preconditioners including sparse approximate inverses. Numerical experiments on linear systems arising from the discretization of partial diﬀerential equations are presented.","2017-05","2022-09-17 17:35:37","2024-07-28 20:24:15","2022-09-17 17:35:37","","","3","24","","Numer. Linear Algebra Appl.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\ZL2JBC57\Benzi e.a. - 2017 - Analysis of Monte Carlo accelerated iterative meth.pdf","","monte carlo; linear systems; random linear algebra; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LMTYA6IM","journalArticle","2019","Wu, Tao; Gleich, David F.","Multiway Monte Carlo Method for Linear Systems","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/18M121527X","https://epubs.siam.org/doi/10.1137/18M121527X","We study a novel variation on the Ulam–von Neumann Monte Carlo method for solving a linear system. This is an old randomized procedure that results from using a random walk to stochastically evaluate terms in the Neumann series. In order to apply this procedure, the variance of the stochastic estimator needs to be bounded. The best known suﬃcient condition for bounding the variance is that the inﬁnity norm of the matrix in the Neumann series is smaller than one, which greatly limits the usability of this method. We improve this condition by proposing a new stochastic estimator based on a diﬀerent type of random walk. Our multiway walk and estimator is based on a time-inhomogeneous Markov process that iterates through a sequence of transition matrices built from the original linear system. For our new method, we prove that a necessary and suﬃcient condition for convergence is that the spectral radius of the elementwise absolute value of the matrix underlying the Neumann series is smaller than one. This is a strictly weaker condition than currently exists. In addition, our new method is often faster than the standard algorithm. Through experiments, we demonstrate the potential for our method to reduce the time needed to solve linear equations by incorporating it into an outer iterative method.","2019-01","2022-09-17 17:35:39","2024-07-28 20:25:11","2022-09-17 17:35:39","A3449-A3475","","6","41","","SIAM J. Sci. Comput.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\UABEC4GQ\Wu en Gleich - 2019 - Multiway Monte Carlo Method for Linear Systems.pdf","","monte carlo; linear systems; random linear algebra; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9MBRHQS","journalArticle","2012","Ji, Hao; Li, Yaohang","Reusing Random Walks in Monte Carlo Methods for Linear Systems","Procedia Computer Science","","18770509","10.1016/j.procs.2012.04.041","https://linkinghub.elsevier.com/retrieve/pii/S1877050912001627","In this paper, we present an approach of reusing random walks in Monte Carlo methods for linear systems. The fundamental idea is, during the Monte Carlo sampling process, the random walks generated to estimate one unknown element can also be effectively reused to estimate the other unknowns in the solution vector. As a result, when the random walks are reused, a single random walk can contribute samples for estimations of multiple unknowns in the solution simultaneously while ensuring that the samples for the same unknown element are statistically independent. Consequently, the total number of random walk transition steps needed for estimating the overall solution vector is reduced, which improves the performance of the Monte Carlo algorithm. We apply this approach to the Monte Carlo algorithm in two linear algebra applications, including solving a system of linear equations and approximating the inversion of a matrix. Our computational results show that compared to the conventional implementations of Monte Carlo algorithms for linear systems without random walk reusing, our approach can significantly improve the performance of Monte Carlo sampling process by reducing the overall number of transition steps in random walks to obtain the entire solution within desired precision.","2012","2022-09-17 17:35:48","2024-07-28 20:26:14","2022-09-17 17:35:48","383-392","","","9","","Procedia Computer Science","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\G5YWGZPN\Ji en Li - 2012 - Reusing Random Walks in Monte Carlo Methods for Li.pdf","","monte carlo; linear systems; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QP5HLNL7","journalArticle","2016","Sabelfeld, Karl K.","Vector Monte Carlo stochastic matrix-based algorithms for large linear systems","Monte Carlo Methods and Applications","","0929-9629, 1569-3961","10.1515/mcma-2016-0112","https://www.degruyter.com/document/doi/10.1515/mcma-2016-0112/html","In this short article we suggest randomized scalable stochastic matrix-based algorithms for large linear systems. The idea behind these stochastic methods is a randomized vector representation of matrix iterations. In addition, to minimize the variance, it is suggested to use stochastic and double stochastic matrices for efficient randomized calculation of matrix iterations and a random gradient based search strategy. The iterations are performed by sampling random rows and columns only, thus avoiding not only matrix matrix but also matrix vector multiplications. Further improvements of the methods can be obtained through projections by a random gaussian matrix.","2016-01-01","2022-09-17 17:35:50","2024-07-28 20:30:55","2022-09-17 17:35:50","","","3","22","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\EX64VKZ2\Sabelfeld - 2016 - Vector Monte Carlo stochastic matrix-based algorit.pdf","","monte carlo; linear systems; random linear algebra; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7WBIRH9","bookSection","2012","Bender, Christian; Steiner, Jessica","Least-Squares Monte Carlo for Backward SDEs","Numerical Methods in Finance","978-3-642-25745-2 978-3-642-25746-9","","","http://link.springer.com/10.1007/978-3-642-25746-9_8","In this paper we ﬁrst give a review of the least-squares Monte Carlo approach for approximating the solution of backward stochastic diﬀerential equations (BSDEs) ﬁrst suggested by Gobet, Lemor, and Warin (Ann. Appl. Probab., 15, 2005, 2172–2202). We then propose the use of basis functions, which form a system of martingales, and explain how the least-squares Monte Carlo scheme can be simpliﬁed by exploiting the martingale property of the basis functions. We partially compare the convergence behavior of the original scheme and the scheme based on martingale basis functions, and provide several numerical examples related to option pricing problems under diﬀerent interest rates for borrowing and investing.","2012","2022-09-17 18:49:45","2024-08-08 13:35:13","2022-09-17 18:49:45","257-289","","","12","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Springer Proceedings in Mathematics DOI: 10.1007/978-3-642-25746-9_8","","C:\Users\isido\Zotero\storage\4LPW4G2Q\Bender en Steiner - 2012 - Least-Squares Monte Carlo for Backward SDEs.pdf","","monte carlo; ♥; BSDE","","Carmona, René A.; Del Moral, Pierre; Hu, Peng; Oudjane, Nadia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JZU8UXC","preprint","2015","Deaconu, Madalina; Herrmann, Samuel; Maire, Sylvain","The walk on moving spheres: a new tool for simulating Brownian motion's exit time from a domain","","","","","http://arxiv.org/abs/1401.3695","In this paper we introduce a new method for the simulation of the exit time and exit position of a δ-dimensional Brownian motion from a domain. The main interest of our method is that it avoids splitting time schemes as well as inversion of complicated series. The method, called walk on moving spheres algorithm, was ﬁrst introduced for hitting times of Bessel processes. In this study this method is adapted and developed for the ﬁrst time for the Brownian motion hitting times. The idea is to use the connexion between the δdimensional Bessel process and the δ-dimensional Brownian motion thanks to an explicit Bessel hitting time distribution associated with a particular curved boundary. This allows to build a fast and accurate numerical scheme for approximating the hitting time. We introduce also an overview of existing methods for the simulation of the Brownian hitting time and perform numerical comparisons with existing methods.","2015-10-16","2022-09-17 18:51:56","2024-07-28 20:30:36","2022-09-17 18:51:56","","","","","","","The walk on moving spheres","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1401.3695 [math]","","C:\Users\isido\Zotero\storage\EIYVRXS7\Deaconu e.a. - 2015 - The walk on moving spheres a new tool for simulat.pdf","","monte carlo; PDE; walk on spheres; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:1401.3695","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C95PBMD4","journalArticle","2022","Sawhney, Rohan; Seyb, Dario; Jarosz, Wojciech; Crane, Keenan","Grid-free Monte Carlo for PDEs with spatially varying coefficients","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3528223.3530134","https://dl.acm.org/doi/10.1145/3528223.3530134","Partial differential equations (PDEs) with spatially varying coefficients arise throughout science and engineering, modeling rich heterogeneous material behavior. Yet conventional PDE solvers struggle with the immense complexity found in nature, since they must first discretize the problem---leading to spatial aliasing, and global meshing/sampling that is costly and error-prone. We describe a method that approximates neither the domain geometry, the problem data, nor the solution space, providing the exact solution (in expectation) even for problems with extremely detailed geometry and intricate coefficients. Our main contribution is to extend the               walk on spheres (WoS)               algorithm from constant- to variable-coefficient problems, by drawing on techniques from volumetric rendering. In particular, an approach inspired by               null-scattering               yields unbiased Monte Carlo estimators for a large class of 2nd order elliptic PDEs, which share many attractive features with Monte Carlo rendering: no meshing, trivial parallelism, and the ability to evaluate the solution at any point without solving a global system of equations.","2022-07","2022-09-17 18:52:00","2024-07-28 20:30:12","2022-09-17 18:52:00","1-17","","4","41","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>this paper is the motivation for my master thesis</p> </div>","C:\Users\isido\Zotero\storage\52QDH9YB\Sawhney e.a. - 2022 - Grid-free Monte Carlo for PDEs with spatially vary.pdf","","monte carlo; PDE; walk on spheres; rendering; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTZ3JB83","preprint","2018","Ferguson, Ryan; Green, Andrew","Deeply Learning Derivatives","","","","","http://arxiv.org/abs/1809.02233","This paper uses deep learning to value derivatives. The approach is broadly applicable, and we use a call option on a basket of stocks as an example. We show that the deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. We develop a methodology to randomly generate appropriate training data and explore the impact of several parameters including layer width and depth, training data quality and quantity on model speed and accuracy.","2018-10-17","2022-09-19 13:22:35","2024-07-28 20:41:27","2022-09-19 13:22:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1809.02233 [cs, q-fin]","<div data-schema-version=""8""><p>interested in these type of applications</p> </div>","C:\Users\isido\Zotero\storage\5KLHN2WM\Ferguson en Green - 2018 - Deeply Learning Derivatives.pdf","","machine learning; deep learning; ♥♥; option pricing","Computer Science - Machine Learning; Quantitative Finance - Computational Finance","","","","","","","","","","","","","","","","","","","arXiv:1809.02233","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8CL8VZC","preprint","2018","Lehtinen, Jaakko; Munkberg, Jacob; Hasselgren, Jon; Laine, Samuli; Karras, Tero; Aittala, Miika; Aila, Timo","Noise2Noise: Learning Image Restoration without Clean Data","","","","","http://arxiv.org/abs/1803.04189","We apply basic statistical reasoning to signal reconstruction by machine learning – learning to map corrupted observations to clean signals – with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans – all corrupted by different processes – based on noisy data only.","2018-10-29","2022-09-19 14:04:13","2024-07-28 20:42:38","2022-09-19 14:04:13","","","","","","","Noise2Noise","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1803.04189 [cs, stat]","Comment: Added link to official implementation and updated MRI results to match it","C:\Users\isido\Zotero\storage\NAGQ9NY7\Lehtinen e.a. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf","","rendering; machine learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1803.04189","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTGTF5I3","webpage","2020","","Accelerating Python for Exotic Option Pricing","NVIDIA Technical Blog","","","","https://developer.nvidia.com/blog/accelerating-python-for-exotic-option-pricing/","In finance, computation efficiency can be directly converted to trading profits sometimes. Quants are facing the challenges of trading off research efficiency with computation efficiency.","2020-03-19","2022-09-19 14:08:08","2024-07-28 20:43:15","2022-09-19 14:08:08","","","","","","","","","","","","","","en-US","","","","","","","","","C:\Users\isido\Zotero\storage\LNJMET6I\accelerating-python-for-exotic-option-pricing.html","","♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLFA8VRN","webpage","2022","","GitHub - NVIDIA/fsi-samples: A collection of open-source GPU accelerated Python tools and examples for quantitative analyst tasks and leverages RAPIDS AI project, Numba, cuDF, and Dask.","","","","","https://github.com/NVIDIA/fsi-samples","","2022-09-19","2022-09-19 14:17:17","2024-08-12 14:54:28","2022-09-19 14:17:17","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\FTUN9SNE\fsi-samples.html","","♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQLA8IME","encyclopediaArticle","2022","","Neville's algorithm","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Neville%27s_algorithm&oldid=1100728778","In mathematics, Neville's algorithm is an algorithm used for polynomial interpolation that was derived by the mathematician Eric Harold Neville in 1934. Given n + 1 points, there is a unique polynomial of degree ≤ n which goes through the given points. Neville's algorithm evaluates this polynomial. Neville's algorithm is based on the Newton form of the interpolating polynomial and the recursion relation for the divided differences. It is similar to Aitken's algorithm (named after Alexander Aitken), which is nowadays not used.","2022-07-27","2022-09-26 06:48:39","2024-07-28 20:43:52","2022-09-26 06:48:39","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1100728778","","C:\Users\isido\Zotero\storage\J69UYIG8\Neville's_algorithm.html","","finite differnces; interpolation; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CT3DDRGG","preprint","2021","Vanroose, Wim; Cornelis, Jeffrey","Krylov-Simplex method that minimizes the residual in $\ell_1$-norm or $\ell_\infty$-norm","","","","","http://arxiv.org/abs/2101.11416","The paper presents two variants of a Krylov-Simplex iterative method that combines Krylov and simplex iterations to minimize the residual r = b−Ax. The ﬁrst method minimizes r ∞, i.e. maximum of the absolute residuals. The second minimizes r 1, and ﬁnds the solution with the least absolute residuals. Both methods search for an optimal solution xk in a Krylov subspace which results in a small linear programming problem. A specialized simplex algorithm solves this projected problem and ﬁnds the optimal linear combination of Krylov basis vectors to approximate the solution. The resulting simplex algorithm requires the solution of a series of small dense linear systems that only diﬀer by rank-one updates. The QR factorization of these matrices is updated each iteration. We demonstrate the eﬀectiveness of the methods with numerical experiments.","2021-01-27","2022-09-26 09:08:47","2024-07-28 20:44:24","2022-09-26 09:08:47","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2101.11416 [cs, math]","Comment: 22 pages","C:\Users\isido\Zotero\storage\M2G9WBDA\Vanroose en Cornelis - 2021 - Krylov-Simplex method that minimizes the residual .pdf","","optimization; krylov; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2101.11416","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A46LQ3HD","journalArticle","2007","Caglar, Hikmet; Caglar, Nazan","Solution of fifth order boundary value problems by using local polynomial regression","Applied Mathematics and Computation","","00963003","10.1016/j.amc.2006.08.046","https://linkinghub.elsevier.com/retrieve/pii/S0096300306010204","In this paper, we present a novel method based on the local polynomial regression for solving of ﬁfth order boundary value problems. The method is tested on numerical example to demonstrate its usefulness. The method presented in this paper is also compared with those developed by Siddiqi and Akram [Solution of ﬁfth order boundary value problems using nonpolynomial spline technique, Appl. Math. Comput. 175 (2006) 1575–1581], as well and is observed to be better.","2007-03","2022-09-26 12:57:28","2024-07-28 20:44:39","2022-09-26 12:57:28","952-956","","2","186","","Applied Mathematics and Computation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\Z8EJ5CDR\Caglar en Caglar - 2007 - Solution of fifth order boundary value problems by.pdf","","ODE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZZXAYPQ","journalArticle","2012","Su, Liyun; Yan, Tianshun; Zhao, Yanyong; Li, Fenglan; Liu, Ruihua","Numerical Solution of Integro-Differential Equations with Local Polynomial Regression","Open Journal of Statistics","","2161-718X, 2161-7198","10.4236/ojs.2012.23043","http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ojs.2012.23043","In recent years, there has been a growing interest in the Integro-Differential Equations (IDEs) which are a combination of differential and Fredholm-Volterra integral equations. IDEs play an important role in many branches of linear and nonlinear functional analysis and their applications in the theory of engineering, mechanics, physics, chemistry, astronomy, biology, economics, potential theory and electrostatics. The mentioned integro-differential equations are usually difficult to solve analytically, so a numerical method is required. Many different methods are used to obtain the solution of the linear and nonlinear IDEs such as the successive approximations, A domain decomposition, Homotopy perturbation method, Chebyshev and Taylor collocation, Haar Wavelet, Tau and Walsh series methods [1-8]. Recently, the authors [9], have used local polynomial regression (LPR) method for the numerical solution of linear and non-linear Fredholm and Volterra integral equations. In this paper, we consider the linear IDEs,     x a yx pxyx gx K     (1) where the upper limit of the integral is constant or variable,    are constants, g xpx  and the kernel     K xt y  are given functions, whereas x 0 needs to be determined. The subject of this paper is to try to find numerical solutions of integro-differential equations by means of local polynomial regression method which is presented firstly by Hikmat Caglar [9]. Finally, we show the method to achieve the desired accuracy. Details of the structure of the present method are explained in sections. We apply LPR method for IDEs. In Section 3, it’s proved the efficiency of numerical method. Finally, Section 4 contains some conclusions and directions for future expectations and researches.","2012","2022-09-26 13:07:47","2024-07-28 20:44:48","2022-09-26 13:07:47","352-355","","03","02","","OJS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\MDM22BGE\Su e.a. - 2012 - Numerical Solution of Integro-Differential Equatio.pdf","","ODE; integral equations; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPP5ULJB","journalArticle","1966","Haji-Sheikh, A.; Sparrow, E. M.","The Floating Random Walk and Its Application to Monte Carlo Solutions of Heat Equations","SIAM Journal on Applied Mathematics","","","","http://www.jstor.org/stable/2946271","Introduction. As longago as the turnof thecentury, it hadbeen recognizedthatprobabilitysamplingtechniquescould,in principle, be employed in solvingpartialdifferential equationsarisingin physicsand engineering. In 1899,LordRayleigh[1]demonstratet dherelationshipbetweenstochastic processesand parabolicdifferential equations,whilea similarrelationship was establishedin 1928by Courantand hiscoworkers[2]forellipticdifferentialequations.The earliestknowncomputationalexperiments involving samplingtechniquesare due to Todd[3],whoworkedwithLaplace's equationin asquaregeometryhavingprescribedboundarytemperatures. Somewhat more recently,Ehrlich [4] used samplingtechniquesto solve the equation a2u +au kau _+ - + -- = 0 ax2 ay2 y ay in simplepolygonswithprescribedboundaryvalues ofu. In general,there have been relativelyfew suchcomputationalexperiments. The name ""Monte Carlo"" is characteristically used to describeprobabilitysamplingtechniquesthat approximatethe solutionofmathematical or physicalproblems. A valuable surveyofMonte Carlomethodsas applied to the solutionof differential and difference equationshas been provided by Curtiss[5]. A primereferencesourceforthe randomwalk technique, which is basic to Monte Carlo methods,is Feller's text on probability theory[6]","1966","2022-09-27 19:05:10","2024-07-28 20:44:54","","370-389","","2","14","","","","","","","","","","en","","","","","Zotero","","","<div data-schema-version=""8""><p><img src=""https://yt3.ggpht.com/ytc/AMLnZu83ZEk4KYdaQ1rTQAu1i_B7_Fc7ROEYO-7w1Q=s48-c-k-c0x00ffffff-no-rj"" alt=""Kram1032"" data-attachment-key=""65I69TKU"" width=""40"" height=""40""></p> <h3><a href=""https://www.youtube.com/channel/UCFpAdN2JtiAPl5rXGla8j2w"" rel=""noopener noreferrer nofollow"">Kram1032</a></h3> <p><a href=""https://www.youtube.com/watch?v=dXROl0KGPXc&amp;lc=Ugw3ALptclBG2_SKp5J4AaABAg"" rel=""noopener noreferrer nofollow"">1 month ago</a></p> <p>Love to see this progress! I wonder whether this could be done in the time domain as well? I know, it tends to be the case with Monte-Carlo methods, that they are timeless. They try to sample some steady state, right? But if you make the entire spacio-temporal volume your ""steady state"", it ought to be possible regardless. Normally you couldn't properly save, and keep access to all that data so you could completely skip out on frames. But perhaps there is a way around that somehow? Maybe something inspired by NeRFs. A sort of data structure that's iteratively updated with gradient descend anyways, so you could probably reformulate that as a PDE that can be handled with a method like this I'd think? Then you could effectively get rid of all explicit grids (globally spatial, locally surface- or volume-spatial, and temporal) and go gridless continuous end to end perhaps. It seems to me the application of refining subsurface scattering is a nice example of how that might be helpful.</p> <p><img src=""https://yt3.ggpht.com/ytc/AMLnZu82dmfedMJ9Lv-6GvfhqhTdOiDxuraxI8a75DBx=s48-c-k-c0x00ffffff-no-rj"" alt=""Rohan Sawhney"" data-attachment-key=""KQZWEPCW"" width=""24"" height=""24""></p> <p>·</p> <p><img src=""https://yt3.ggpht.com/ytc/AMLnZu82dmfedMJ9Lv-6GvfhqhTdOiDxuraxI8a75DBx=s48-c-k-c0x00ffffff-no-rj"" alt=""Rohan Sawhney"" data-attachment-key=""L868C8AZ"" width=""40"" height=""40""></p> <h3></h3> <p><a href=""https://www.youtube.com/channel/UC1Gk7ZLF1D4oM76gYeTqD4g"" rel=""noopener noreferrer nofollow"">Rohan Sawhney</a></p> <p><a href=""https://www.youtube.com/watch?v=dXROl0KGPXc&amp;lc=Ugw3ALptclBG2_SKp5J4AaABAg.9eRF4DoVe2w9ed5Evh-8p2"" rel=""noopener noreferrer nofollow"">1 month ago</a></p> <p>Hi! It's possible to handle temporal problems such as the heat equation with walk on spheres, I recommend looking at this paper: <a href=""https://www.youtube.com/redirect?event=comments&amp;redir_token=QUFFLUhqbjQ5aUVZTGtDTTN1cV9KMmVJVlFwVXVvZXh4d3xBQ3Jtc0tsRGU3OU1kcFlPSkZxWnYwLVc0cldRWmc0Q2R4eGZCdHNWVUI4VFZRd0w4NWlrMmE3dGpGMWZocVlnd0dMSnpYR1NrdmM2UWQ3WWl4OVlDR0NtSWtCY21pOENtWDlib3QyWVdFWUhWOU1KOXZlcEdodw&amp;q=https%3A%2F%2Fepubs.siam.org%2Fdoi%2F10.1137%2F0114031&amp;stzid=Ugw3ALptclBG2_SKp5J4AaABAg.9eRF4DoVe2w9ed5Evh-8p2"" rel=""noopener noreferrer nofollow"">https://epubs.siam.org/doi/10.1137/0114031</a> Since the heat equation is an initial value problem, the high-level idea is to also sample an ""exit time"" at each step of the random walk from a known distribution (in addition to a uniform exit location on the sphere), and to keep a counter of the total time elapsed during the walk. If the counter value exceeds the predetermined amount/time for which one wants to flow the heat, then the initial value at the current position of the random walk inside the domain is added to the Monte Carlo estimate.</p> <p> 1 </p> <p><img src=""https://yt3.ggpht.com/ytc/AMLnZu83ZEk4KYdaQ1rTQAu1i_B7_Fc7ROEYO-7w1Q=s48-c-k-c0x00ffffff-no-rj"" alt=""Kram1032"" data-attachment-key=""XLJNAPHA"" width=""40"" height=""40""></p> <h3><a href=""https://www.youtube.com/channel/UCFpAdN2JtiAPl5rXGla8j2w"" rel=""noopener noreferrer nofollow"">Kram1032</a></h3> <p><a href=""https://www.youtube.com/watch?v=dXROl0KGPXc&amp;lc=Ugw3ALptclBG2_SKp5J4AaABAg.9eRF4DoVe2w9ed81sb6x8g"" rel=""noopener noreferrer nofollow"">1 month ago (edited)</a></p> <p><a href=""https://www.youtube.com/channel/UC1Gk7ZLF1D4oM76gYeTqD4g"" rel=""noopener noreferrer nofollow"">&nbsp;@Rohan Sawhney&nbsp;</a> thanks I'll check it out! sounds interesting</p> </div>","C:\Users\isido\Zotero\storage\M4YQIH62\Haji-Sheikh en Sparrow - 1966 - The Floating Random Walk and Its Application to Mo.pdf","","monte carlo; PDE; walk on spheres; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYW4WWLU","book","2001","Hastie, Trevor; Friedman, Jerome; Tibshirani, Robert","The Elements of Statistical Learning  Data Mining, Inference, and Prediction","","","","","","The many topics include neural networks, support vector machines, classification trees and boosting - the first comprehensive treatment of this topic in any book machine learning statistical learning","2001","2022-09-27 19:42:11","2024-07-28 20:46:25","","","","","","","","","","","","","Springer Berlin Heidelberg","","","","","","","","","","<div data-schema-version=""8""><p>started my passion for machine learning</p> <p>a long time ago</p> </div>","C:\Users\isido\Zotero\storage\Q8DWLFW6\ESLII_print12_toc-compressed.pdf","","machine learning; statistics; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZWD66Q9A","journalArticle","2019","Cockayne, Jon; Oates, Chris; Sullivan, Tim; Girolami, Mark","Bayesian Probabilistic Numerical Methods","SIAM Review","","0036-1445, 1095-7200","10.1137/17M1139357","http://arxiv.org/abs/1702.03673","The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.","2019-01","2022-09-30 13:49:16","2024-07-28 20:48:33","2022-09-30 13:49:16","756-789","","3","61","","SIAM Rev.","","","","","","","","en","","","","","arXiv.org","","arXiv:1702.03673 [cs, math, stat]","<div data-schema-version=""8""><p>Received from professor In’t Hout on 30/09/2022.</p> </div>","C:\Users\isido\Zotero\storage\9ST6SIRW\Cockayne e.a. - 2019 - Bayesian Probabilistic Numerical Methods.pdf","","statistics; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KUI5SU4X","journalArticle","2012","Gondzio, Jacek","Interior point methods 25 years later","European Journal of Operational Research","","03772217","10.1016/j.ejor.2011.09.017","https://linkinghub.elsevier.com/retrieve/pii/S0377221711008204","Interior point methods for optimization have been around for more than 25 years now. Their presence has shaken up the ﬁeld of optimization. Interior point methods for linear and (convex) quadratic programming display several features which make them particularly attractive for very large scale optimization. Among the most impressive of them are their low-degree polynomial worst-case complexity and an unrivalled ability to deliver optimal solutions in an almost constant number of iterations which depends very little, if at all, on the problem dimension. Interior point methods are competitive when dealing with small problems of dimensions below one million constraints and variables and are beyond competition when applied to large problems of dimensions going into millions of constraints and variables.","2012-05","2022-10-27 11:24:12","2024-07-28 20:48:36","2022-10-27 11:24:12","587-601","","3","218","","European Journal of Operational Research","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\NNLGF4AU\Gondzio - 2012 - Interior point methods 25 years later.pdf","","interior point methods; optimization; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H4M3QKNQ","preprint","2021","Kitapbayev, Yerkin","Closed form optimal exercise boundary of the American put option","","","","","http://arxiv.org/abs/1912.05438","We present three models of stock price with time-dependent interest rate, dividend yield, and volatility, respectively, that allow for explicit forms of the optimal exercise boundary of the finite maturity American put option. The optimal exercise boundary satisfies the nonlinear integral equation of Volterra type. We choose time-dependent parameters of the model so that the integral equation for the exercise boundary can be solved in the closed form. We also define the contracts of put type with time-dependent strike price that support the explicit optimal exercise boundary.","2021-01-09","2022-12-03 08:44:33","2024-07-28 20:49:08","2022-12-03 08:44:33","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1912.05438 [q-fin]","","C:\Users\isido\Zotero\storage\99M3PKZV\Kitapbayev - 2021 - Closed form optimal exercise boundary of the Ameri.pdf","","american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1912.05438","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52YIE38K","journalArticle","2021","Healy, Jherek","Pricing American options under negative rates","The Journal of Computational Finance","","14601559, 17552850","10.21314/JCF.2021.004","http://arxiv.org/abs/2109.15157","This paper starts by deﬁning the criteria where the early-exercise of an American option is never optimal, under positive, or negative rates. It follows with a short analysis of the various shapes of the exercise region under negative interest rates. It then presents a new integral equation, which establishes the option price, and the two early exercise boundaries, under negative rates. It shows how to solve this new equation, through modiﬁcations of the modern and efﬁcient algorithm of Andersen and Lake, from the initial guess of the two boundaries to more subtle changes required in their ﬁxed point method for stability. Finally, the performance and accuracy of the resulting algorithm is assessed against a cutting edge ﬁnite difference method implementation.","2021","2022-12-03 09:21:35","2024-07-28 20:49:12","2022-12-03 09:21:35","","","","","","JCF","","","","","","","","en","","","","","arXiv.org","","arXiv:2109.15157 [q-fin]","","C:\Users\isido\Zotero\storage\8RBI4CZR\Healy - 2021 - Pricing American options under negative rates.pdf","","american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VT99V4NF","journalArticle","2010","Bossy, Mireille; Baude, Françoise; Doan, Viet Dung; Gaikwad, Abhijeet; Stokes-Rees, Ian","Parallel Pricing Algorithms for Multi--Dimensional Bermudan/American Options using Monte Carlo methods","Mathematics and Computers in Simulation","","03784754","10.1016/j.matcom.2010.08.005","http://arxiv.org/abs/0805.1827","In this paper we present two parallel Monte Carlo based algorithms for pricing multi–dimensional Bermudan/American options. First approach relies on computation of the optimal exercise boundary while the second relies on classiﬁcation of continuation and exercise values. We also evaluate the performance of both the algorithms in a desktop grid environment. We show the eﬀectiveness of the proposed approaches in a heterogeneous computing environment, and identify scalability constraints due to the algorithmic structure.","2010-11","2022-12-03 09:36:45","2024-07-28 20:49:19","2022-12-03 09:36:45","568-577","","3","81","","Mathematics and Computers in Simulation","","","","","","","","en","","","","","arXiv.org","","arXiv:0805.1827 [cs]","","C:\Users\isido\Zotero\storage\R7AAXVAF\Bossy e.a. - 2010 - Parallel Pricing Algorithms for Multi--Dimensional.pdf","","monte carlo; american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EF9KMM7N","preprint","2007","Sevcovic, Daniel","An iterative algorithm for evaluating approximations to the optimal exercise boundary for a nonlinear Black-Scholes equation","","","","","http://arxiv.org/abs/0710.5301","The purpose of this paper is to analyze and compute the early exercise boundary for a class of nonlinear Black–Scholes equations with a nonlinear volatility which can be a function of the second derivative of the option price itself. A motivation for studying the nonlinear Black–Scholes equation with a nonlinear volatility arises from option pricing models taking into account e.g. nontrivial transaction costs, investor’s preferences, feedback and illiquid markets effects and risk from a volatile (unprotected) portfolio. We present a new method how to transform the free boundary problem for the early exercise boundary position into a solution of a time depending nonlinear parabolic equation deﬁned on a ﬁxed domain. We furthermore propose an iterative numerical scheme that can be used to ﬁnd an approximation of the free boundary. We present results of numerical approximation of the early exercise boundary for various types of nonlinear Black–Scholes equations and we discuss dependence of the free boundary on various model parameters.","2007-10-28","2022-12-03 09:39:03","2024-07-28 20:49:36","2022-12-03 09:39:03","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:0710.5301 [math, q-fin]","Comment: 17 pages","C:\Users\isido\Zotero\storage\QEPCMV5C\Sevcovic - 2007 - An iterative algorithm for evaluating approximatio.pdf","","♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:0710.5301","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TGBVP9JJ","preprint","1998","Sorge, H.","Valuation of path-dependent American options using a Monte Carlo approach","","","","","http://arxiv.org/abs/math/9801057","It is shown how to obtain accurate values for American options using Monte Carlo simulation. The main feature of the novel algorithm consists of tracking the boundary between exercise and hold regions via optimization of a certain payoﬀ function. We compare estimates from simulation for some types of claims with results from binomial tree calculations and ﬁnd very good agreement. The novel method allows to calculate so far untractable path-dependent option values.","1998-01-12","2022-12-03 09:44:48","2024-07-28 20:49:54","2022-12-03 09:44:48","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:math/9801057","Comment: 32 pages LaTeX including 4 postscript figures","C:\Users\isido\Zotero\storage\TEBUQCAP\Sorge - 1998 - Valuation of path-dependent American options using.pdf","","american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:math/9801057","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQR3WI22","preprint","2021","Hout, Karel in 't; Snoeijer, Jacob","Numerical valuation of American basket options via partial differential complementarity problems","","","","","http://arxiv.org/abs/2106.01200","We study the principal component analysis based approach introduced by Reisinger & Wittum [1] and the comonotonic approach considered by Hanbali & Linders [2] for the approximation of American basket option values via multidimensional partial diﬀerential complementarity problems (PDCPs). Both approximation approaches require the solution of just a limited number of low-dimensional PDCPs. It is demonstrated by ample numerical experiments that they deﬁne approximations that lie close to each other. Next, an eﬃcient discretisation of the pertinent PDCPs is presented that leads to a favourable convergence behaviour.","2021-06-02","2022-12-03 10:47:53","2024-07-28 20:50:54","2022-12-03 10:47:53","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.01200 [cs, math, q-fin]","","C:\Users\isido\Zotero\storage\VYEP4C9L\Hout en Snoeijer - 2021 - Numerical valuation of American basket options via.pdf","","american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2106.01200","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LNZX599X","encyclopediaArticle","2022","","Free boundary problem","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Free_boundary_problem&oldid=1117659232","In mathematics, a free boundary problem (FB problem) is a partial differential equation to be solved for both an unknown function                         u                 {\displaystyle u}    and an unknown domain                         Ω                 {\displaystyle \Omega }   . The segment                         Γ                 {\displaystyle \Gamma }    of the boundary of                         Ω                 {\displaystyle \Omega }    which is not known at the outset of the problem is the free boundary. FBs arise in various mathematical models encompassing applications that ranges from physical to economical, financial and biological phenomena, where there is an extra effect of the medium. This effect is in general a qualitative change of the medium and hence an appearance of a phase transition: ice to water, liquid to crystal, buying to selling (assets), active to inactive (biology), blue to red (coloring games), disorganized to organized (self-organizing criticality). An interesting aspect of such a criticality is the so-called sandpile dynamic (or Internal DLA). The most classical example is the melting of ice: Given a block of ice, one can solve the heat equation given appropriate initial and boundary conditions to determine its temperature. But, if in any region the temperature is greater than the melting point of ice, this domain will be occupied by liquid water instead. The boundary formed from the ice/liquid interface is controlled dynamically by the solution of the PDE.","2022-10-22","2022-12-03 10:56:17","2024-07-28 20:51:00","2022-12-03 10:56:17","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1117659232","","C:\Users\isido\Zotero\storage\T8LYY8CI\Free_boundary_problem.html","","♥♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6DRMT66","preprint","2014","Herrera, Calypso; Paulot, Louis","Parallel American Monte Carlo","","","","","http://arxiv.org/abs/1404.1180","In this paper we introduce a new algorithm for American Monte Carlo that can be used either for American-style options, callable structured products or for computing counterparty credit risk (e.g. CVA or PFE computation). Leveraging least squares regressions, the main novel feature of our algorithm is that it can be fully parallelized. Moreover, there is no need to store the paths and the payoﬀ computation can be done forwards: this allows to price structured products with complex path and exercise dependencies. The key idea of our algorithm is to split the set of paths in several subsets which are used iteratively. We give the convergence rate of the algorithm. We illustrate our method on an American put option and compare the results with the Longstaﬀ-Schwartz algorithm.","2014-04-04","2022-12-03 13:09:37","2024-07-28 20:51:06","2022-12-03 13:09:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1404.1180 [q-fin]","Comment: 36 pages","C:\Users\isido\Zotero\storage\MKELBXNC\Herrera en Paulot - 2014 - Parallel American Monte Carlo.pdf","","monte carlo; american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1404.1180","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W37U7KH8","encyclopediaArticle","2022","","Parareal","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Parareal&oldid=1117710630","Parareal is a parallel algorithm from numerical analysis and used for the solution of initial value problems. It was introduced in 2001 by Lions, Maday and Turinici. Since then, it has become one of the most widely studied parallel-in-time integration methods.","2022-10-23","2022-12-04 20:18:43","2024-07-28 20:51:20","2022-12-04 20:18:42","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1117710630","","C:\Users\isido\Zotero\storage\RCGZQ9J3\Parareal.html","","ODE; parareal; ♥♥; parallel in time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNL3J7SL","encyclopediaArticle","2022","","Multilevel Monte Carlo method (wikipedia)","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Multilevel_Monte_Carlo_method&oldid=1070124752","Multilevel Monte Carlo (MLMC) methods in numerical analysis are algorithms for computing expectations that arise in stochastic simulations. Just as Monte Carlo methods, they rely on repeated random sampling, but these samples are taken on different levels of accuracy. MLMC methods can greatly reduce the computational cost of standard Monte Carlo methods by taking most samples with a low accuracy and corresponding low cost, and only very few samples are taken at high accuracy and corresponding high cost.","2022-02-05","2022-12-04 20:33:45","2024-07-28 20:52:17","2022-12-04 20:33:45","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1070124752","","C:\Users\isido\Zotero\storage\NSTZZI8D\Multilevel_Monte_Carlo_method.html","","monte carlo; ♥♥♥; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAEDCF37","journalArticle","2011","Cliffe, K. A.; Giles, M. B.; Scheichl, R.; Teckentrup, A. L.","Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients","Computing and Visualization in Science","","1432-9360, 1433-0369","10.1007/s00791-011-0160-x","http://link.springer.com/10.1007/s00791-011-0160-x","We consider the numerical solution of elliptic partial differential equations with random coefﬁcients. Such problems arise, for example, in uncertainty quantiﬁcation for groundwater ﬂow. We describe a novel variance reduction technique for the standard Monte Carlo method, called the multilevel Monte Carlo method, and demonstrate numerically its superiority. The asymptotic cost of solving the stochastic problem with the multilevel method is always significantly lower than that of the standard method and grows only proportionally to the cost of solving the deterministic problem in certain circumstances. Numerical calculations demonstrating the effectiveness of the method for one- and two-dimensional model problems arising in groundwater ﬂow are presented.","2011-01","2022-12-04 20:35:41","2024-07-28 20:54:56","2022-12-04 20:35:41","3-15","","1","14","","Comput. Visual Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\EM2WHUW8\Cliffe e.a. - 2011 - Multilevel Monte Carlo methods and applications to.pdf","","monte carlo; random PDE; ♥; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47E5JX3J","preprint","2013","Giles, Michael B.","Multilevel Monte Carlo methods","","","","","http://arxiv.org/abs/1304.5472","The author’s presentation of multilevel Monte Carlo path simulation at the MCQMC 2006 conference stimulated a lot of research into multilevel Monte Carlo methods. This paper reviews the progress since then, emphasising the simplicity, ﬂexibility and generality of the multilevel Monte Carlo approach. It also offers a few original ideas and suggests areas for future research.","2013-04-19","2022-12-04 20:40:38","2024-07-28 20:57:50","2022-12-04 20:40:38","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1304.5472 [math]","","C:\Users\isido\Zotero\storage\YRKQ5XCV\Giles - 2013 - Multilevel Monte Carlo methods.pdf","","monte carlo; ♥♥♥♥; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1304.5472","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GYXF7ZLI","book","1995","Øksendal, Bernt","Stochastic Differential Equations","","978-3-540-60243-9 978-3-662-03185-8","","","http://link.springer.com/10.1007/978-3-662-03185-8","","1995","2022-12-04 20:48:20","2024-07-28 20:59:17","2022-12-04 20:48:20","","","","","","","","Universitext","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-662-03185-8","<div data-schema-version=""8""><p>gets cited for feynman kac formula</p> </div>","C:\Users\isido\Zotero\storage\NN2LBDZP\Øksendal - 1995 - Stochastic Differential Equations.pdf","","SDE; first passage; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VJQ8UL4","journalArticle","2007","Zhu, Song-Ping; He, Zhi-Wei","CALCULATING THE EARLY EXERCISE BOUNDARY OF AMERICAN PUT OPTIONS WITH AN APPROXIMATION FORMULA","International Journal of Theoretical and Applied Finance","","0219-0249, 1793-6322","10.1142/S0219024907004615","https://www.worldscientific.com/doi/abs/10.1142/S0219024907004615","In this paper, an algorithm to improve the computational accuracy of the analytical approximation to the value of American put options and their optimal exercise boundary proposed by Zhu (2004) is presented. In the current approach, Zhu’s simple approximation formula is used as an initial guess for the optimal exercise boundary of American put options. The determination of an improved optimal exercise boundary is then achieved by setting a null value of the Theta of option value on the optimal exercise boundary. Test example results show that the improvement is indeed signiﬁcant.","2007-11","2022-12-04 20:49:18","2024-07-28 21:00:08","2022-12-04 20:49:18","1203-1227","","07","10","","Int. J. Theor. Appl. Finan.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\2ZSHAVUG\Zhu en He - 2007 - CALCULATING THE EARLY EXERCISE BOUNDARY OF AMERICA.pdf","","american option; ♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SCG2GC94","journalArticle","2006","Bormetti, G.; Montagna, G.; Moreni, N.; Nicrosini, O.","Pricing exotic options in a path integral approach","Quantitative Finance","","1469-7688, 1469-7696","10.1080/14697680500510878","http://www.tandfonline.com/doi/abs/10.1080/14697680500510878","In the framework of Black-Scholes-Merton model of ﬁnancial derivatives, a path integral approach to option pricing is presented. A general formula to price European path dependent options on multidimensional assets is obtained and implemented by means of various ﬂexible and eﬃcient algorithms. As an example, we detail the cases of Asian, barrier knock out, reverse cliquet and basket call options, evaluating prices and Greeks. The numerical results are compared with those obtained with other procedures used in quantitative ﬁnance and found to be in good agreement. In particular, when pricing at-the-money and out-of-the-money options, the path integral approach exhibits competitive performances.","2006-02","2022-12-04 21:32:15","2024-07-28 21:00:36","2022-12-04 21:32:15","55-66","","1","6","","Quantitative Finance","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>we had a path integral course with application on option pricing</p> </div>","C:\Users\isido\Zotero\storage\5RXL3P32\Bormetti e.a. - 2006 - Pricing exotic options in a path integral approach.pdf","","♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U55LBZVJ","journalArticle","2021","Capuozzo, Pietro; Panella, Emanuele; Schettini Gherardini, Tancredi; Vvedensky, Dimitri D.","Path integral Monte Carlo method for option pricing","Physica A: Statistical Mechanics and its Applications","","03784371","10.1016/j.physa.2021.126231","https://linkinghub.elsevier.com/retrieve/pii/S0378437121005045","The Markov chain Monte Carlo (MCMC) method, in conjunction with the Metropolis–Hastings algorithm, is used to simulate the path integral for the Black–Scholes–Merton model of option pricing. After a brief derivation of the path integral solution of this model, we develop the MCMC method by discretizing the path integral on a time lattice and evaluating this discretized form for various scenarios. Particular attention is paid to the existence of autocorrelations, their decay with the number of sweeps, and the resulting estimate of the corresponding errors. After testing our approach against closed-form solutions, we demonstrate the utility and flexibility of our method with applications to non-Gaussian models.","2021-11","2022-12-05 17:14:42","2024-07-28 21:01:24","2022-12-05 17:14:42","126231","","","581","","Physica A: Statistical Mechanics and its Applications","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\4543G9SW\Capuozzo e.a. - 2021 - Path integral Monte Carlo method for option pricin.pdf","","monte carlo; ♥; MCMC; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SPI4W4S","journalArticle","2010","Devreese, Jeroen P. A.; Lemmens, Damiaan; Tempere, Jacques","Path integral approach to Asian options in the Black-Scholes model","Physica A: Statistical Mechanics and its Applications","","03784371","10.1016/j.physa.2009.10.020","http://arxiv.org/abs/0906.4456","We derive a closed-form solution for the price of an average price as well as an average strike geometric Asian option, by making use of the path integral formulation. Our results are compared to a numerical Monte Carlo simulation. We also develop a pricing formula for an Asian option with a barrier on a control process, combining the method of images with a partitioning of the set of paths according to the average along the path. This formula is exact when the correlation is zero, and is approximate when the correlation increases.","2010-02","2022-12-05 17:15:01","2024-07-28 21:02:16","2022-12-05 17:15:01","780-788","","4","389","","Physica A: Statistical Mechanics and its Applications","","","","","","","","en","","","","","arXiv.org","","arXiv:0906.4456 [q-fin]","Comment: 13 pages, 3 figures, updated version has added references to path integral literature","C:\Users\isido\Zotero\storage\TRU3Y9RR\Devreese e.a. - 2010 - Path integral approach to Asian options in the Bla.pdf","","♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JR9ZTF7","preprint","2011","Xia, Yuan","Multilevel Monte Carlo method for jump-diffusion SDEs","","","","","http://arxiv.org/abs/1106.4730","We investigate the extension of the multilevel Monte Carlo path simulation method to jump-diﬀusion SDEs. We consider models with ﬁnite rate activity , using a jump-adapted discretisation in which the jump times are computed and added to the standard uniform discretisation times. The key component in multilevel analysis is the calculation of an expected payoﬀ diﬀerence between a coarse path simulation and a ﬁne path simulation with twice as many timesteps. If the Poisson jump rate is constant, the jump times are the same on both paths and the multilevel extension is relatively straightforward, but the implementation is more complex in the case of state-dependent jump rates for which the jump times naturally diﬀer.","2011-06-23","2022-12-05 17:41:12","2024-07-28 21:03:21","2022-12-05 17:41:12","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1106.4730 [q-fin]","Comment: 36 pages, 10 figures; Comment: 36 pages, 10 figures","C:\Users\isido\Zotero\storage\YGXG767H\Xia - 2011 - Multilevel Monte Carlo method for jump-diffusion S.pdf; C:\Users\isido\Zotero\storage\M5JSKST3\Xia - 2011 - Multilevel Monte Carlo method for jump-diffusion S.pdf","","monte carlo; SDE; ♥; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1106.4730","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FV8BJ945","journalArticle","2011","Casella, Bruno; Roberts, Gareth O.","Exact Simulation of Jump-Diffusion Processes with Monte Carlo Applications","Methodology and Computing in Applied Probability","","1387-5841, 1573-7713","10.1007/s11009-009-9163-1","http://link.springer.com/10.1007/s11009-009-9163-1","We introduce a novel algorithm (JEA) to simulate exactly from a class of one-dimensional jump-diffusion processes with state-dependent intensity. The simulation of the continuous component builds on the recent Exact Algorithm ((1)). The simulation of the jump component instead employes a thinning algorithm with stochastic acceptance probabilities in the spirit of (14). In turn JEA allows unbiased Monte Carlo simulation of a wide class of functionals of the process’ trajectory, including discrete averages, max/min, crossing events, hitting times. Our numerical experiments show that the method outperforms Monte Carlo methods based on the Euler discretization.","2011-09","2022-12-05 17:46:55","2024-07-28 21:04:26","2022-12-05 17:46:55","449-473","","3","13","","Methodol Comput Appl Probab","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\6PDHDU6J\Casella en Roberts - 2011 - Exact Simulation of Jump-Diffusion Processes with .pdf","","monte carlo; SDE; first passage; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUEPNLKV","conferencePaper","2009","Binder, Ilia; Braverman, Mark","The complexity of simulating Brownian Motion","Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms","978-0-89871-680-1 978-1-61197-306-8","","10.1137/1.9781611973068.7","https://epubs.siam.org/doi/10.1137/1.9781611973068.7","We analyze the complexity of the Walk on Spheres algorithm for simulating Brownian Motion in a domain Ω ⊂ Rd. The algorithm, which was ﬁrst proposed in the 1950s, produces samples from the hitting probability distribution of the Brownian Motion process on ∂Ω within an error of ε. The algorithm is used as a building block for solving a variety of diﬀerential equations, including the Dirichlet Problem.","2009-01-04","2022-12-07 07:37:52","2024-07-28 21:05:05","2022-12-07 07:37:52","58-67","","","","","","","","","","","Society for Industrial and Applied Mathematics","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\9LATAZYY\Binder en Braverman - 2009 - The complexity of simulating Brownian Motion.pdf","","monte carlo; walk on spheres; first passage; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms","","","","","","","","","","","","","","",""
"39YC8CRC","preprint","2022","Azzone, Michele; Baviera, Roberto","A fast Monte Carlo scheme for additive processes and option pricing","","","","","http://arxiv.org/abs/2112.08291","In this paper, we present a very fast Monte Carlo scheme for additive processes: the computational time is of the same order of magnitude of standard algorithms for Brownian motions. We analyze in detail numerical error sources and propose a technique that reduces the two major sources of error. We also compare our results with a benchmark method: the jump simulation with Gaussian approximation.","2022-11-18","2022-12-07 07:46:49","2024-07-28 21:07:24","2022-12-07 07:46:49","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2112.08291 [q-fin]","","C:\Users\isido\Zotero\storage\NKJIF6K7\Azzone en Baviera - 2022 - A fast Monte Carlo scheme for additive processes a.pdf","","monte carlo; ♥♥♥; levy processes; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2112.08291","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94VFW5X6","preprint","2021","Herrmann, Samuel; Massin, Nicolas","Exact simulation of the first passage time through a given level for jump diffusions","","","","","http://arxiv.org/abs/2106.05560","Continuous-time stochastic processes play an important role in the description of random phenomena, it is therefore of prime interest to study particular variables depending on their paths, like stopping time for example. One approach consists in pointing out explicit expressions of the probability distributions, an other approach is rather based on the numerical generation of the random variables. We propose an algorithm in order to generate the ﬁrst passage time through a given level of a one-dimensional jump diﬀusion. This process satisﬁes a stochastic diﬀerential equation driven by a Brownian motion and subject to random shocks characterized by an independent Poisson process. Our algorithm belongs to the family of rejection sampling procedures, also called exact simulation in this context: the outcome of the algorithm and the stopping time under consideration are identically distributed. It is based on both the exact simulation of the diﬀusion at a given time and on the exact simulation of ﬁrst passage time for continuous diﬀusions. It is therefore based on an extension of the algorithm introduced by Herrmann and Zucca [16] in the continuous framework. The challenge here is to generate the exact position of a continuous diﬀusion conditionally to the fact that the given level has not been reached before. We present the construction of the algorithm and give numerical illustrations, conditions on the recurrence of jump diﬀusions are also discussed.","2021-06-10","2022-12-07 07:48:30","2024-07-28 21:11:29","2022-12-07 07:48:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.05560 [math]","","C:\Users\isido\Zotero\storage\DBB4B6I2\Herrmann en Massin - 2021 - Exact simulation of the first passage time through.pdf","","first passage; ♥♥; levy processes","","","","","","","","","","","","","","","","","","","","arXiv:2106.05560","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPTA6MLJ","bookSection","2010","Devroye, Luc","On Exact Simulation Algorithms for Some Distributions Related to Brownian Motion and Brownian Meanders","Recent Developments in Applied Probability and Statistics","978-3-7908-2597-8 978-3-7908-2598-5","","","http://link.springer.com/10.1007/978-3-7908-2598-5_1","We survey and develop exact random variate generators for several distributions related to Brownian motion, Brownian bridge, Brownian excursion, Brownian meander, and related restricted Brownian motion processes. Various parameters such as maxima and ﬁrst passage times are dealt with at length. We are particularly interested in simulating process variables in expected time uniformly bounded over all parameters.","2010","2022-12-08 15:17:42","2024-07-28 21:11:37","2022-12-08 15:17:42","1-35","","","","","","","","","","","Physica-Verlag HD","Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-7908-2598-5_1","","C:\Users\isido\Zotero\storage\IKVR3JRG\Devroye - 2010 - On Exact Simulation Algorithms for Some Distributi.pdf","","first passage; ♥","","Devroye, Luc; Karasözen, Bülent; Kohler, Michael; Korn, Ralf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YDUMFRIC","preprint","2017","Dieker, A. B.; Lagos, Guido","On the Euler discretization error of Brownian motion about random times","","","","","http://arxiv.org/abs/1708.04356","In this paper we derive weak limits for the discretization errors of sampling barrierhitting and extreme events of Brownian motion by using the Euler discretization simulation method. Speciﬁcally, we consider the Euler discretization approximation of Brownian motion to sample barrier-hitting events, i.e. hitting for the ﬁrst time a deterministic “barrier” function; and to sample extreme events, i.e. attaining a minimum on a given compact time interval or unbounded closed time interval. For each case we study the discretization error between the actual time the event occurs versus the time the event occurs for the discretized path, and also the discretization error on the position of the Brownian motion at these times. We show limits in distribution for the discretization errors normalized by their convergence rate, and give closed-form analytic expressions for the limiting random variables. Additionally, we use these limits to study the asymptotic behaviour of Gaussian random walks in the following situations: (1.) the overshoot of a Gaussian walk above a barrier that goes to inﬁnity; (2.) the minimum of a Gaussian walk compared to the minimum of the Brownian motion obtained when interpolating the Gaussian walk with Brownian bridges, both up to the same time horizon that goes to inﬁnity; and (3.) the global minimum of a Gaussian walk compared to the global minimum of the Brownian motion obtained when interpolating the Gaussian walk with Brownian bridges, when both have the same positive drift decreasing to zero. In deriving these limits in distribution we provide a uniﬁed framew√ork to understand the relation between several papers where the constant −ζ(1/2)/ 2π has appeared, where ζ is the Riemann zeta function. In particular, we show that this constant is the mean of some of the limiting distributions we derive.","2017-08-14","2022-12-08 15:19:28","2024-07-28 21:12:12","2022-12-08 15:19:28","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1708.04356 [math]","","C:\Users\isido\Zotero\storage\BPSGSE6D\Dieker en Lagos - 2017 - On the Euler discretization error of Brownian moti.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1708.04356","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YI8CANNN","journalArticle","2013","Xu, Lin; Zhu, Dongjin","On the Distribution of First Exit Time for Brownian Motion with Double Linear Time-Dependent Barriers","ISRN Applied Mathematics","","2090-5572","10.1155/2013/865347","https://www.hindawi.com/journals/isrn/2013/865347/","This paper focuses on the first exit time for a Brownian motion with a double linear time-dependent barrier specified by. We are concerned in this paper with the distribution of the Brownian motion hitting the upper barrier before hitting the lower linear barrier. The main method we applied here is the Girsanov transform formula. As a result, we expressed the density of such exit time in terms of a finite series. This result principally provides us an analytical expression for the distribution of the aforementioned exit time and  an easy way to compute the distribution of first exit time numerically.","2013-09-26","2022-12-08 15:31:50","2024-07-28 21:12:41","2022-12-08 15:31:50","1-5","","","2013","","ISRN Applied Mathematics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\R3QCYSHV\Xu en Zhu - 2013 - On the Distribution of First Exit Time for Brownia.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PVWRGL2","journalArticle","2012","Beskos, Alexandros; Peluchetti, Stefano; Roberts, Gareth","$\varepsilon$-Strong simulation of the Brownian path","Bernoulli","","1350-7265","10.3150/11-BEJ383","https://projecteuclid.org/journals/bernoulli/volume-18/issue-4/varepsilon-Strong-simulation-of-the-Brownian-path/10.3150/11-BEJ383.full","We present an iterative sampling method which delivers upper and lower bounding processes for the Brownian path. We develop such processes with particular emphasis on being able to unbiasedly simulate them on a personal computer. The dominating processes converge almost surely in the supremum and L1 norms. In particular, the rate of converge in L1 is of the order O(K−1/2), K denoting the computing cost. The a.s. enfolding of the Brownian path can be exploited in Monte Carlo applications involving Brownian paths whence our algorithm (termed the ε-strong algorithm) can deliver unbiased Monte Carlo estimators over path expectations, overcoming discretisation errors characterising standard approaches. We will show analytical results from applications of the ε-strong algorithm for estimating expectations arising in option pricing. We will also illustrate that individual steps of the algorithm can be of separate interest, giving new simulation methods for interesting Brownian distributions.","2012-11-01","2022-12-08 15:51:49","2024-07-28 21:12:46","2022-12-08 15:51:49","","","4","18","","Bernoulli","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\G8BZTKXZ\Beskos e.a. - 2012 - $varepsilon$-Strong simulation of the Brownian pa.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPW2CNRW","journalArticle","2002","Metwally, Steve A.K.; Atiya, Amir F.","Using Brownian Bridge for Fast Simulation of Jump-Diffusion Processes and Barrier Options","The Journal of Derivatives","","1074-1240, 2168-8524","10.3905/jod.2002.319189","http://jod.pm-research.com/lookup/doi/10.3905/jod.2002.319189","Barrier options are one of the most popular derivatives in the financial markets. The authors present a fast and unbiased Monte Carlo approach to pricing barrier options when the underlying security follows a simple jump-diffusion process with constant parameters and a continuously monitored barrier. Two algorithms are based on the Brownian bridge concept. The first one is based on a sampling approach to evaluate an integral that results from application of the Brownian bridge. The second approach approximates that integral using a Taylor series expansion. Both methods significantly reduce bias and speed convergence compared to the standard Monte Carlo simulation approach. For example, the first method achieves zero bias. In addition, it is about 100 times faster than the conventional Monte Carlo method that achieves acceptable bias. In developing the second algorithm, the authors derive a novel approach for obtaining a first-passage time density integral using a Taylor series expansion. This approach is potentially useful in other applications, where the expectation of some function over the first-passage time distribution needs to be derived.","2002-08-31","2022-12-08 16:05:02","2024-07-28 21:12:49","2022-12-08 16:05:02","43-54","","1","10","","JOD","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\ABLIPAX8\Metwally en Atiya - 2002 - Using Brownian Bridge for Fast Simulation of Jump-.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQYEVNSX","journalArticle","2006","Deaconu, Madalina; Lejay, Antoine","A Random Walk on Rectangles Algorithm","Methodology and Computing in Applied Probability","","1387-5841, 1573-7713","10.1007/s11009-006-7292-3","http://link.springer.com/10.1007/s11009-006-7292-3","In this article, we introduce an algorithm that simulates eﬃciently the ﬁrst exit time and position from a rectangle (or a parallelepiped) for a Brownian motion that starts at any point inside. This method provides an exact way to simulate the ﬁrst exit time and position from any polygonal domain and then to solve some Dirichlet problems, whatever the dimension. This method can be used as a replacement or complement of the method of the random walk on spheres and can be easily adapted to deal with Neumann boundary conditions or Brownian motion with a constant drift.","2006-03","2022-12-13 07:57:59","2024-07-28 21:12:52","2022-12-13 07:57:59","135-151","","1","8","","Methodol Comput Appl Probab","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\LETY9EC9\Deaconu en Lejay - 2006 - A Random Walk on Rectangles Algorithm.pdf","","monte carlo; PDE; walk on spheres; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RKK8AUU","journalArticle","2015","Hwang, Chi-Ok; Hong, Sungpyo; Kim, Jinwoo","Off-centered “Walk-on-Spheres” (WOS) algorithm","Journal of Computational Physics","","00219991","10.1016/j.jcp.2015.10.002","https://linkinghub.elsevier.com/retrieve/pii/S0021999115006646","The “Walk-on-Spheres” (WOS) algorithm has played the central role in simulating the diffusion process in Diffusion Monte Carlo methods. In this paper, based on the isomorphism between the electrostatic Poisson problem and the corresponding diffusion motion expectation of the first-passage, we develop an off-centered WOS algorithm to replace the old WOS one. We find that the new off-centered WOS algorithm is much more efficient than the old one.","2015-12","2022-12-13 11:30:39","2024-07-28 21:12:55","2022-12-13 11:30:39","331-335","","","303","","Journal of Computational Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\3YHCI5ZF\Hwang e.a. - 2015 - Off-centered “Walk-on-Spheres” (WOS) algorithm.pdf","","monte carlo; PDE; walk on spheres; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RK5YI5P9","preprint","2022","Yılmazer, Ekrem Fatih; Vicini, Delio; Jakob, Wenzel","Solving Inverse PDE Problems using Grid-Free Monte Carlo Estimators","","","","","http://arxiv.org/abs/2208.02114","Modeling physical phenomena like heat transport and diffusion is crucially dependent on the numerical solution of partial differential equations (PDEs). A PDE solver finds the solution given coefficients and a boundary condition, whereas an inverse PDE solver goes the opposite way and reconstructs these inputs from an existing solution. In this article, we investigate techniques for solving inverse PDE problems using a gradient-based methodology. Conventional PDE solvers based on the finite element method require a domain meshing step that can be fragile and costly. Grid-free Monte Carlo methods instead stochastically sample paths using variations of the walk on spheres algorithm to construct an unbiased estimator of the solution. The uncanny similarity of these methods to physically-based rendering algorithms has been observed by several recent works. In the area of rendering, recent progress has led to the development of efficient unbiased derivative estimators. They solve an adjoint form of the problem and exploit arithmetic invertibility to compute gradients using a constant amount of memory and linear time complexity. Could these two lines of work be combined to compute cheap parametric derivatives of a grid-free PDE solver? We investigate this question and present preliminary results. CCS Concepts: • Mathematics of computing → Partial differential equations; • Computing methodologies → Rendering.","2022-08-03","2022-12-13 11:39:58","2024-07-28 21:13:03","2022-12-13 11:39:58","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2208.02114 [cs, math]","Comment: 9 pages (2 pages references and appendix), 9 figures","C:\Users\isido\Zotero\storage\RQAZ53HP\Yılmazer e.a. - 2022 - Solving Inverse PDE Problems using Grid-Free Monte.pdf","","monte carlo; PDE; walk on spheres; rendering; inverse problem; ♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2208.02114","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FE7DSJT7","journalArticle","2003","Hwang, Chi-Ok; Mascagni, Michael","Analysis and comparison of Green’s function first-passage algorithms with “Walk on Spheres” algorithms","Mathematics and Computers in Simulation","","03784754","10.1016/S0378-4754(03)00091-0","https://linkinghub.elsevier.com/retrieve/pii/S0378475403000910","We analyze the optimization of the running times of Green’s function ﬁrst-passage (GFFP) algorithms. The running times for these new ﬁrst-passage (FP) algorithms [1–4], which use exact Green’s functions for the Laplacian to eliminate the absorption layer in the “walk on spheres” (WOS) method [5–9], are compared with those for WOS algorithms. It has been empirically observed that GFFP algorithms are more eﬃcient than WOS algorithms when high accuracy is required [2–4]. Additionally, it has been observed that there is always an optimal distance from the surface of the absorbing boundary, δI , for a GFFP algorithm within which a FP surface can be permitted to intersect the boundary [2–4]. In this paper, we will provide a rigorous complexity analysis consistent with these observations. This analysis is based on estimating the numbers of WOS and GFFP steps needed for absorption on the boundary, and the complexity and running times of each WOS and GFFP step. As an illustration, we analyze the running times for calculating the capacitance of the unit cube using both GFFP and WOS.","2003-11","2022-12-18 13:10:56","2024-07-28 21:13:06","2022-12-18 13:10:56","605-613","","6","63","","Mathematics and Computers in Simulation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\HBPK37PS\Hwang en Mascagni - 2003 - Analysis and comparison of Green’s function first-.pdf","","monte carlo; PDE; walk on spheres; green function; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JC22BPH9","journalArticle","2001","Hwang, Chi-Ok; Given, James A.; Mascagni, Michael","The Simulation–Tabulation Method for Classical Diffusion Monte Carlo","Journal of Computational Physics","","00219991","10.1006/jcph.2001.6947","https://linkinghub.elsevier.com/retrieve/pii/S0021999101969475","Many important classes of problems in materials science and biotechnology require the solution of the Laplace or Poisson equation in disordered two-phase domains in which the phase interface is extensive and convoluted. Green’s function first-passage (GFFP) methods solve such problems efficiently by generalizing the “walk on spheres” (WOS) method to allow first-passage (FP) domains to be not just spheres but a wide variety of geometrical shapes. (In particular, this solves the difficulty of slow convergence with WOS by allowing FP domains that contain patches of the phase interface.) Previous studies accomplished this by using geometries for which the Green’s function was available in quasi-analytic form. Here, we extend these studies by using the simulation–tabulation (ST) method. We simulate and then tabulate surface Green’s functions that cannot be obtained analytically. The ST method is applied to the Solc–Stockmayer model with zero potential, to the mean trapping rate of a diffusing particle in a domain of nonoverlapping spherical traps, and to the effective conductivity for perfectly insulating, nonoverlapping spherical inclusions in a matrix of finite conductivity. In all cases, this class of algorithms provides the most efficient methods known to solve these problems to high accuracy.","2001-12","2022-12-18 13:14:08","2024-07-28 21:13:13","2022-12-18 13:14:08","925-946","","2","174","","Journal of Computational Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BTSXYC8S\Hwang e.a. - 2001 - The Simulation–Tabulation Method for Classical Dif.pdf","","green function; first passage; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZKL4UBC","journalArticle","1999","Milstein, G. N.; Tretyakov, M. V.","Simulation of a space-time bounded diffusion","The Annals of Applied Probability","","1050-5164","10.1214/aoap/1029962812","https://projecteuclid.org/journals/annals-of-applied-probability/volume-9/issue-3/Simulation-of-a-space-time-bounded-diffusion/10.1214/aoap/1029962812.full","Mean-square approximations, which ensure boundedness of both time and space increments, are constructed for stochastic differential equations in a bounded domain. The proposed algorithms are based on a space–time discretization using a random walk over boundaries of small space–time parallelepipeds. To realize the algorithms, exact distributions for exit points of the space–time Brownian motion from a space–time parallelepiped are given. Convergence theorems are stated for the proposed algorithms. A method of approximate searching for exit points of the space–time diffusion from the bounded domain is constructed. Results of several numerical tests are presented.","1999-08-01","2022-12-20 11:10:43","2024-07-28 21:13:16","2022-12-20 11:10:43","","","3","9","","Ann. Appl. Probab.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\3A6SZGK6\Milstein en Tretyakov - 1999 - Simulation of a space-time bounded diffusion.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8S65NS4H","journalArticle","2016","Allab, Imene; Watier, Francois","First-passage Time Estimation of Diffusion Processes through Time-Varying Boundaries with an Application in Finance","International Journal of Statistics and Probability","","1927-7040, 1927-7032","10.5539/ijsp.v6n1p59","http://www.ccsenet.org/journal/index.php/ijsp/article/view/63088","In this paper, we develop a Monte Carlo based algorithm for estimating the FPT (ﬁrst passage time) density of the solution of a one-dimensional time-homogeneous SDE (stochastic diﬀerential equation) through a time-dependent frontier. We consider Brownian bridges as well as local Daniels curve approximations to obtain tractable estimations of the FPT probability between successive points of a simulated path of the process. Under mild assumptions, a (unique) Daniels curve local approximation can easily be obtained by explicitly solving a non-linear system of equations.","2016-12-20","2022-12-20 11:13:32","2024-07-28 21:13:20","2022-12-20 11:13:32","59","","1","6","","IJSP","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\8DTMPTXR\Allab en Watier - 2016 - First-passage Time Estimation of Diffusion Process.pdf","","first passage; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQLG9NPZ","journalArticle","2016","Herrmann, Samuel; Tanré, Etienne","The first-passage time of the Brownian motion to a curved boundary: an algorithmic approach","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/151006172","http://arxiv.org/abs/1501.07060","Under some weak conditions, the ﬁrst-passage time of the Brownian motion to a continuous curved boundary is an almost surely ﬁnite stopping time. Its probability density function (pdf) is explicitly known only in few particular cases. Several mathematical studies proposed to approximate the pdf in a quite general framework or even to simulate this hitting time using a discrete time approximation of the Brownian motion. The authors study a new algorithm which permits to simulate the ﬁrst-passage time using an iterating procedure. The convergence rate presented in this paper suggests that the method is very eﬃcient.","2016-01","2022-12-20 11:18:59","2024-07-28 21:14:30","2022-12-20 11:18:59","A196-A215","","1","38","","SIAM J. Sci. Comput.","The first-passage time of the Brownian motion to a curved boundary","","","","","","","en","","","","","arXiv.org","","arXiv:1501.07060 [math]","","C:\Users\isido\Zotero\storage\5NRAE68H\Herrmann en Tanré - 2016 - The first-passage time of the Brownian motion to a.pdf","","first passage; ♥♥♥","Mathematics - Probability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2VT9SM9","journalArticle","2013","Lejay, Antoine; Maire, Sylvain","New Monte Carlo schemes for simulating diffusions in discontinuous media","Journal of Computational and Applied Mathematics","","03770427","10.1016/j.cam.2012.12.013","https://linkinghub.elsevier.com/retrieve/pii/S0377042712005444","We introduce new Monte Carlo simulation schemes for diﬀusions in a discontinuous media divided in subdomains with piecewise constant diﬀusivity. These schemes are higher order extensions of the usual schemes and take into account the two dimensional aspects of the diﬀusion at the interface between subdomains. This is achieved using either stochastic processes techniques or an approach based on ﬁnite diﬀerences. Numerical tests on elliptic, parabolic and eigenvalue problems involving an operator in divergence form show the eﬃciency of these new schemes.","2013-06","2022-12-20 11:32:05","2024-07-28 21:15:14","2022-12-20 11:32:05","97-116","","","245","","Journal of Computational and Applied Mathematics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\U84MVIQQ\Lejay en Maire - 2013 - New Monte Carlo schemes for simulating diffusions .pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYVUVPLG","preprint","2015","Yang, Xuxin; Rasila, Antti; Sottinen, Tommi","Walk on Spheres Algorithm for Helmholtz and Yukawa Equations via Duffin Correspondence","","","","","http://arxiv.org/abs/1512.07725","We show that a constant-potential time-independent Schro¨dinger equation with Dirichlet boundary data can be reformulated as a Laplace equation with Dirichlet boundary data. With this reformulation, which we call the Duﬃn correspondence, we provide a classical Walk On Spheres (WOS) algorithm for Monte Carlo simulation of the solutions of the boundary value problem. We compare the obtained Duﬃn WOS algorithm with existing modiﬁed WOS algorithms.","2015-12-24","2022-12-20 11:34:46","2024-07-28 21:15:19","2022-12-20 11:34:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1512.07725 [math]","Comment: 14 pages, 5 figures","C:\Users\isido\Zotero\storage\3XFZ3E64\Yang e.a. - 2015 - Walk on Spheres Algorithm for Helmholtz and Yukawa.pdf","","monte carlo; PDE; walk on spheres; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:1512.07725","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2ZFT9T2","encyclopediaArticle","2022","","Volterra integral equation","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Volterra_integral_equation&oldid=1125547908","In mathematics, the Volterra integral equations are a special type of integral equations.[1] They are divided into two groups referred to as the first and the second kind. A linear Volterra equation of the first kind is     f ( t ) = ∫ a t K ( t , s ) x ( s ) d s f(t)=\int _{a}^{t}K(t,s)\,x(s)\,ds where f is a given function and x is an unknown function to be solved for. A linear Volterra equation of the second kind is     x ( t ) = f ( t ) + ∫ a t K ( t , s ) x ( s ) d s . x(t)=f(t)+\int _{a}^{t}K(t,s)x(s)\,ds. In operator theory, and in Fredholm theory, the corresponding operators are called Volterra operators. A useful method to solve such equations, the Adomian decomposition method, is due to George Adomian. A linear Volterra integral equation is a convolution equation if     x ( t ) = f ( t ) + ∫ t 0 t K ( t − s ) x ( s ) d s . x(t)=f(t)+\int _{{t_{0}}}^{t}K(t-s)x(s)\,ds. The function K K in the integral is called the kernel. Such equations can be analyzed and solved by means of Laplace transform techniques. For a weakly singular kernel of the form K ( t , s ) = ( t 2 − s 2 ) − α {\displaystyle K(t,s)=(t^{2}-s^{2})^{-\alpha }} with 0 < α < 1 0<\alpha<1, Volterra integral equation of the first kind can conveniently be transformed into a classical Abel integral equation. The Volterra integral equations were introduced by Vito Volterra and then studied by Traian Lalescu in his 1908 thesis, Sur les équations de Volterra, written under the direction of Émile Picard. In 1911, Lalescu wrote the first book ever on integral equations. Volterra integral equations find application in demography as Lotka's integral equation,[2] the study of viscoelastic materials, in actuarial science through the renewal equation,[3] and in fluid mechanics to describe the flow behavior near finite-sized boundaries.[4][5]","2022-12-04","2022-12-20 11:41:19","2024-07-28 21:15:25","2022-12-20 11:41:19","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1125547908","","C:\Users\isido\Zotero\storage\BXA6JUFN\Volterra_integral_equation.html","","integral equations; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSIFH68Z","preprint","2017","Herrmann, Samuel; Zucca, Cristina","Exact simulation of the first-passage time of diffusions","","","","","http://arxiv.org/abs/1705.06881","Since diﬀusion processes arise in so many diﬀerent ﬁelds, eﬃcient technics for the simulation of sample paths, like discretization schemes, represent crucial tools in applied probability. Such methods permit to obtain approximations of the ﬁrst-passage times as a by-product. For eﬃciency reasons, it is particularly challenging to simulate directly this hitting time by avoiding to construct the whole paths. In the Brownian case, the distribution of the ﬁrst-passage time is explicitly known and can be easily used for simulation purposes. The authors introduce a new rejection sampling algorithm which permits to perform an exact simulation of the ﬁrst-passage time for general one-dimensional diﬀusion processes. The efﬁciency of the method, which is essentially based on Girsanov’s transformation, is described through theoretical results and numerical examples.","2017-05-19","2022-12-20 11:45:15","2024-07-28 21:15:30","2022-12-20 11:45:15","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1705.06881 [math]","","C:\Users\isido\Zotero\storage\UZ8C3VE3\Herrmann en Zucca - 2017 - Exact simulation of the first-passage time of diff.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1705.06881","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6S2E2MV5","journalArticle","2005","Drabeck, Florian","Monte Carlo Simulation of Boundary Crossing Probabilities for a Brownian Motion and Curved Boundaries","","","","","","We are concerned with the probability that a standard Brownian motion W (t) crosses a curved boundary c(t) on a  nite interval [0, T ]. Let this probability be denoted by Q(c(t); T ). Except for linear functions c(t) and a few other special cases no explicit, analytic formula for Q(c(t); T ) is available. Thus numerical methods need to be applied for general boundaries to obtain approximate solutions. Some authors use for example integral equations. However, most of these numerical methods are either intractable or di cult to asses in terms of their accuracy. Due to recent advances in research another way of estimating Q(c(t); T ) seems feasible: Monte Carlo Simulation. Wang and Pötzelberger (1997) derived an explicit formula for the boundary crossing probability of piecewise linear functions which has the form of an expectation. Based on this formula we proceed as follows: First we approximate the general boundary c(t) by a piecewise linear function cm(t) on a uniform partition 0 = t0 < t1... < tm = T . Then we simulate Brownian sample paths in order to evaluate the expectation in the formula of the authors for cm(t). The bias resulting when estimating Q(cm(t); T ) rather than Q(c(t); T ) can be bounded by a formula of Borovkov and Novikov (2005). Whereas the bias decreases at a rate of O(1/m2) for a partition rank m, the standard error due to Monte Carlo simulation only decays at a rate of O(1/√n), where n is the number of simulation cycles. Hence the standard deviation   or the variance respectively   is the main limiting factor when increasing the accuracy. The main goal of this dissertation is to  nd and evaluate variance reducing techniques in order to enhance the quality of the Monte Carlo estimator for Q(c(t); T ). Among the techniques we discuss are: • Antithetic Sampling, • Strati ed Sampling, • Importance Sampling, • Control Variates, • Transforming the original problem. We analyze each of these techniques thoroughly from a theoretical point of view. Further, we test each technique empirically through simulation experiments on several carefully chosen boundaries. In order to asses our results we set them in relation to a previously established benchmark. We are interested in the relative reduction in the mean squared error (= sum of the squared bias and variance) due to a given technique, where the computational e ort remains constant. As a result of this dissertation we derive some very potent techniques that yield a substantial improvement in terms of accuracy. We even discuss an approach that improves the rate at which our biased Monte Carlo estimator converges to the correct result.","2005-01-01","2022-12-20 11:47:54","2024-08-12 14:21:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\36MEIWXP\Drabeck - Monte Carlo Simulation of Boundary Crossing Probab.pdf","","♥; first passage","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2EV8VV9","journalArticle","2017","Jin, Zhiyong; Wang, Liqun","First Passage Time for Brownian Motion and Piecewise Linear Boundaries","Methodology and Computing in Applied Probability","","1387-5841, 1573-7713","10.1007/s11009-015-9475-2","http://link.springer.com/10.1007/s11009-015-9475-2","We propose a new approach to calculating the first passage time densities for Brownian motion crossing piecewise linear boundaries which can be discontinuous. Using this approach we obtain explicit formulas for the first passage densities and show that they are continuously differentiable except at the break points of the boundaries. Furthermore, these formulas can be used to approximate the first passage time distributions for general nonlinear boundaries. The numerical computation can be easily done by using the Monte Carlo integration, which is straightforward to implement. Some numerical examples are presented for illustration. This approach can be further extended to compute two-sided boundary crossing distributions.","2017-03","2022-12-20 11:52:56","2024-07-28 21:16:30","2022-12-20 11:52:56","237-253","","1","19","","Methodol Comput Appl Probab","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BNBKMEFU\Jin en Wang - 2017 - First Passage Time for Brownian Motion and Piecewi.pdf","","first passage; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRSNTZYG","journalArticle","2018","Sabelfeld, Karl K.","Application of the von Mises–Fisher distribution to Random Walk on Spheres method for solving high-dimensional diffusion–advection–reaction equations","Statistics & Probability Letters","","01677152","10.1016/j.spl.2018.03.002","https://linkinghub.elsevier.com/retrieve/pii/S0167715218301160","We suggest a new efficient and reliable random walk method, continuous both in space and time, for solving high-dimensional diffusion–advection–reaction equations. It is based on a discovered intrinsic relation between the von Mises–Fisher distribution on a sphere with this type of equations. It can be formulated as follows: the von Mises–Fisher distribution uniquely defines the solution of a diffusion–advection equation in any bounded or unbounded domain if the relevant boundary value problem for this equation satisfies regular existence and uniqueness conditions. Both two- and three-dimensional transient equations are included in our considerations. The accuracy and the cost of the suggested random walk on spheres method are estimated.","2018-07","2022-12-20 18:49:27","2024-07-28 21:16:33","2022-12-20 18:49:27","137-142","","","138","","Statistics & Probability Letters","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\E78HU5DQ\Sabelfeld - 2018 - Application of the von Mises–Fisher distribution t.pdf","","first passage; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MXJ5LN5U","journalArticle","2022","Qi, Yang; Seyb, Dario; Bitterli, Benedikt; Jarosz, Wojciech","A bidirectional formulation for Walk on Spheres","Computer Graphics Forum","","0167-7055, 1467-8659","10.1111/cgf.14586","https://onlinelibrary.wiley.com/doi/10.1111/cgf.14586","Numerically solving partial differential equations (PDEs) is central to many applications in computer graphics and scientific modeling. Conventional methods for solving PDEs often need to discretize the space first, making them less efficient for complex geometry. Unlike conventional methods, the walk on spheres (WoS) algorithm recently introduced to graphics is a grid-free Monte Carlo method that can provide numerical solutions of Poisson equations without discretizing space. We draw analogies between WoS and classical rendering algorithms, and find that the WoS algorithm is conceptually equivalent to forward path tracing. Inspired by similar approaches in light transport, we propose a novel WoS reformulation that operates in the reverse direction, starting at source points and estimating the Green’s function at “sensor” points. Implementations of this algorithm show improvement over classical WoS in solving Poisson equation with sparse sources. Our approach opens exciting avenues for future algorithms for PDE estimation which, analogous to light transport, connect WoS walks starting from sensors and sources and combine different strategies for robust solution algorithms in all cases.","2022-07","2022-12-22 14:21:42","2024-07-28 21:16:36","2022-12-22 14:21:42","51-62","","4","41","","Computer Graphics Forum","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\Y6LCPRIN\Qi e.a. - 2022 - A bidirectional formulation for Walk on Spheres.pdf","","monte carlo; PDE; walk on spheres; green function; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E23XPQSS","journalArticle","2022","Rioux-Lavoie, Damien; Sugimoto, Ryusuke; Özdemir, Tümay; Shimada, Naoharu H.; Batty, Christopher; Nowrouzezahrai, Derek; Hachisuka, Toshiya","A Monte Carlo Method for Fluid Simulation","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3550454.3555450","https://dl.acm.org/doi/10.1145/3550454.3555450","We present a novel Monte Carlo-based fluid simulation approach capable of pointwise and stochastic estimation of fluid motion. Drawing on the Feynman-Kac representation of the vorticity transport equation, we propose a recursive Monte Carlo estimator of the Biot-Savart law and extend it with a stream function formulation that allows us to treat free-slip boundary conditions using a Walk-on-Spheres algorithm. Inspired by the Monte Carlo literature in rendering, we design and compare variance reduction schemes suited to a fluid simulation context for the first time, show its applicability to complex boundary settings, and detail a simple and practical implementation with temporal grid caching. We validate the correctness of our approach via quantitative and qualitative evaluations – across a range of settings and domain geometries – and thoroughly explore its parameters’ design space. Finally, we provide an in-depth discussion of several axes of future work building on this new numerical simulation modality. CCS Concepts: • Mathematics of computing → Probabilistic algorithms; Partial differential equations; • Computing methodologies → Modeling and simulation.","2022-12","2022-12-22 15:05:35","2024-07-28 21:16:39","2022-12-22 15:05:35","1-16","","6","41","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\76CW9YCY\Rioux-Lavoie e.a. - 2022 - A Monte Carlo Method for Fluid Simulation.pdf","","monte carlo; PDE; walk on spheres; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YCBGETU","journalArticle","2019","Sabelfeld, Karl","Random walk on rectangles and parallelepipeds algorithm for solving transient anisotropic drift-diffusion-reaction problems","Monte Carlo Methods and Applications","","","10.1515/mcma-2019-2039","","In this paper a random walk on arbitrary rectangles (2D) and parallelepipeds (3D) algorithm is developed for solving transient anisotropic drift-diffusion-reaction equations. The method is meshless, both in space and time. The approach is based on a rigorous representation of the first passage time and exit point distributions for arbitrary rectangles and parallelepipeds. The probabilistic representation is then transformed to a form convenient for stochastic simulation. The method can be used to calculate fluxes to any desired part of the boundary, from arbitrary sources. A global version of the method we call here as a stochastic expansion from cell to cell (SECC) algorithm for calculating the whole solution field is suggested. Application of this method to solve a system of transport equations for electrons and holes in a semicoductor is discussed. This system consists of the continuity equations for particle densities and a Poisson equation for electrostatic potential. To validate the method we have derived a series of exact solutions of the drift-diffusion-reaction problem in a three-dimensional layer presented in the last section in details.","2019-05-10","2022-12-22 15:21:22","2024-07-28 21:16:44","","","","","25","","Monte Carlo Methods and Applications","","","","","","","","","","","","","ResearchGate","","","","","https://www.researchgate.net/publication/333002793_Random_walk_on_rectangles_and_parallelepipeds_algorithm_for_solving_transient_anisotropic_drift-diffusion-reaction_problems","monte carlo; PDE; walk on spheres; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPFUSDD2","journalArticle","2017","Sabelfeld, Karl K.","Random walk on spheres algorithm for solving transient drift-diffusion-reaction problems","Monte Carlo Methods and Applications","","1569-3961","10.1515/mcma-2017-0113","https://www.degruyter.com/document/doi/10.1515/mcma-2017-0113/html","We suggest in this paper a Random Walk on Spheres (RWS) method for solving transient drift-diffusion-reaction problems which is an extension of our algorithm we developed recently [26] for solving steady-state drift-diffusion problems. Both two-dimensional and three-dimensional problems are solved. Survival probability, first passage time and the exit position for a sphere (disc) of the drift-diffusion-reaction process are explicitly derived from a generalized spherical integral relation we prove both for two-dimensional and three-dimensional problems. The distribution of the exit position on the sphere has the form of the von Mises–Fisher distribution which can be simulated efficiently. Rigorous expressions are derived in the case of constant velocity drift, but the algorithm is then extended to solve drift-diffusion-reaction problems with arbitrary varying drift velocity vector. The method can efficiently be applied to calculate the fluxes of the solution to any part of the boundary. This can be done by applying a reciprocity theorem which we prove here for the drift-diffusion-reaction problems with general boundary conditions. Applications of this approach to methods of cathodoluminescence (CL) and electron beam induced current (EBIC) imaging of defects and dislocations in semiconductors are presented.","2017-09-01","2022-12-22 15:24:08","2024-07-28 21:16:48","2022-12-22 15:24:08","189-212","","3","23","","","","","","","","","","en","","","","","www.degruyter.com","","Publisher: De Gruyter","","","","PDE; walk on spheres; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU58MPQ4","journalArticle","1997","Veach, Eric","Robust Monte Carlo Methods for Light Transport Simulation. Ph.D. Dissertation. Stanford University.","Robust Monte Carlo Methods for Light Transport Simulation.","","","","","Light transport algorithms generate realistic images by simulating the emission and scattering of light in an artificial environment. Applications include lighting design, architecture, and computer animation, while related engineering disciplines include neutron transport and radiative heat transfer. The main challenge with these algorithms is the high complexity of the geometric, scattering, and illumination models that are typically used. In this dissertation, we develop new Monte Carlo techniques that greatly extend the range of input models for which light transport simulations are practical. Our contributions include new theoretical models, statistical methods, and rendering algorithms. We start by developing a rigorous theoretical basis for bidirectional light transport algorithms (those that combine direct and adjoint techniques). First, we propose a linear operator formulation that does not depend on any assumptions about the physical validity of the input scene. We show how to obtain mathematically correct results using a variety of bidirectional techniques. Next we derive a different formulation, such that for any physically valid input scene, the transport operators are symmetric. This symmetry is important for both theory and implementations, and is based on a new reciprocity condition that we derive for transmissive materials. Finally, we show how light transport can be formulated as an integral over a space of paths. This framework allows new sampling and integration techniques to be applied, such as the Metropolis sampling algorithm. We also use this model to investigate the limitations of unbiased Monte Carlo methods, and to show that certain kinds of paths cannot be sampled. Our statistical contributions include a new technique called multiple importance sampling, which can greatly increase the robustness of Monte Carlo integration. It uses more than one sampling technique to evaluate an integral, and then combines these samples in a vi way that is provably close to optimal. This leads to estimators that have low variance for a broad class of integrands. We also describe a new variance reduction technique called efficiency-optimized Russian roulette. Finally, we link these ideas together to obtain new Monte Carlo light transport algorithms. Bidirectional path tracing uses a family of different path sampling techniques that generate some path vertices starting from a light source, and some starting from a sensor. We show that when these techniques are combined using multiple importance sampling, a large range of difficult lighting effects can be handled efficiently. The algorithm is unbiased, handles arbitrary geometry and materials, and is relatively simple to implement. The second algorithm we describe is Metropolis light transport, inspired by the Metropolis sampling method from computational physics. Paths are generated by following a random walk through path space, such that the probability density of visiting each path is proportional to the contribution it makes to the ideal image. The resulting algorithm is unbiased, uses little storage, handles arbitrary geometry and materials, and can be orders of magnitude more efficient than previous unbiased approaches. It performs especially well on problems that are usually considered difficult, e.g. those involving bright indirect light, small geometric holes, or glossy surfaces. To our knowledge, this is the first application of the Metropolis method to transport problems of any kind.","1997","2022-12-23 16:13:42","2024-07-28 21:16:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\HDZ56Y9U\Veach - 1997 - Robust Monte Carlo Methods for Light Transport Sim.pdf","","monte carlo; rendering; importance sampling; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EDUNYYUM","journalArticle","2018","Novák, Jan; Georgiev, Iliyan; Hanika, Johannes; Jarosz, Wojciech","Monte Carlo Methods for Volumetric Light Transport Simulation","Computer Graphics Forum","","01677055","10.1111/cgf.13383","https://onlinelibrary.wiley.com/doi/10.1111/cgf.13383","The wide adoption of path-tracing algorithms in high-end realistic rendering has stimulated many diverse research initiatives. In this paper we present a coherent survey of methods that utilize Monte Carlo integration for estimating light transport in scenes containing participating media. Our work complements the volume-rendering state-of-the-art report by Cerezo et al. [CPP∗05]; we review publications accumulated since its publication over a decade ago, and include earlier methods that are key for building light transport paths in a stochastic manner. We begin by describing analog and non-analog procedures for freepath sampling and discuss various expected-value, collision, and track-length estimators for computing transmittance. We then review the various rendering algorithms that employ these as building blocks for path sampling. Special attention is devoted to null-collision methods that utilize ﬁctitious matter to handle spatially varying densities; we import two “next-ﬂight” estimators originally developed in nuclear sciences. Whenever possible, we draw connections between image-synthesis techniques and methods from particle physics and neutron transport to provide the reader with a broader context.","2018-05","2022-12-23 16:33:45","2024-07-28 21:16:59","2022-12-23 16:33:45","551-576","","2","37","","Computer Graphics Forum","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\8Y3IJRVP\Novák e.a. - 2018 - Monte Carlo Methods for Volumetric Light Transport.pdf","","rendering; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4KWRF38L","journalArticle","1990","Delaurentis, J.M; Romero, L.A","A Monte Carlo method for poisson's equation","Journal of Computational Physics","","00219991","10.1016/0021-9991(90)90199-B","https://linkinghub.elsevier.com/retrieve/pii/002199919090199B","This investigation presents an analysis of a Monte Carlo method for estimating local solutions to the Dirichlet problem for Poisson’s equation. The probabilistic algorithm consists of a modified “walk on spheres” that includes the effects from internal sources as part of the random process. A new derivation of the asymptotic expressions for the rate of convergence and average runtime of the algorithm is presented. These estimates are used to compare the Monte Carlo method with discrete difference schemes.Numerical experiments involving some two-dimensional problems contirm the efficiency of the probabilistic scheme. ‘87 1990 Academic Press. Inc","1990-09","2022-12-26 14:52:10","2024-07-28 21:17:12","2022-12-26 14:52:10","123-140","","1","90","","Journal of Computational Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\HV3ST26K\Delaurentis en Romero - 1990 - A Monte Carlo method for poisson's equation.pdf","","monte carlo; PDE; walk on spheres; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2L8527MK","document","1967","","screened poisson equation via WoS","","","","","","FOR a bounded region D (with boundary F) of a three-dimensional Euclidean space X we consider the problem AU--B= -g, + = 4F, c = eonst > 0, (1) where the function g satisfies a Holder condition in D, and 4 is a continuous function on the boundary F. Let t(l) be a random Wiener process in the space X, g(O) = PO = (% Yo, 20) E D, and 7 the time of the first exit of the process J(t) from the region D. With the conditions formulated in 111, Theorem 13.16, the solution of problem (1) at the point P, can be represented by an integral in Wiener measure: In [21 a general algorithm for the approximate evaluation of Wiener integrals by the Monte Carlo method was considered, based on the successive modelling of the positions of a Wiener trajectory at fixed instants of time with stop At. A ~aw~ck of this method is the su~t~ti~ increase in compu~r time as Al * 0. For the solution of the problem Au = 0, + = Q, (3) the alg~ithm for “walks on spheres” is well known from 13, 41.We shall briefly","1967-10-18","2022-12-27 11:06:11","2024-08-12 14:58:12","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\M6U2RW8L\screened poisson with WoS.pdf","","♥♥; monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X46BAB9Z","journalArticle","2008","Halton, John H.","Sequential Monte Carlo for linear systems – a practical summary","Monte Carlo Methods and Applications","","0929-9629, 1569-3961","10.1515/MCMA.2008.001","https://www.degruyter.com/document/doi/10.1515/MCMA.2008.001/html","This paper has been written in response to many requests for a practical guide to the use of the technique of sequential Monte Carlo in the fast numerical solving of large systems of linear equations. This method, which I have used with considerable success to solve such problems, improving the tricks of the trade as I learned more about it, has suffered from some neglect through the mathematical difﬁculty, for some of those who are more interested in using the tool than in thinking about it, of some of the theoretical aspects of rigorously proving its validity, which – at this juncture – is no longer in question. I hope that I have now closed this gap in the related literature.","2008-01","2023-01-04 12:53:02","2024-07-28 21:18:04","2023-01-04 12:53:02","1-27","","1","14","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>We like this if it was the paper that solves for the residual with MC</p> </div>","C:\Users\isido\Zotero\storage\YGVFCZZK\Halton - 2008 - Sequential Monte Carlo for linear systems – a prac.pdf","","monte carlo; linear systems; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X59QGGAT","journalArticle","1997","Linetsky, Vadim","The Path Integral Approach to Financial Modeling and Options Pricing","","","","","","In this paper we review some applications of the path integral methodology of quantum mechanics to ﬁnancial modeling and options pricing. A path integral is deﬁned as a limit of the sequence of ﬁnite-dimensional integrals, in a much the same way as the Riemannian integral is deﬁned as a limit of the sequence of ﬁnite sums. The risk-neutral valuation formula for path-dependent options contingent upon multiple underlying assets admits an elegant representation in terms of path integrals (Feynman–Kac formula). The path integral representation of transition probability density (Green’s function) explicitly satisﬁes the diffusion PDE. Gaussian path integrals admit a closed-form solution given by the Van Vleck formula. Analytical approximations are obtained by means of the semiclassical (moments) expansion. Difﬁcult path integrals are computed by numerical procedures, such as Monte Carlo simulation or deterministic discretization schemes. Several examples of pathdependent options are treated to illustrate the theory (weighted Asian options, ﬂoating barrier options, and barrier options with ladder-like barriers).","1997-03-20","2023-01-05 15:46:56","2024-08-12 14:31:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\3ZDBN84W\Linetsky - The Path Integral Approach to Financial Modeling a.pdf","","♥; examen padintegralen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J58S7M9S","journalArticle","2015","Das Sarma, Atish; Molla, Anisur Rahaman; Pandurangan, Gopal; Upfal, Eli","Fast distributed PageRank computation","Theoretical Computer Science","","03043975","10.1016/j.tcs.2014.04.003","https://linkinghub.elsevier.com/retrieve/pii/S0304397514002709","Over the last decade, PageRank has gained importance in a wide range of applications and domains, ever since it ﬁrst proved to be effective in determining node importance in large graphs (and was a pioneering idea behind Google’s search engine). In distributed computing alone, PageRank vector, or more generally random walk based quantities have been used for several different applications ranging from determining important nodes, load balancing, search, and identifying connectivity structures. Surprisingly, however, there has been little work towards designing provably eﬃcient fully-distributed algorithms for computing PageRank. The diﬃculty is that traditional matrix–vector multiplication style iterative methods may not always adapt well to the distributed setting owing to communication bandwidth restrictions and convergence rates.","2015-01","2023-01-05 19:24:52","2024-07-29 07:52:41","2023-01-05 19:24:52","113-121","","","561","","Theoretical Computer Science","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\FLUGEZHI\Das Sarma e.a. - 2015 - Fast distributed PageRank computation.pdf","","page rank; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP2CP2XW","preprint","2021","Zhang, Qi; Yao, Zhengan; Liang, Jun; Zhang, Zanbo","An Advanced Parallel PageRank Algorithm","","","","","http://arxiv.org/abs/2112.07363","Initially used to rank web pages, PageRank has now been applied in may ﬁelds. In general case, there are plenty of special vertices such as dangling vertices and unreferenced vertices in the graph. Existing PageRank algorithms usually consider them as ‘bad‘ vertices since they may take troubles. However, in this paper, we propose a parallel PageRank algorithm which can take advantage of these special vertices. For this end, we ﬁrstly interpret PageRank from the information transmitting perspective and give a constructive deﬁnition of PageRank. Then, based on the information transmitting interpretation, a parallel PageRank algorithm which we call the Information Transmitting Algorithm(ITA) is proposed. We prove that the dangling vertices can increase ITA’s convergence rate and the unreferenced vertices and weak unreferenced vertices can decrease ITA’s calculations. Compared with the MONTE CARLO method, ITA has lower bandwidth requirement. Compared with the power method, ITA has higher convergence rate and generates less calculations. Finally, experimental results on four data sets demonstrate that ITA is 1.5-4 times faster than the power method and converges more uniformly.","2021-12-12","2023-01-05 19:43:35","2024-07-29 07:52:17","2023-01-05 19:43:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2112.07363 [cs]","","C:\Users\isido\Zotero\storage\YZS4ETMH\Zhang e.a. - 2021 - An Advanced Parallel PageRank Algorithm.pdf","","page rank; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2112.07363","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRIUFVDM","preprint","2022","Salaün, Corentin; Gruson, Adrien; Hua, Binh-Son; Hachisuka, Toshiya; Singh, Gurprit","Regression-based Monte Carlo Integration","","","","","http://arxiv.org/abs/2211.07422","Monte Carlo integration is typically interpreted as an estimator of the expected value using stochastic samples. There exists an alternative interpretation in calculus where Monte Carlo integration can be seen as estimating a \emph{constant} function -- from the stochastic evaluations of the integrand -- that integrates to the original integral. The integral mean value theorem states that this \emph{constant} function should be the mean (or expectation) of the integrand. Since both interpretations result in the same estimator, little attention has been devoted to the calculus-oriented interpretation. We show that the calculus-oriented interpretation actually implies the possibility of using a more \emph{complex} function than a \emph{constant} one to construct a more efficient estimator for Monte Carlo integration. We build a new estimator based on this interpretation and relate our estimator to control variates with least-squares regression on the stochastic samples of the integrand. Unlike prior work, our resulting estimator is \emph{provably} better than or equal to the conventional Monte Carlo estimator. To demonstrate the strength of our approach, we introduce a practical estimator that can act as a simple drop-in replacement for conventional Monte Carlo integration. We experimentally validate our framework on various light transport integrals. The code is available at \url{https://github.com/iribis/regressionmc}.","2022-11-14","2023-01-05 21:27:42","2024-07-29 08:29:41","2023-01-05 21:27:42","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.07422 [cs]","Comment: 14 pages, 16 figures, ACM Trans. Graph., Vol. 41, No. 4, Article 79. Publication date: July 2022; <div data-schema-version=""8""><p>reference for control variates for recursive</p> <p>Monte Carlo but doesn’t seem aware </p> <p>of a lot of other works like </p> <p>stein control variates</p> </div>; <div data-schema-version=""8""><p>see randomised trapezoidal</p> </div>","C:\Users\isido\Zotero\storage\FDCESJF7\Salaün e.a. - 2022 - Regression-based Monte Carlo Integration.pdf","","monte carlo; rendering; control variates; ♥♥♥; variance reduction","","","","","","","","","","","","","","","","","","","","arXiv:2211.07422","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTW7X9CU","preprint","2023","Albert, Jaroslav","A tau-leaping method for computing joint probability distributions of the first-passage time and position of a Brownian particle","","","","","http://arxiv.org/abs/2301.00647","First passage time (FPT), also known as ﬁrst hitting time, is the time a particle, subject to some stochastic process, hits or crosses a closed surface for the very ﬁrst time. τ -leaping methods are a class of stochastic algorithms in which, instead of simulating every single reaction, many reactions are “leaped” over in order to shorten the computing time. In this paper we developed a τ -leaping method for computing the FPT and position in arbitrary volumes for a Brownian particle governed by the Langevin equation. The τ -leaping method proposed here works as follows. A sphere is inscribed within the volume of interest (VOI) centered at the initial particle’s location. On this sphere, the FPT is sampled, as well as the position, which becomes the new initial position. Then, another sphere, centered at this new location, is inscribed. This process continues until the sphere becomes smaller than some minimal radius Rmin. When this occurs, the τ -leaping switches to the conventional Monte Carlo, which runs until the particle either crosses the surface of the VOI or ﬁnds its way to a position where a sphere of radius > Rmin can be inscribed. The switching between τ -leaping and MC continues until the particle crosses the surface of the VOI. The purpose of a minimal radius is to avoid having to sample the velocities, which become irrelevant when the particle diﬀuses beyond a certain distance, i. e. Rmin The size of this radius depends on the system parameters and on one’s notion of accuracy: the larger this radius the more accurate the τ -leaping method, but also less eﬃcient. This trade oﬀ between accuracy and eﬃciency is discussed. For two VOI, the τ -leaping method is shown to be accurate and more eﬃcient than MC by at least a factor of 10 and up to a factor of about 110. However, while MC becomes exponentially slower with increasing VOI, the eﬃciency of the τ -leaping method remains relatively unchanged. Thus, the τ -leaping method can potentially be many orders of magnitude more eﬃcient than MC.","2023-01-02","2023-01-05 21:32:56","2024-07-29 08:01:27","2023-01-05 21:32:56","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.00647 [cond-mat, q-bio]","<div data-schema-version=""8""><p>proposes WoS coming from tau leaping,</p> <p>no references to OG WoS</p> </div>","C:\Users\isido\Zotero\storage\J5YXETE7\Albert - 2023 - A tau-leaping method for computing joint probabili.pdf","","first passage; ♥; tau-leaping","","","","","","","","","","","","","","","","","","","","arXiv:2301.00647","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KV3SI95Y","journalArticle","2019","Kondapaneni, Ivo; Vevoda, Petr; Grittmann, Pascal; Skřivan, Tomáš; Slusallek, Philipp; Křivánek, Jaroslav","Optimal multiple importance sampling","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3306346.3323009","https://dl.acm.org/doi/10.1145/3306346.3323009","Multiple Importance Sampling (MIS) is a key technique for achieving robustness of Monte Carlo estimators in computer graphics and other fields. We derive optimal weighting functions for MIS that provably minimize the variance of an MIS estimator, given a set of sampling techniques. We show that the resulting variance reduction over the balance heuristic can be higher than predicted by the variance bounds derived by Veach and Guibas, who assumed only non-negative weights in their proof. We theoretically analyze the variance of the optimal MIS weights and show the relation to the variance of the balance heuristic. Furthermore, we establish a connection between the new weighting functions and control variates as previously applied to mixture sampling. We apply the new optimal weights to integration problems in light transport and show that they allow for new design considerations when choosing the appropriate sampling techniques for a given integration problem.","2019-08-31","2023-01-06 10:08:17","2024-07-29 08:29:35","2023-01-06 10:08:17","1-14","","4","38","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\LN9YUIJZ\Kondapaneni e.a. - 2019 - Optimal multiple importance sampling.pdf","","monte carlo; rendering; importance sampling; ♥♥♥; variance reduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYB3M8IG","preprint","2020","Müller, Thomas; Rousselle, Fabrice; Novák, Jan; Keller, Alexander","Neural Control Variates","","","","","http://arxiv.org/abs/2006.01524","We propose neural control variates (NCV) for unbiased variance reduction in parametric Monte Carlo integration. So far, the core challenge of applying the method of control variates has been finding a good approximation of the integrand that is cheap to integrate. We show that a set of neural networks can face that challenge: a normalizing flow that approximates the shape of the integrand and another neural network that infers the solution of the integral equation. We also propose to leverage a neural importance sampler to estimate the difference between the original integrand and the learned control variate. To optimize the resulting parametric estimator, we derive a theoretically optimal, variance-minimizing loss function, and propose an alternative, composite loss for stable online training in practice. When applied to light transport simulation, neural control variates are capable of matching the state-of-the-art performance of other unbiased approaches, while providing means to develop more performant, practical solutions. Specifically, we show that the learned light-field approximation is of sufficient quality for high-order bounces, allowing us to omit the error correction and thereby dramatically reduce the noise at the cost of negligible visible bias.","2020-09-04","2023-01-06 10:14:46","2024-07-29 08:30:13","2023-01-06 10:14:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2006.01524 [cs, stat]","Comment: To appear at SIGGRAPH Asia 2020. Updated with better loss function, leading to better results. 19 pages, 14 figures","C:\Users\isido\Zotero\storage\VJD8P7DL\Müller e.a. - 2020 - Neural Control Variates.pdf","","monte carlo; rendering; control variates; machine learning; deep learning; ♥♥♥♥; variance reduction","","","","","","","","","","","","","","","","","","","","arXiv:2006.01524","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4Y7RS8G","preprint","2019","Müller, Thomas; McWilliams, Brian; Rousselle, Fabrice; Gross, Markus; Novák, Jan","Neural Importance Sampling","","","","","http://arxiv.org/abs/1808.03856","We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the KL and the $\chi^2$ divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.","2019-09-03","2023-01-06 10:20:22","2024-07-29 08:30:08","2023-01-06 10:20:22","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1808.03856 [cs, stat]","Comment: 19 pages, 15 figures. Accepted for publication in ACM Transactions on Graphics; presented at SIGGRAPH 2019","C:\Users\isido\Zotero\storage\UQDR2FVH\Müller e.a. - 2019 - Neural Importance Sampling.pdf","","monte carlo; rendering; importance sampling; machine learning; deep learning; ♥♥♥♥; variance reduction","","","","","","","","","","","","","","","","","","","","arXiv:1808.03856","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9BFESRU","journalArticle","2020","Ruppert, Lukas; Herholz, Sebastian; Lensch, Hendrik P. A.","Robust fitting of parallax-aware mixtures for path guiding","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3386569.3392421","https://dl.acm.org/doi/10.1145/3386569.3392421","Effective local light transport guiding demands for high quality guiding information, i.e., a precise representation of the directional incident radiance distribution at every point inside the scene. We introduce a parallax-aware distribution model based on parametric mixtures. By parallax-aware warping of the distribution, the local approximation of the 5D radiance field remains valid and precise across large spatial regions, even for close-by contributors. Our robust optimization scheme fits parametric mixtures to radiance samples collected in previous rendering passes. Robustness is achieved by splitting and merging of components refining the mixture. These splitting and merging decisions minimize and bound the expected variance of the local radiance estimator. In addition, we extend the fitting scheme to a robust, iterative update method, which allows for incremental training of our model using smaller sample batches. This results in more frequent training updates and, at the same time, significantly reduces the required sample memory footprint. The parametric representation of our model allows for the application of advanced importance sampling methods such as radiance-based, cosine-aware, and even product importance sampling. Our method further smoothly integrates next-event estimation (NEE) into path guiding, avoiding importance sampling of contributions better covered by NEE. The proposed robust fitting and update scheme, in combination with the parallax-aware representation, results in faster learning and lower variance compared to state-of-the-art path guiding approaches.","2020-08-31","2023-01-06 10:21:50","2024-07-29 08:24:14","2023-01-06 10:21:50","","","4","39","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\Y3W4KYQT\Ruppert e.a. - 2020 - Robust fitting of parallax-aware mixtures for path.pdf","","rendering; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFWUYRXJ","preprint","2017","Dahm, Ken; Keller, Alexander","Learning Light Transport the Reinforced Way","","","","","http://arxiv.org/abs/1701.07403","We show that the equations of reinforcement learning and light transport simulation are related integral equations. Based on this correspondence, a scheme to learn importance while sampling path space is derived. The new approach is demonstrated in a consistent light transport simulation algorithm that uses reinforcement learning to progressively learn where light comes from. As using this information for importance sampling includes information about visibility, too, the number of light transport paths with zero contribution is dramatically reduced, resulting in much less noisy images within a ﬁxed time budget.","2017-08-15","2023-01-06 10:22:43","2024-07-29 08:25:47","2023-01-06 10:22:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1701.07403 [cs]","Comment: Revised version","C:\Users\isido\Zotero\storage\2D4V4CM6\Dahm en Keller - 2017 - Learning Light Transport the Reinforced Way.pdf","","rendering; machine learning; reinforcement learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1701.07403","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPSS8XTH","journalArticle","2015","Manzi, Marco; Kettunen, Markus; Aittala, Miika; Lehtinen, Jaakko; Durand, Frédo; Zwicker, Matthias","Gradient-Domain Bidirectional Path Tracing","","","","","","Gradient-domain path tracing has recently been introduced as an efﬁcient realistic image synthesis algorithm. This paper introduces a bidirectional gradient-domain sampler that outperforms traditional bidirectional path tracing often by a factor of two to ﬁve in terms of squared error at equal render time. It also improves over unidirectional gradient-domain path tracing in challenging visibility conditions, similarly to how conventional bidirectional path tracing improves over its unidirectional counterpart. Our algorithm leverages a novel multiple importance sampling technique and an efﬁcient implementation of a high-quality shift mapping suitable for bidirectional path tracing. We demonstrate the versatility of our approach in several challenging light transport scenarios.","2015-06-22","2023-01-06 10:23:18","2024-08-12 14:33:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\G9T7TZDT\Manzi e.a. - Gradient-Domain Bidirectional Path Tracing.pdf","","♥; rendering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3A579MRC","preprint","2019","Durkan, Conor; Bekasov, Artur; Murray, Iain; Papamakarios, George","Cubic-Spline Flows","","","","","http://arxiv.org/abs/1906.02145","A normalizing ﬂow models a complex probability density as an invertible transformation of a simple density. The invertibility means that we can evaluate densities and generate samples from a ﬂow. In practice, autoregressive ﬂow-based models are slow to invert, making either density estimation or sample generation slow. Flows based on coupling transforms are fast for both tasks, but have previously performed less well at density estimation than autoregressive ﬂows. We stack a new coupling transform, based on monotonic cubic splines, with LU-decomposed linear layers. The resulting cubic-spline ﬂow retains an exact onepass inverse, can be used to generate high-quality images, and closes the gap with autoregressive ﬂows on a suite of density-estimation tasks.","2019-06-05","2023-01-06 10:24:27","2024-07-29 08:33:02","2023-01-06 10:24:27","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1906.02145 [cs, stat]","Comment: Appeared at the 1st Workshop on Invertible Neural Networks and Normalizing Flows at ICML 2019","C:\Users\isido\Zotero\storage\FG5VAIUR\Durkan e.a. - 2019 - Cubic-Spline Flows.pdf","","machine learning; deep learning; density estimation; ♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:1906.02145","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JXNHCLY","preprint","2019","Durkan, Conor; Bekasov, Artur; Murray, Iain; Papamakarios, George","Neural Spline Flows","","","","","http://arxiv.org/abs/1906.04032","A normalizing ﬂow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the ﬂexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the ﬂexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline ﬂows improve density estimation, variational inference, and generative modeling of images.","2019-12-02","2023-01-06 10:24:49","2024-07-29 08:32:59","2023-01-06 10:24:49","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1906.04032 [cs, stat]","Comment: Published at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada","C:\Users\isido\Zotero\storage\RNWFF9CZ\Durkan e.a. - 2019 - Neural Spline Flows.pdf","","machine learning; deep learning; density estimation; ♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:1906.04032","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5FBPAZ4","preprint","2021","Papamakarios, George; Nalisnick, Eric; Rezende, Danilo Jimenez; Mohamed, Shakir; Lakshminarayanan, Balaji","Normalizing Flows for Probabilistic Modeling and Inference","","","","","http://arxiv.org/abs/1912.02762","Normalizing ﬂows provide a general mechanism for deﬁning expressive probability distributions, only requiring the speciﬁcation of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing ﬂows, ranging from improving their expressive power to expanding their application. We believe the ﬁeld has now matured and is in need of a uniﬁed perspective. In this review, we attempt to provide such a perspective by describing ﬂows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of ﬂow design, and discuss foundational topics such as expressive power and computational trade-oﬀs. We also broaden the conceptual framing of ﬂows by relating them to more general probability transformations. Lastly, we summarize the use of ﬂows for tasks such as generative modeling, approximate inference, and supervised learning.","2021-04-08","2023-01-06 10:25:05","2024-07-29 08:32:55","2023-01-06 10:25:05","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1912.02762 [cs, stat]","Comment: Review article, 64 pages, 9 figures. Published in the Journal of Machine Learning Research (see https://jmlr.org/papers/v22/19-1028.html)","C:\Users\isido\Zotero\storage\WS7G7WN2\Papamakarios e.a. - 2021 - Normalizing Flows for Probabilistic Modeling and I.pdf","","importance sampling; machine learning; deep learning; density estimation; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:1912.02762","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LVTNKR3","preprint","2019","Jaini, Priyank; Selby, Kira A.; Yu, Yaoliang","Sum-of-Squares Polynomial Flow","","","","","http://arxiv.org/abs/1905.02325","Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. Based on triangular maps, we propose a general framework for high-dimensional density estimation, by specifying one-dimensional transformations (equivalently conditional densities) and appropriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and ﬂow based methods, (b) allows a uniﬁed understanding of the limitations and representation power of these recent approaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) ﬂow that is interpretable, universal, and easy to train. We perform several synthetic experiments on various density geometries to demonstrate the beneﬁts (and shortcomings) of such transformations. SOS ﬂows achieve competitive results in simulations and several real-world datasets.","2019-06-10","2023-01-06 10:25:18","2024-07-29 08:32:05","2023-01-06 10:25:18","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1905.02325 [cs, stat]","Comment: 13 pages, ICML'2019","C:\Users\isido\Zotero\storage\PCM8RNZR\Jaini e.a. - 2019 - Sum-of-Squares Polynomial Flow.pdf","","importance sampling; machine learning; density estimation; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1905.02325","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AK43FR5V","preprint","2022","Beznea, Lucian; Cimpean, Iulian; Lupascu-Stamate, Oana; Popescu, Ionel; Zarnescu, Arghir","From Monte Carlo to neural networks approximations of boundary value problems","","","","","http://arxiv.org/abs/2209.01432","In this paper we study probabilistic and neural network approximations for solutions to Poisson equation subject to H¨older or C2 data in general bounded domains of Rd. We aim at two fundamental goals.","2022-09-03","2023-01-06 10:25:51","2024-07-29 08:32:03","2023-01-06 10:25:51","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.01432 [cs, math]","","C:\Users\isido\Zotero\storage\G4JL3TT7\Beznea e.a. - 2022 - From Monte Carlo to neural networks approximations.pdf","","monte carlo; PDE; machine learning; boundary value problems; deep learning; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2209.01432","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDJCDHRQ","journalArticle","2007","Zergaïnoh, Anissa; Chihab, Najat; Astruc, Jean Pierre","Construction of Orthonormal Piecewise Polynomial Scaling and Wavelet Bases on Non-Equally Spaced Knots","EURASIP Journal on Advances in Signal Processing","","1687-6180","10.1155/2007/27427","https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/27427","his paper investigates the mathematical framework of multiresolution analysis based on irregularly spaced knots sequence. Our presentation is based on the construction of nested nonuniform spline multiresolution spaces. From these spaces, we present the construction of orthonormal scaling and wavelet basis functions on bounded intervals. For any arbitrary degree of the spline function, we provide an explicit generalization allowing the construction of the scaling and wavelet bases on the nontraditional sequences. We show that the orthogonal decomposition is implemented using filter banks where the coefficients depend on the location of the knots on the sequence. Examples of orthonormal spline scaling and wavelet bases are provided. This approach can be used to interpolate irregularly sampled signals in an efficient way, by keeping the multiresolution approach.","2007-12","2023-01-08 14:49:13","2024-07-29 08:34:49","2023-01-08 14:49:13","027427","","1","2007","","EURASIP J. Adv. Signal Process.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\IA4NHCZX\Zergaïnoh e.a. - 2007 - Construction of Orthonormal Piecewise Polynomial S.pdf","","wavelets; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSZZAWZI","preprint","2021","Subr, Kartic","Q-NET: A Network for Low-Dimensional Integrals of Neural Proxies","","","","","http://arxiv.org/abs/2006.14396","Many applications require the calculation of integrals of multidimensional functions. A general and popular procedure is to estimate integrals by averaging multiple evaluations of the function. Often, each evaluation of the function entails costly computations. The use of a \emph{proxy} or surrogate for the true function is useful if repeated evaluations are necessary. The proxy is even more useful if its integral is known analytically and can be calculated practically. We propose the use of a versatile yet simple class of artificial neural networks -- sigmoidal universal approximators -- as a proxy for functions whose integrals need to be estimated. We design a family of fixed networks, which we call Q-NETs, that operate on parameters of a trained proxy to calculate exact integrals over \emph{any subset of dimensions} of the input domain. We identify transformations to the input space for which integrals may be recalculated without resampling the integrand or retraining the proxy. We highlight the benefits of this scheme for a few applications such as inverse rendering, generation of procedural noise, visualization and simulation. The proposed proxy is appealing in the following contexts: the dimensionality is low ($<10$D); the estimation of integrals needs to be decoupled from the sampling strategy; sparse, adaptive sampling is used; marginal functions need to be known in functional form; or when powerful Single Instruction Multiple Data/Thread (SIMD/SIMT) pipelines are available for computation.","2021-03-30","2023-01-08 17:33:21","2024-07-29 08:36:49","2023-01-08 17:33:21","","","","","","","Q-NET","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2006.14396 [cs, stat]","Comment: 11 pages (including appendix and references)","C:\Users\isido\Zotero\storage\N8BQBFKD\Subr - 2021 - Q-NET A Network for Low-Dimensional Integrals of .pdf","","rendering; machine learning; deep learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2006.14396","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XZWWP2V","conferencePaper","2019","Duminil-Copin, Hugo","SIXTY YEARS OF PERCOLATION","Proceedings of the International Congress of Mathematicians (ICM 2018)","978-981-327-287-3 978-981-327-288-0","","10.1142/9789813272880_0162","https://www.worldscientific.com/doi/abs/10.1142/9789813272880_0162","Percolation models describe the inside of a porous material. The theory emerged timidly in the middle of the twentieth century before becoming one of the major objects of interest in probability and mathematical physics. The golden age of percolation is probably the eighties, during which most of the major results were obtained for the most classical of these models, named Bernoulli percolation, but it is really the two following decades which put percolation theory at the crossroad of several domains of mathematics. In this broad review, we propose to describe brieﬂy some recent progress as well as some famous challenges remaining in the ﬁeld. This review is not intended to probabilists (and a fortiori not to specialists in percolation theory): the target audience is mathematicians of all kinds.","2019-05","2023-01-09 20:52:43","2024-07-29 08:39:06","2023-01-09 20:52:43","2829-2856","","","","","","","","","","","WORLD SCIENTIFIC","Rio de Janeiro, Brazil","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\KAH28XWG\Duminil-Copin - 2019 - SIXTY YEARS OF PERCOLATION.pdf","","♥♥♥; percolation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Congress of Mathematicians 2018","","","","","","","","","","","","","","",""
"MTNHCBJF","videoRecording","2018","Enthought","UMAP Uniform Manifold Approximation and Projection for Dimension Reduction | SciPy 2018 |","","","","","https://www.youtube.com/watch?v=nq6iPZVUxZU","This talk will present a new approach to dimension reduction called UMAP. UMAP is grounded in manifold learning and topology, making an effort to preserve the topological structure of the data. The resulting algorithm can provide both 2D visualisations of data of comparable quality to t-SNE, and general purpose dimension reduction. UMAP has been implemented as a (scikit-learn compatible) python library that can perform efficient dimension reduction, scaling out to much larger datasets than t-SNE or other comparable algorithms (see http://github.com/lmcinnes/umap).","2018-07-14","2023-01-10 16:37:30","2024-07-29 08:40:29","2023-01-10 16:37:30","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; dimension reduction; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","26:05","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CV9BW8H","videoRecording","2019","PyData","A Bluffer's Guide to Dimension Reduction - Leland McInnes","","","","","https://www.youtube.com/watch?v=9iol3Lk6kyU","Dimension reduction is a complicated topic with a vast zoo of diverse techniques for different specialised problems. This talk will seek to cut through the technical detail and focus on the core intuitions that lie behind dimension reduction. From this point of view we'll see that there are only really two core ideas you need to know to understand dimension reduction. === www.pydata.org PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.  PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases. 00:00 Welcome! 00:10 Help us add time stamps or captions to this video! See the description for details.","2019-02-01","2023-01-10 16:50:08","2024-07-29 09:46:28","2023-01-10 16:50:08","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; dimension reduction; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","36:33","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFL6BJ5W","videoRecording","2020","Stitch Fix Multithreaded","Algo Hour - Nearest Neighbor Descent (and friends) | Dr. Leland McInnes","","","","","https://www.youtube.com/watch?v=OvT2NY_FV_g","Finding nearest neighbors in a dataset is an important fundamental problem for many different machine learning tasks, particularly in unsupervised learning. The problem becomes particularly challenging for higher dimensional data. We’ll look at some of the classical tree based techniques, and look at some of the difficulties that occur. Next we’ll explore Nearest Neighbor Descent and related graph based algorithms, working from very simple intuitions to efficient implementations. The approaches provide a powerful and flexible framework for (approximate) nearest neighbor searching, and, as we will demonstrate, significantly outperform tree based approaches on a number of real world datasets. Dr. McInnes is a research mathematician and Data Scientist at the Tutte Institute for Mathematics and Computing in Ottawa, Ontario. He’s well-known for his and collaborators’ creation of the Accelerated HDBSCAN* algorithm, and UMAP – a manifold learning dimension reduction technique.","2020-05-22","2023-01-10 17:07:12","2024-07-29 09:47:34","2023-01-10 17:07:12","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","54:10","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8ZXGQLI","videoRecording","2016","Enthought","High Quality, High Performance Clustering with HDBSCAN | SciPy 2016 | Leland McInnes","","","","","https://www.youtube.com/watch?v=AgPQ76RIi6A","Data clustering is a powerful tool for data analysis. It can be particularly useful in exploratory data analysis for helping to summarize and give intuition about a dataset. Despite it's power clustering is used for this task far less frequently than it could be. A plethora of options for clustering algorithms exist, and we will provide a survey of some of the more popular options, discussing their strengths and weaknesses, particularly with regard to exploratory data analysis. Our focus, however, is on a relatively new algorithm that appears to be the best equipped to meet the needs of exploratory data analysis: HDBSCAN* has the strengths of density based algorithms, has a small robust set of parameters, and with suitable implementation can be made highly scalable to large datasets. We will discuss how the algorithm works, taking a few different perspectives, and explain the techniques used for a high performance implementation. Finally we'll discuss ways to extend the algorithm, drawing on ideas from topological data analysis.","2016-07-15","2023-01-10 17:19:13","2024-07-29 09:47:54","2023-01-10 17:19:13","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; ♥♥♥♥; clustering","","","","","","","","","","","","","","","","","","","","","","22:56","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLR4E6C8","preprint","2021","Hajimohammadi, Zeinab; Parand, Kourosh; Ghodsi, Ali","Legendre Deep Neural Network (LDNN) and its application for approximation of nonlinear Volterra Fredholm Hammerstein integral equations","","","","","http://arxiv.org/abs/2106.14320","Various phenomena in biology, physics, and engineering are modeled by diﬀerential equations. These diﬀerential equations including partial diﬀerential equations and ordinary diﬀerential equations can be converted and represented as integral equations. In particular, Volterra–Fredholm–Hammerstein integral equations are the main type of these integral equations and researchers are interested in investigating and solving these equations. In this paper, we propose Legendre Deep Neural Network (LDNN) for solving nonlinear Volterra–Fredholm–Hammerstein integral equations (V-F-H-IEs). LDNN utilizes Legendre orthogonal polynomials as activation functions of the Deep structure. We present how LDNN can be used to solve nonlinear V-F-H-IEs. We show using the Gaussian quadrature collocation method in combination with LDNN results in a novel numerical solution for nonlinear V-F-H-IEs. Several examples are given to verify the performance and accuracy of LDNN.","2021-06-27","2023-01-10 17:54:46","2024-07-29 09:48:33","2023-01-10 17:54:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.14320 [cs, math]","","C:\Users\isido\Zotero\storage\PYEIWQ2R\Hajimohammadi e.a. - 2021 - Legendre Deep Neural Network (LDNN) and its applic.pdf","","integral equations; machine learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2106.14320","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILUFNPTC","preprint","2021","Formica, M. R.; Ostrovsky, E.; Sirota, L.","Method Monte-Carlo for solving of non-linear integral equations","","","","","http://arxiv.org/abs/2102.07859","We oﬀer in this short report a simple Monte - Carlo method for solving a well posed non - linear integral equations of second Fredholm’s and Volterra’s type and built a conﬁdence region for solution in an uniform norm, applying the grounded Central Limit Theorem in the Banach space of continuous functions.","2021-02-15","2023-01-10 17:58:16","2024-07-29 09:51:05","2023-01-10 17:58:16","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2102.07859 [cs, math]","","C:\Users\isido\Zotero\storage\INK9IZXC\Formica e.a. - 2021 - Method Monte-Carlo for solving of non-linear integ.pdf","","monte carlo; integral equations; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2102.07859","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YN9EMEQQ","preprint","2022","Azze, Abel; D'Auria, Bernardo; García-Portugués, Eduardo","Optimal exercise of American options under time-dependent Ornstein-Uhlenbeck processes","","","","","http://arxiv.org/abs/2211.04095","We study the barrier that gives the optimal time to exercise an American option written on a time-dependent Ornstein–Uhlenbeck process, a diﬀusion often adopted by practitioners to model commodity prices and interest rates. By framing the optimal exercise of the American option as a problem of optimal stopping and relying on probabilistic arguments, we provide a non-linear Volterra-type integral equation characterizing the exercise boundary, develop a novel comparison argument to derive upper and lower bounds for such a boundary, and prove its diﬀerentiability and Lipschitz continuity in any closed interval that excludes the expiration date. We implement a Picard iteration algorithm to solve the Volterra integral equation and show illustrative examples that shed light on the boundary’s dependence on the process’s drift and volatility.","2022-11-08","2023-01-10 17:59:14","2024-07-29 09:52:15","2023-01-10 17:59:14","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.04095 [math, q-fin]","Comment: 22 pages, 3 figures","C:\Users\isido\Zotero\storage\G2H9S8M4\Azze e.a. - 2022 - Optimal exercise of American options under time-de.pdf","","american option; ♥♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2211.04095","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FEWY6WAM","preprint","2022","Schneider, Ryan; Gharibnejad, Heman; Schneider, Barry I.","ITVOLT: An Iterative Solver for Volterra Integral Equations with Application to the Time-Dependent Schr\""odinger Equation","","","","","http://arxiv.org/abs/2210.15677","We present a novel iterative method for solving Volterra integral equations of the second kind. Based on global Lagrange interpolation, the method is simple to implement and applicable to a wide variety of problems. Here, we present the method in detail and discuss several applications, emphasizing in particular its use on the time-dependent Schro¨dinger equation.","2022-10-27","2023-01-10 18:00:48","2024-07-29 11:21:36","2023-01-10 18:00:48","","","","","","","ITVOLT","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.15677 [physics]","Comment: 20 pages, 9 tables, 5 figures","C:\Users\isido\Zotero\storage\VKU8FNSZ\Schneider e.a. - 2022 - ITVOLT An Iterative Solver for Volterra Integral .pdf","","chebychev; integral equations; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2210.15677","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQYEJXLI","preprint","2021","Crucinio, Francesca R.; Doucet, Arnaud; Johansen, Adam M.","A Particle Method for Solving Fredholm Equations of the First Kind","","","","","http://arxiv.org/abs/2009.09974","Fredholm integral equations of the ﬁrst kind are the prototypical example of ill-posed linear inverse problems. They model, among other things, reconstruction of distorted noisy observations and indirect density estimation and also appear in instrumental variable regression. However, their numerical solution remains a challenging problem. Many techniques currently available require a preliminary discretization of the domain of the solution and make strong assumptions about its regularity. For example, the popular expectation maximization smoothing (EMS) scheme requires the assumption of piecewise constant solutions which is inappropriate for most applications. We propose here a novel particle method that circumvents these two issues. This algorithm can be thought of as a Monte Carlo approximation of the EMS scheme which not only performs an adaptive stochastic discretization of the domain but also results in smooth approximate solutions. We analyze the theoretical properties of the EMS iteration and of the corresponding particle algorithm. Compared to standard EMS, we show experimentally that our novel particle method provides state-of-the-art performance for realistic systems, including motion deblurring and reconstruction of cross-section images of the brain from positron emission tomography.","2021-04-23","2023-01-10 18:16:51","2024-07-29 11:23:50","2023-01-10 18:16:51","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2009.09974 [stat]","","C:\Users\isido\Zotero\storage\HMHHAWGP\Crucinio e.a. - 2021 - A Particle Method for Solving Fredholm Equations o.pdf","","monte carlo; integral equations; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2009.09974","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H4GRUBHP","preprint","2019","Keller, Alexander; Dahm, Ken","Integral Equations and Machine Learning","","","","","http://arxiv.org/abs/1712.06115","As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Eﬃciency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artiﬁcial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.","2019-01-30","2023-01-10 18:24:11","2024-07-29 12:15:54","2023-01-10 18:24:11","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1712.06115 [cs]","","C:\Users\isido\Zotero\storage\Q43J8XEA\Keller en Dahm - 2019 - Integral Equations and Machine Learning.pdf","","integral equations; rendering; machine learning; reinforcement learning; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1712.06115","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7IQWMKP","preprint","2021","Gopalakrishna, Chaitanya","A note on Fredholm integral equation","","","","","http://arxiv.org/abs/2106.07194","This note gives results on the existence of semi-continuous solutions of a Fredholm integral equation of the second kind using Tarski’s ﬁxed point theorem.","2021-06-14","2023-01-10 18:48:19","2024-07-29 12:19:22","2023-01-10 18:48:19","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.07194 [math]","Comment: 7 pages","C:\Users\isido\Zotero\storage\BF5MEFDI\Gopalakrishna - 2021 - A note on Fredholm integral equation.pdf","","integral equations; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2106.07194","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWYAGEF9","journalArticle","2021","Yang, Xiao-lin; Wang, Jian-cheng; Yang, Chu-yuan","A New Fast Monte Carlo Code for Solving Radiative Transfer Equations based on Neumann Solution","The Astrophysical Journal Supplement Series","","0067-0049, 1538-4365","10.3847/1538-4365/abec73","http://arxiv.org/abs/2104.07007","In this paper, we proposed a new Monte Carlo radiative transport (MCRT) scheme, which is based completely on the Neumann series solution of Fredholm integral equation. This scheme indicates that the essence of MCRT is the calculation of inﬁnite terms of multiple integrals in Neumann solution simultaneously. Under this perspective we redescribed MCRT procedure systematically, in which the main work amounts to choose an associated probability distribution function (PDF) for a set of random variables and the corresponding unbiased estimation functions. We can select a relatively optimal estimation procedure that has a lower variance from an inﬁnite possible choices, such as the term by term estimation. In this scheme, MCRT can be regarded as a pure problem of integral evaluation, rather than as the tracing of random walking photons. Keeping this in mind, one can avert some subtle intuitive mistakes. In addition the δ-functions in these integrals can be eliminated in advance by integrating them out directly. This fact together with the optimal chosen random variables can remarkably improve the Monte Carlo (MC) computational eﬃciency and accuracy, especially in systems with axial or spherical symmetry. An MCRT code, Lemona(Linear Integral Equations’ Monte Carlo Solver Based on the Neumann solution), has been developed completely based on this scheme. Finally, we intend to verify the validation of Lemon, a suite of test problems mainly restricted to ﬂat spacetime have been reproduced and the corresponding results are illustrated in detail.","2021-06-01","2023-01-10 18:54:28","2024-07-29 12:21:05","2023-01-10 18:54:28","29","","2","254","","ApJS","","","","","","","","en","","","","","arXiv.org","","arXiv:2104.07007 [astro-ph, physics:physics]","Comment: 37 pages, 28 figures. The code can be download from: https://github.com/yangxiaolinyn/Lemon (or https://bitbucket.org/yangxiaolinsc/lemonsourcecode/src/main/) and https://doi.org/10.5281/zenodo.4686355. Comments are welcome","C:\Users\isido\Zotero\storage\5R9X8WF9\2104.07007.pdf","","monte carlo; integral equations; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BN2E9PFX","preprint","2014","Ostrovsky, E.; Sirota, L.","Unbiased Monte Carlo estimation for solving of linear integral equation, with error estimate","","","","","http://arxiv.org/abs/1408.4205","We oﬀer a new Monte-Carlo method for solving linear integral equation which gives the unbiased estimation for solution of Volterra’s and Fredholm’s type, and consider the problem of conﬁdence region building.","2014-08-18","2023-01-10 19:07:59","2024-07-29 12:31:18","2023-01-10 19:07:59","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1408.4205 [math]","<div data-citation-items=""%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10093548%2Fitems%2FBN2E9PFX%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10093548%2Fitems%2FBN2E9PFX%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22We%20o%EF%AC%80er%20a%20new%20Monte-Carlo%20method%20for%20solving%20linear%20integral%20equation%20which%20gives%20the%20unbiased%20estimation%20for%20solution%20of%20Volterra%E2%80%99s%20and%20Fredholm%E2%80%99s%20type%2C%20and%20consider%20the%20problem%20of%20con%EF%AC%81dence%20region%20building.%22%2C%22language%22%3A%22en%22%2C%22note%22%3A%22arXiv%3A1408.4205%20%5Bmath%5D%22%2C%22number%22%3A%22arXiv%3A1408.4205%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Unbiased%20Monte%20Carlo%20estimation%20for%20solving%20of%20linear%20integral%20equation%2C%20with%20error%20estimate%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1408.4205%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Ostrovsky%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Sirota%22%2C%22given%22%3A%22L.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C1%2C10%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222014%22%2C8%2C18%5D%5D%7D%7D%7D%5D"" data-schema-version=""8""><p>Neuman’s series manipulation into simulatable</p> <p>form is likeable</p> </div>","C:\Users\isido\Zotero\storage\UBUR3CE9\Ostrovsky en Sirota - 2014 - Unbiased Monte Carlo estimation for solving of lin.pdf","","monte carlo; integral equations; ♥♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:1408.4205","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNV9Y9VM","preprint","2022","He, Haoze; Dube, Parijat","Accelerating Parallel Stochastic Gradient Descent via Non-blocking Mini-batches","","","","","http://arxiv.org/abs/2211.00889","SOTA decentralized SGD algorithms can overcome the bandwidth bottleneck at the parameter server by using communication collectives like Ring All-Reduce for synchronization. While the parameter updates in distributed SGD may happen asynchronously there is still a synchronization barrier to make sure that the local training epoch at every learner is complete before the learners can advance to the next epoch. The delays in waiting for the slowest learners(stragglers) remain to be a problem in the synchronization steps of these state-of-the-art decentralized frameworks. In this paper, we propose the (de)centralized Non-blocking SGD (Non-blocking SGD) which can address the straggler problem in a heterogeneous environment. The main idea of Non-blocking SGD is to split the original batch into mini-batches, then accumulate the gradients and update the model based on ﬁnished mini-batches. The Non-blocking idea can be implemented using decentralized algorithms including Ring All-reduce, D-PSGD, and MATCHA to solve the straggler problem. Moreover, using gradient accumulation to update the model also guarantees convergence and avoids gradient staleness. Run-time analysis with random straggler delays and computational efﬁciency/throughput of devices is also presented to show the advantage of Non-blocking SGD. Experiments on a suite of datasets and deep learning networks validate the theoretical analyses and demonstrate that Non-blocking SGD speeds up the training and fastens the convergence. Compared with the state-of-the-art decentralized asynchronous algorithms like D-PSGD and MACHA, Non-blocking SGD takes up to 2x fewer time to reach the same training loss in a heterogeneous environment.","2022-11-09","2023-01-11 12:54:14","2024-07-29 13:04:53","2023-01-11 12:54:14","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.00889 [cs]","Comment: 12 pages, 4 figures","C:\Users\isido\Zotero\storage\5TLNEYY3\He en Dube - 2022 - Accelerating Parallel Stochastic Gradient Descent .pdf","","optimization; gradient descent; SGD; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2211.00889","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VU5QKL6E","preprint","2022","Mohamad, Saad; Alamri, Hamad; Bouchachia, Abdelhamid","Scaling up Stochastic Gradient Descent for Non-convex Optimisation","","","","","http://arxiv.org/abs/2210.02882","Stochastic gradient descent (SGD) is a widely adopted iterative method for optimizing differentiable objective functions. In this paper, we propose and discuss a novel approach to scale up SGD in applications involving non-convex functions and large datasets. We address the bottleneck problem arising when using both shared and distributed memory. Typically, the former is bounded by limited computation resources and bandwidth whereas the latter suffers from communication overheads. We propose a uniﬁed distributed and parallel implementation of SGD (named DPSGD) that relies on both asynchronous distribution and lock-free parallelism. By combining two strategies into a uniﬁed framework, DPSGD is able to strike a better trade-off between local computation and communication. The convergence properties of DPSGD are studied for non-convex problems such as those arising in statistical modelling and machine learning. Our theoretical analysis shows that DPSGD leads to speed-up with respect to the number of cores a√nd number of workers while guaranteeing an asymptotic convergence rate of O(1/ T ) given that the number of cores is bounded by T 1/4 and the number of workers is bounded by T 1/2 where T is the number of iterations. The potential gains that can be achieved by DPSGD are demonstrated empirically on a stochastic variational inference problem (Latent Dirichlet Allocation) and on a deep reinforcement learning (DRL) problem (advantage actor critic - A2C) resulting in two algorithms: DPSVI and HSA2C. Empirical results validate our theoretical ﬁndings. Comparative studies are conducted to show the performance of the proposed DPSGD against the state-of-the-art DRL algorithms.","2022-10-06","2023-01-11 12:58:02","2024-07-29 13:05:47","2023-01-11 12:58:02","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.02882 [cs, math, stat]","","C:\Users\isido\Zotero\storage\UJS9IZZ9\Mohamad e.a. - 2022 - Scaling up Stochastic Gradient Descent for Non-con.pdf","","optimization; gradient descent; SGD; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2210.02882","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z6HNLBL","journalArticle","2010","Zinkevich, Martin A; Smola, Alex; Weimer, Markus; Li, Lihong","Parallelized Stochastic Gradient Descent","","","","","","With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the ﬁrst parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].","2010-12-06","2023-01-11 13:02:51","2024-08-12 14:42:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\7SDXTQBD\Zinkevich e.a. - Parallelized Stochastic Gradient Descent.pdf","","♥; gradient descent; optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9P3M9CG2","videoRecording","2020","Abhishek Gupta","Recursive Stochastic Algorithms: A Markov Chain Perspective","","","","","https://www.youtube.com/watch?v=f1IP6rpqaEE","In this video, I present my research work with multiple collaborators and students over the past five years. We can view many machine learning and reinforcement learning algorithms as random operators acting over infinite dimensional spaces. We use tools from Markov chain theory to determine stability and asymptotic consistency properties of the output of such algorithms. We demonstrate our results through some simple simulations.","2020-10-08","2023-01-11 13:58:11","2024-07-29 13:13:05","2023-01-11 13:58:11","","","","","","","Recursive Stochastic Algorithms","","","","","","","","","","","","YouTube","","","","","","optimization; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","1:18:09","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZC3GT8J","preprint","2021","Gupta, Abhishek; Haskell, William B.","Convergence of Recursive Stochastic Algorithms using Wasserstein Divergence","","","","","http://arxiv.org/abs/2003.11403","This paper develops a uniﬁed framework, based on iterated random operator theory, to analyze the convergence of constant stepsize recursive stochastic algorithms (RSAs). RSAs use randomization to eﬃciently compute expectations, and so their iterates form a stochastic process. The key idea of our analysis is to lift the RSA into an appropriate higher-dimensional space and then express it as an equivalent Markov chain. Instead of determining the convergence of this Markov chain (which may not converge under constant stepsize), we study the convergence of the distribution of this Markov chain. To study this, we deﬁne a new notion of Wasserstein divergence. We show that if the distribution of the iterates in the Markov chain satisfy a contraction property with respect to the Wasserstein divergence, then the Markov chain admits an invariant distribution. We show that convergence of a large family of constant stepsize RSAs can be understood using this framework, and we provide several detailed examples.","2021-01-05","2023-01-11 14:08:41","2024-07-29 13:13:24","2023-01-11 14:08:41","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2003.11403 [cs, eess, math, stat]","Comment: 34 pages, submitted to SIMODS","C:\Users\isido\Zotero\storage\IAT28HNB\Gupta en Haskell - 2021 - Convergence of Recursive Stochastic Algorithms usi.pdf","","♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2003.11403","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9AVEBRT4","journalArticle","2015","Akhtar, Muhammad Naveed; Durad, Muhammad Hanif; Ahmed, Asad","SOLVING INITIAL VALUE ORDINARY DIFFERENTIAL EQUATIONS BY MONTE CARLO METHOD","","","","","","The objective of this paper is to perform a computational analysis of an existing Monte Carlo based algorithm to solve initial value problem of ordinary differential equations (ODEs). Firstly the problems associated with the existing algorithm have been rectified by suggesting a new elaborate algorithm. Then the new algorithm has been applied to solve different types of ODEs including simple, explicit coupled, implicit and coupled system of first order ODEs. Furthermore the same has also been implemented to known physical systems such as Van der Pol equation and SIR epidemic model. The limitations of proposed algorithm have also been identified by applying Lipschitz continuity check for an exemplary ODE. Finally it has been demonstrated that it still very difficult to propose a computationally efficient algorithm to solve ODEs with considerable accuracy using Monte Carlo method.","2015","2023-01-12 15:03:28","2024-07-29 13:14:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\WTUKCBSR\Akhtar e.a. - 2015 - SOLVING INITIAL VALUE ORDINARY DIFFERENTIAL EQUATI.pdf","","monte carlo; ODE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6VVX6NL9","encyclopediaArticle","2022","","Magnus expansion","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Magnus_expansion&oldid=1117728056","In mathematics and physics, the Magnus expansion, named after Wilhelm Magnus (1907–1990), provides an exponential representation of the solution of a first-order homogeneous linear differential equation for a linear operator. In particular, it furnishes the fundamental matrix of a system of linear ordinary differential equations of order n with varying coefficients. The exponent is aggregated as an infinite series, whose terms involve multiple integrals and nested commutators.","2022-10-23","2023-01-12 17:52:00","2024-07-29 13:14:09","2023-01-12 17:52:00","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1117728056","","C:\Users\isido\Zotero\storage\WKBPK8ZV\Magnus_expansion.html","","♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YT2SM8YE","encyclopediaArticle","2022","","Baker–Campbell–Hausdorff formula","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula&oldid=1115922051","In mathematics, the Baker–Campbell–Hausdorff formula is the solution for Z Z to the equation e X e Y = e Z {\displaystyle e^{X}e^{Y}=e^{Z}} for possibly noncommutative X and Y in the Lie algebra of a Lie group. There are various ways of writing the formula, but all ultimately yield an expression for Z Z in Lie algebraic terms, that is, as a formal series (not necessarily convergent) in X X and Y Y and iterated commutators thereof. The first few terms of this series are: Z = X + Y + 1 2 [ X , Y ] + 1 12 [ X , [ X , Y ] ] − 1 12 [ Y , [ X , Y ] ] + ⋯ , {\displaystyle Z=X+Y+{\frac {1}{2}}[X,Y]+{\frac {1}{12}}[X,[X,Y]]-{\frac {1}{12}}[Y,[X,Y]]+\cdots \,,} where "" ⋯ \cdots "" indicates terms involving higher commutators of X X and Y Y. If X X and Y Y are sufficiently small elements of the Lie algebra g {\mathfrak {g}} of a Lie group G G, the series is convergent. Meanwhile, every element g g sufficiently close to the identity in G G can be expressed as g = e X {\displaystyle g=e^{X}} for a small X X in g {\mathfrak {g}}. Thus, we can say that near the identity the group multiplication in G G—written as e X e Y = e Z {\displaystyle e^{X}e^{Y}=e^{Z}}—can be expressed in purely Lie algebraic terms. The Baker–Campbell–Hausdorff formula can be used to give comparatively simple proofs of deep results in the Lie group–Lie algebra correspondence. If X X and Y Y are sufficiently small n × n n\times n matrices, then Z Z can be computed as the logarithm of e X e Y {\displaystyle e^{X}e^{Y}}, where the exponentials and the logarithm can be computed as power series. The point of the Baker–Campbell–Hausdorff formula is then the highly nonobvious claim that Z := log ⁡ ( e X e Y ) {\displaystyle Z:=\log \left(e^{X}e^{Y}\right)} can be expressed as a series in repeated commutators of X X and Y Y. Modern expositions of the formula can be found in, among other places, the books of Rossmann[1] and Hall.[2]","2022-10-13","2023-01-12 19:07:44","2024-07-29 13:14:07","2023-01-12 19:07:44","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1115922051","","","","exponential integrators; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JW5HV8Q","journalArticle","2013","Galtier, M.; Blanco, S.; Caliot, C.; Coustet, C.; Dauchet, J.; El Hafi, M.; Eymet, V.; Fournier, R.; Gautrais, J.; Khuong, A.; Piaud, B.; Terrée, G.","Integral formulation of null-collision Monte Carlo algorithms","Journal of Quantitative Spectroscopy and Radiative Transfer","","00224073","10.1016/j.jqsrt.2013.04.001","https://linkinghub.elsevier.com/retrieve/pii/S0022407313001350","At the kinetic level, the meaning of null-collisions is straightforward: they correspond to pureforward scattering events. We here discuss their technical signiﬁcance in integral terms. We ﬁrst consider a most standard null-collision Monte Carlo algorithm and show how it can be rigorously justiﬁed starting from a Fredholm equivalent to the radiative transfer equation. Doing so, we also prove that null-collision algorithms can be slightly modiﬁed so that they deal with unexpected occurrences of negative values of the null-collision coeﬃcient (when the upper bound of the heterogeneous extinction coeﬃcient is nonstrict). We then describe technically, in full details, the resulting algorithm, when applied to the evaluation of the local net-power density within a bounded, heterogeneous, multiple scattering and emitting/absorbing medium. The corresponding integral formulation is then explored theoretically in order to distinguish the statistical signiﬁcance of introducing null-collisions from that of the integral-structure underlying modiﬁcation.","2013-08","2023-01-12 20:06:16","2024-07-29 13:24:56","2023-01-12 20:06:16","57-68","","","125","","Journal of Quantitative Spectroscopy and Radiative Transfer","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\7WE6YFYN\Galtier e.a. - 2013 - Integral formulation of null-collision Monte Carlo.pdf","","monte carlo; rendering; ♥♥♥; kinetic equations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTEYQJ78","journalArticle","2017","Kutz, Peter; Habel, Ralf; Li, Yining Karl; Novák, Jan","Spectral and decomposition tracking for rendering heterogeneous volumes","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3072959.3073665","https://dl.acm.org/doi/10.1145/3072959.3073665","We present two novel unbiased techniques for sampling free paths in heterogeneous participating media. Our               decomposition tracking               accelerates free-path construction by splitting the medium into a control component and a residual component and sampling each of them separately. To minimize expensive evaluations of spatially varying collision coefficients, we define the control component to allow constructing free paths in closed form. The residual heterogeneous component is then homogenized by adding a fictitious medium and handled using weighted delta tracking, which removes the need for computing strict bounds of the extinction function. Our second contribution,               spectral tracking               , enables efficient light transport simulation in chromatic media. We modify free-path distributions to minimize the fluctuation of path throughputs and thereby reduce the estimation variance. To demonstrate the correctness of our algorithms, we derive them               directly               from the radiative transfer equation by extending the integral formulation of null-collision algorithms recently developed in reactor physics. This mathematical framework, which we thoroughly review, encompasses existing trackers and postulates an entire family of new estimators for solving transport problems; our algorithms are examples of such. We analyze the proposed methods in canonical settings and on production scenes, and compare to the current state of the art in simulating light transport in heterogeneous participating media.","2017-08-31","2023-01-12 20:07:57","2024-07-29 13:26:08","2023-01-12 20:07:57","1-16","","4","36","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\49PCP84W\Kutz e.a. - 2017 - Spectral and decomposition tracking for rendering .pdf","","rendering; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4JVTM6R","bookSection","2020","Billaud-Friess, Marie; Macherey, Arthur; Nouy, Anthony; Prieur, Clémentine","Stochastic methods for solving high-dimensional partial differential equations","","","","","http://arxiv.org/abs/1905.05423","We propose algorithms for solving high-dimensional Partial Diﬀerential Equations (PDEs) that combine a probabilistic interpretation of PDEs, through Feynman-Kac representation, with sparse interpolation. Monte-Carlo methods and time-integration schemes are used to estimate pointwise evaluations of the solution of a PDE. We use a sequential control variates algorithm, where control variates are constructed based on successive approximations of the solution of the PDE. Two diﬀerent algorithms are proposed, combining in diﬀerent ways the sequential control variates algorithm and adaptive sparse interpolation. Numerical examples will illustrate the behavior of these algorithms.","2020","2023-01-14 09:22:48","2024-07-29 13:27:06","2023-01-14 09:22:48","125-141","","","324","","","","","","","","","","en","","","","","arXiv.org","","DOI: 10.1007/978-3-030-43465-6_6 arXiv:1905.05423 [math]","","C:\Users\isido\Zotero\storage\9PRXYL9X\Billaud-Friess e.a. - 2020 - Stochastic methods for solving high-dimensional pa.pdf","","PDE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4H34FJQ","preprint","2022","Leluc, Rémi; Portier, François; Segers, Johan; Zhuman, Aigerim","A Quadrature Rule combining Control Variates and Adaptive Importance Sampling","","","","","http://arxiv.org/abs/2205.11890","Driven by several successful applications such as in stochastic gradient descent or in Bayesian computation, control variates have become a major tool for Monte Carlo integration. However, standard methods do not allow the distribution of the particles to evolve during the algorithm, as is the case in sequential simulation methods. Within the standard adaptive importance sampling framework, a simple weighted least squares approach is proposed to improve the procedure with control variates. The procedure takes the form of a quadrature rule with adapted quadrature weights to reﬂect the information brought in by the control variates. The quadrature points and weights do not depend on the integrand, a computational advantage in case of multiple integrands. Moreover, the target density needs to be known only up to a multiplicative constant. Our main result is a non-asymptotic bound on the probabilistic error of the procedure. The bound proves that for improving the estimate’s accuracy, the beneﬁts from adaptive importance sampling and control variates can be combined. The good behavior of the method is illustrated empirically on synthetic examples and real-world data for Bayesian linear regression.","2022-10-05","2023-01-14 10:00:33","2024-07-29 13:29:44","2023-01-14 10:00:33","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2205.11890 [cs, math, stat]","","C:\Users\isido\Zotero\storage\BVHY6BXX\Leluc e.a. - 2022 - A Quadrature Rule combining Control Variates and A.pdf","","importance sampling; control variates; ♥♥; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2205.11890","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2679QTUF","preprint","2022","Fu, Fengjiang; Wang, Xiaoqun","Convergence analysis of a quasi-Monte Carlo-based deep learning algorithm for solving partial differential equations","","","","","http://arxiv.org/abs/2210.16196","Deep learning methods have achieved great success in solving partial diﬀerential equations (PDEs), where the loss is often deﬁned as an integral. The accuracy and eﬃciency of these algorithms depend greatly on the quadrature method. We propose to apply quasi-Monte Carlo (QMC) methods to the Deep Ritz Method (DRM) for solving the Neumann problems for the Poisson equation and the static Schr¨odinger equation. For error estimation, we decompose the error of using the deep learning algorithm to solve PDEs into the generalization error, the approximation error and the training error. We establish the upper bounds and prove that QMC-based DRM achieves an asymptotically smaller error bound than DRM. Numerical experiments show that the proposed method converges faster in all cases and the variances of the gradient estimators of randomized QMC-based DRM are much smaller than those of DRM, which illustrates the superiority of QMC in deep learning over MC.","2022-10-28","2023-01-14 20:47:26","2024-07-29 13:32:07","2023-01-14 20:47:26","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.16196 [cs, math]","Comment: 27 pages, 4 figures, 2 tables","C:\Users\isido\Zotero\storage\SAFYTMXD\Fu en Wang - 2022 - Convergence analysis of a quasi-Monte Carlo-based .pdf","","monte carlo; PDE; deep learning; ♥♥♥; quasi monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2210.16196","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V8LT2F5X","preprint","2022","Fang, Yuchen; Na, Sen; Mahoney, Michael W.; Kolar, Mladen","Fully Stochastic Trust-Region Sequential Quadratic Programming for Equality-Constrained Optimization Problems","","","","","http://arxiv.org/abs/2211.15943","We propose a trust-region stochastic sequential quadratic programming algorithm (TR-StoSQP) to solve nonlinear optimization problems with stochastic objectives and deterministic equality constraints. We consider a fully stochastic setting, where in each iteration a single sample is generated to estimate the objective gradient. The algorithm adaptively selects the trust-region radius and, compared to the existing line-search StoSQP schemes, allows us to employ indeﬁnite Hessian matrices (i.e., Hessians without modiﬁcation) in SQP subproblems. As a trust-region method for constrained optimization, our algorithm needs to address an infeasibility issue—the linearized equality constraints and trust-region constraints might lead to infeasible SQP subproblems. In this regard, we propose an adaptive relaxation technique to compute the trial step that consists of a normal step and a tangential step. To control the lengths of the two steps, we adaptively decompose the trust-region radius into two segments based on the proportions of the feasibility and optimality residuals to the full KKT residual. The normal step has a closed form, while the tangential step is solved from a trust-region subproblem, to which a solution ensuring the Cauchy reduction is suﬃcient for our study. We establish the global almost sure convergence guarantee for TR-StoSQP, and illustrate its empirical performance on both a subset of problems in the CUTEst test set and constrained logistic regression problems using data from the LIBSVM collection.","2022-11-29","2023-01-15 12:28:52","2024-07-29 13:32:52","2023-01-15 12:28:52","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.15943 [math, stat]","Comment: 6 figures, 28 pages","C:\Users\isido\Zotero\storage\HIZ86IVL\Fang e.a. - 2022 - Fully Stochastic Trust-Region Sequential Quadratic.pdf","","optimization; SGD; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2211.15943","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VN6R4RF","preprint","2019","Curtis, Frank E.; Shi, Rui","A Fully Stochastic Second-Order Trust Region Method","","","","","http://arxiv.org/abs/1911.06920","A stochastic second-order trust region method is proposed, which can be viewed as a second-order extension of the trust-region-ish (TRish) algorithm proposed by Curtis et al. [INFORMS J. Optim. 1(3) 200–220, 2019]. In each iteration, a search direction is computed by (approximately) solving a trust region subproblem deﬁned by stochastic gradient and Hessian estimates. The algorithm has convergence guarantees for stochastic minimization in the fully stochastic regime, meaning that guarantees hold when each stochastic gradient is required merely to be an unbiased estimate of the true gradient with bounded variance and when the stochastic Hessian estimates are bounded uniformly in norm. The algorithm is also equipped with a worst-case complexity guarantee in the nearly deterministic regime, i.e., when the stochastic gradient and Hessian estimates are very close in expectation to the true gradients and Hessians. The results of numerical experiments for training convolutional neural networks for image classiﬁcation and training a recurrent neural network for time series forecasting are presented. These results show that the algorithm can outperform a stochastic gradient approach and the ﬁrst-order TRish algorithm in practice.","2019-11-15","2023-01-15 12:31:06","2024-07-29 13:33:08","2023-01-15 12:31:06","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1911.06920 [math]","","C:\Users\isido\Zotero\storage\3QTRRJEB\Curtis en Shi - 2019 - A Fully Stochastic Second-Order Trust Region Metho.pdf","","optimization; SGD; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1911.06920","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GC9E4KQ3","journalArticle","2020","Chauhan, Vinod Kumar; Sharma, Anuj; Dahiya, Kalpana","Stochastic Trust Region Inexact Newton Method for Large-scale Machine Learning","International Journal of Machine Learning and Cybernetics","","1868-8071, 1868-808X","10.1007/s13042-019-01055-9","http://arxiv.org/abs/1812.10426","Nowadays stochastic approximation methods are one of the major research direction to deal with the large-scale machine learning problems. From stochastic ﬁrst order methods, now the focus is shifting to stochastic second order methods due to their faster convergence and availability of computing resources. In this paper, we have proposed a novel Stochastic Trust RegiOn Inexact Newton method, called as STRON, to solve large-scale learning problems which uses conjugate gradient (CG) to inexactly solve trust region subproblem. The method uses progressive subsampling in the calculation of gradient and Hessian values to take the advantage of both, stochastic and full-batch regimes. We have extended STRON using existing variance reduction techniques to deal with the noisy gradients and using preconditioned conjugate gradient (PCG) as subproblem solver, and empirically proved that they do not work as expected, for the large-scale learning problems. Finally, our empirical results prove eﬃcacy of the proposed method against existing methods with bench marked datasets.","2020-07","2023-01-15 12:31:26","2024-07-29 13:33:23","2023-01-15 12:31:26","1541-1555","","7","11","","Int. J. Mach. Learn. & Cyber.","","","","","","","","en","","","","","arXiv.org","","arXiv:1812.10426 [cs, stat]","Comment: 32 figures, accepted in International Journal of Machine Learning and Cybernetics","C:\Users\isido\Zotero\storage\7YWNREP2\Chauhan e.a. - 2020 - Stochastic Trust Region Inexact Newton Method for .pdf","","optimization; SGD; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBIYSWBJ","videoRecording","2021","Stand-up Maths","How does Dobble (Spot It) work?","","","","","https://www.youtube.com/watch?v=VTDKqW_GLkw","","2021-04-30","2023-01-16 13:29:01","2024-07-29 13:18:57","2023-01-16 13:29:01","","","","","","","","","","","","","","","","","","","YouTube","","","","","","♥♥♥","","","","","","","","","","","","","","","","","","","","","","28:36","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DUKLXYL","encyclopediaArticle","2022","","Aitken's delta-squared process","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Aitken%27s_delta-squared_process&oldid=1115576594","In numerical analysis, Aitken's delta-squared process or Aitken extrapolation is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken, who introduced this method in 1926. Its early form was known to Seki Kōwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of π. It is most useful for accelerating the convergence of a sequence that is converging linearly.","2022-10-12","2023-01-16 16:21:27","2024-07-29 13:18:10","2023-01-16 16:21:27","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1115576594","","C:\Users\isido\Zotero\storage\QPP3CBBR\Aitken's_delta-squared_process.html","","♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D76YE6FY","encyclopediaArticle","2022","","Runge's phenomenon","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Runge%27s_phenomenon&oldid=1127648067","In the mathematical field of numerical analysis, Runge's phenomenon (German: [ˈʁʊŋə]) is a problem of oscillation at the edges of an interval that occurs when using polynomial interpolation with polynomials of high degree over a set of equispaced interpolation points. It was discovered by Carl David Tolmé Runge (1901) when exploring the behavior of errors when using polynomial interpolation to approximate certain functions. The discovery was important because it shows that going to higher degrees does not always improve accuracy. The phenomenon is similar to the Gibbs phenomenon in Fourier series approximations.","2022-12-15","2023-01-18 20:57:45","2024-07-29 13:43:44","2023-01-18 20:57:44","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1127648067","","","","interpolation; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9A9Q5UIA","encyclopediaArticle","2022","","Gibbs phenomenon","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Gibbs_phenomenon&oldid=1120811333","In mathematics, the Gibbs phenomenon, discovered by Henry Wilbraham (1848)  and rediscovered by J. Willard Gibbs (1899), is the oscillatory behavior of the Fourier series of a piecewise continuously differentiable periodic function around a jump discontinuity. The function's                         N                 {\displaystyle N}   th partial Fourier series (formed by summing its                         N                 {\displaystyle N}    lowest constituent sinusoids) produces large peaks around the jump which overshoot and undershoot the function's actual values. This approximation error approaches a limit of about 9% of the jump as more sinusoids are used, though the infinite Fourier series sum does eventually converge almost everywhere except the point of discontinuity.The Gibbs phenomenon was observed by experimental physicists, but was believed to be due to imperfections in the measuring apparatus, and it is one cause of ringing artifacts in signal processing.","2022-11-08","2023-01-18 20:57:51","2024-07-29 13:44:00","2023-01-18 20:57:51","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1120811333","","C:\Users\isido\Zotero\storage\H38JL4E9\Gibbs_phenomenon.html","","♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IGKF6LI","preprint","2022","Penent, Guillaume; Privault, Nicolas","Numerical evaluation of ODE solutions by Monte Carlo enumeration of Butcher series","","","","","http://arxiv.org/abs/2201.05998","We present an algorithm for the numerical solution of ordinary diﬀerential equations by random enumeration of the Butcher trees used in the implementation of the RungeKutta method. Our Monte Carlo scheme allows for the direct numerical evaluation of an ODE solution at any given time within a certain interval, without iteration through multiple time steps. In particular, this approach does not involve a discretization step size, and it does not require the truncation of Taylor series.","2022-08-24","2023-01-20 09:27:05","2024-07-29 13:53:44","2023-01-20 09:27:05","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2201.05998 [cs, math]","","C:\Users\isido\Zotero\storage\UXRFJ68P\Penent en Privault - 2022 - Numerical evaluation of ODE solutions by Monte Car.pdf","","monte carlo; ODE; green function; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2201.05998","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HXPC7H7","journalArticle","2006","Mori, Masatake; Echigo, Toshihiko","Numerical Green’s function method based on the DE transformation","Japan Journal of Industrial and Applied Mathematics","","0916-7005, 1868-937X","10.1007/BF03167550","http://link.springer.com/10.1007/BF03167550","A method for numerical solution of boundary value problems with ordinary diﬀerential equation based on the method of Green’s function incorporated with the double exponential transformation is presented. The method proposed does not require solving a system of linear equations and gives an approximate solution of very high accuracy with a small number of function evaluations. The error of the method is O (exp (−C1N/ log(C2N ))) where N is a parameter representing the number of function evaluations and C1 and C2 are some positive constants. Numerical examples also prove the high eﬃciency of the method. An alternative method via an integral equation is presented which can be used when the Green’s function corresponding to the given equation is not available.","2006-06","2023-01-20 09:42:40","2024-07-29 13:43:00","2023-01-20 09:42:40","193-205","","2","23","","Japan J. Indust. Appl. Math.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\U5ZLG7FE\Mori en Echigo - 2006 - Numerical Green’s function method based on the DE .pdf","","green function; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UA5NRBZW","journalArticle","2019","Ermakov, S. M.; Tovstik, T. M.","Monte Carlo Method for Solving ODE Systems","Vestnik St. Petersburg University, Mathematics","","1063-4541, 1934-7855","10.1134/S1063454119030087","https://link.springer.com/10.1134/S1063454119030087","The Monte Carlo method is applied to solve Cauchy problems for a system of linear and nonlinear ordinary differential equations. The Monte Carlo method is relevant for the solution of large systems of equations and in the case of small smoothness of initial functions. In this case, the system is reduced to an equivalent system of integral equations of the Volterra type. For linear systems, this transformation allows removing constraints connected with a convergence of a majorizing process. Examples of estimates of solution functionals are provided, and a behavior of their variances are discussed. In the general case, a solution interval is divided into finite subintervals, on which the nonlinear function is approximated by a polynomial. The obtained integral equation is solved by using branched Markov chains with absorption. Algorithm parallelization problems arising in this case are discussed in this paper. A one-dimensional cubic equation is considered as an example. A choice of transition densities of branching is discussed. A method of generations is described in detail. Numerical results are compared with a solution obtained by the Runge–Kutta method.","2019-07","2023-01-20 09:43:02","2024-07-29 13:42:31","2023-01-20 09:43:02","272-280","","3","52","","Vestnik St.Petersb. Univ.Math.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\UM598GXH\Ermakov en Tovstik - 2019 - Monte Carlo Method for Solving ODE Systems.pdf","","monte carlo; ODE; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JG29V6PV","journalArticle","2021","Ermakov, S. M.; Smilovitskiy, M. G.","The Monte Carlo Method for Solving Large Systems of Linear Ordinary Differential Equations","Vestnik St. Petersburg University, Mathematics","","1063-4541, 1934-7855","10.1134/S1063454121010064","https://link.springer.com/10.1134/S1063454121010064","The Monte Carlo method to solve the Cauchy problem for large systems of linear differential equations is proposed in this paper. Firstly, a quick overview of previously obtained results from applying the approach towards the Fredholm-type integral equations is made. In the main part of the paper, the method is applied towards a linear ODE system that is transformed into an equivalent system of the Volterra-type integral equations, which makes it possible to remove the limitations due to the conditions of convergence of the majorant series. The following key theorems are stated. Theorem 1 provides the necessary compliance conditions that should be imposed upon the transition propability and initial distribution densities that initiate the corresponding Markov chain, for which equality between the mathematical expectation of the estimate and the functional of interest would hold. Theorem 2 formulates the equation that governs the estimate’s variance. Theorem 3 states the Markov chain parameters that minimize the variance of the estimate of the functional. Proofs are given for all three theorems. In the practical part of this paper, the proposed method is used to solve a linear ODE system that describes a closed queueing system of ten conventional machines and seven conventional service persons. The solutions are obtained for systems with both constant and time-dependent matrices of coefficients, where the machine breakdown intensity is time dependent. In addition, the solutions obtained by the Monte Carlo and Runge–Kutta methods are compared. The results are presented in the corresponding tables.","2021-01","2023-01-20 09:55:47","2024-07-29 13:42:21","2023-01-20 09:55:47","28-38","","1","54","","Vestnik St.Petersb. Univ.Math.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\EXRTHDXP\Ermakov en Smilovitskiy - 2021 - The Monte Carlo Method for Solving Large Systems o.pdf","","monte carlo; linear systems; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULM9AGUM","journalArticle","2006","Halton, John H","Sequential Monte Carlo techniques for solving non-linear systems","Linear Systems","","","","","Given a system of m equations F(x ) = 0 (where m is large and x is an unknown m-vector), we seek to apply sequential Monte Carlo [SMC] methods to find solutions efficiently. This paper follows up on a previous paper by the same author, in which consideration was limited to linear systems of the form Ax = a (where, again, m is large, A is a known (m¥m) matrix, a is a known m-vector, and x is an unknown m-vector). It was shown there that effective techniques could reduce computation times dramatically (speed-up factors of 550 to 26,000 were obtained in sample calculations).","2006-05-10","2023-01-20 18:11:24","2024-08-12 14:24:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\GNEKPJHM\Halton - Sequential Monte Carlo techniques for solving non-.pdf","","♥; monte carlo; nonlinear systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSK2T2V4","journalArticle","1974","Nekrutkin, V.V.","Direct and conjugate Neumann-Ulam schemes for solving non-linear integral equations","USSR Computational Mathematics and Mathematical Physics","","00415553","10.1016/0041-5553(74)90167-0","https://linkinghub.elsevier.com/retrieve/pii/0041555374901670","ELEMENTARY unbiased estimates are constructed for a linear functional of the solution of a non-linear integral equation of fairly general type. The method of constructing the estimates, which is based on the “equivalence” of the initial equation to an infinite system of linear equations, makes it possible to transfer to the so-called conjugate Neumann-Ulam scheme, which can prove more advantageous when solving physical problems.","1974-01","2023-01-20 18:23:26","2024-07-29 13:41:19","2023-01-20 18:23:26","39-45","","6","14","","USSR Computational Mathematics and Mathematical Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\48XNIP74\Nekrutkin - 1974 - Direct and conjugate Neumann-Ulam schemes for solv.pdf","","integral equations; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NW85UZ53","document","1972","","THE NEYMAN-ULAM SCHEME IN THE NON-LINEAR CASE*","","","","","","A COMPUTATIONAL scheme of the Monte Carlo method for solving a non-linear integral equation of the form N cp(x)= SCKits, Y)TJ”(~)~Y + f(x), D i-i connected with a branching Markov process is presented. The estimation of the functional J-4 (x)h (3c)dx on the trajectories of the process is considered and its D variance is studied. At the present time the solution by the Monte Carlo method of the non-linear integral equations describing complex physical processes, involves their linearization. The solution of a non-linear equation is represented as the limit of a sequence of solutions of linear equations, which are found by the Monte Carlo method. This approach is described in [l], and its applications, in a whole series of papers (for example, [2, 31). However, it is easy to see that thereby the fundamental advantages of the Monte Carlo method are lost, since the calculations require the storage of the values of the functions which are the solutions of the linear equations, and the analysis of the errors is made extremely complicated [4].","1972-02-24","2023-01-20 18:24:16","2024-08-12 14:59:41","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\SSN79QHC\1-s2.0-0041555373900980-main.pdf","","♥; monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZB5PTJDF","encyclopediaArticle","2022","","Exponential integrator","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Exponential_integrator&oldid=1123597413","Exponential integrators are a class of numerical methods for the solution of ordinary differential equations, specifically initial value problems.  This large class of methods from numerical analysis is based on the exact integration of the linear part of the initial value problem. Because the linear part is integrated exactly, this can help to mitigate the stiffness of a differential equation. Exponential integrators can be constructed to be explicit or implicit for numerical ordinary differential equations or serve as the time integrator for numerical partial differential equations.","2022-11-24","2023-01-22 21:01:01","2024-07-29 13:40:15","2023-01-22 21:01:01","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1123597413","","C:\Users\isido\Zotero\storage\XH6LIQZ3\Exponential_integrator.html","","ODE; exponential integrators; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJB4PHU9","journalArticle","2005","Hochbruck, Marlis; Ostermann, Alexander","Exponential Runge–Kutta methods for parabolic problems","Applied Numerical Mathematics","","01689274","10.1016/j.apnum.2004.08.005","https://linkinghub.elsevier.com/retrieve/pii/S0168927404001400","The aim of this paper is to construct exponential Runge-Kutta methods of collocation type and to analyze their convergence properties for linear and semilinear parabolic problems. For the analysis, an abstract Banach space framework of sectorial operators and locally Lipschitz continuous nonlinearities is chosen. This framework includes interesting examples like reaction-diﬀusion equations. It is shown that the methods converge at least with their stage order, and that convergence of higher order (up to the classical order) occurs, if the problem has suﬃcient temporal and spatial smoothness. The latter, however, might require the source function to fulﬁl unnatural boundary conditions. Therefore, the classical order is not always obtained and an order reduction must be expected, in general.","2005-05","2023-01-22 21:12:12","2024-07-30 10:41:12","2023-01-22 21:12:12","323-339","","2-4","53","","Applied Numerical Mathematics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\NHR96XVY\Hochbruck en Ostermann - 2005 - Exponential Runge–Kutta methods for parabolic prob.pdf","","ODE; exponential integrators; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5ZS76WE","journalArticle","2010","Hochbruck, Marlis; Ostermann, Alexander","Exponential integrators","Acta Numerica","","0962-4929, 1474-0508","10.1017/S0962492910000048","https://www.cambridge.org/core/product/identifier/S0962492910000048/type/journal_article","In this paper we consider the construction, analysis, implementation and application of exponential integrators. The focus will be on two types of stiff problems. The first one is characterized by a Jacobian that possesses eigenvalues with large negative real parts. Parabolic partial differential equations and their spatial discretization are typical examples. The second class consists of highly oscillatory problems with purely imaginary eigenvalues of large modulus. Apart from motivating the construction of exponential integrators for various classes of problems, our main intention in this article is to present the mathematics behind these methods. We will derive error bounds that are independent of stiffness or highest frequencies in the system.             Since the implementation of exponential integrators requires the evaluation of the product of a matrix function with a vector, we will briefly discuss some possible approaches as well. The paper concludes with some applications, in which exponential integrators are used.","2010-05","2023-01-23 20:33:08","2024-07-30 10:58:11","2023-01-23 20:33:08","209-286","","","19","","Acta Numerica","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>we should read this if we ever going to use exponential integrators</p> </div>","C:\Users\isido\Zotero\storage\C73HIPVU\Hochbruck en Ostermann - 2010 - Exponential integrators.pdf","","ODE; exponential integrators; ♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5F4A5X4","journalArticle","2000","Bergamaschi, Luca; Vianello, Marco","Efficient computation of the exponential operator for large, sparse, symmetric matrices","Numerical Linear Algebra with Applications","","1070-5325, 1099-1506","10.1002/(SICI)1099-1506(200001/02)7:1<27::AID-NLA185>3.0.CO;2-4","https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-1506(200001/02)7:1<27::AID-NLA185>3.0.CO;2-4","In this paper we compare Krylov subspace methods with Chebyshev series expansion for approximating the matrix exponential operator on large, sparse, symmetric matrices. Experimental results upon negative-deﬁnite matrices with very large size, arising from (2D and 3D) FE and FD spatial discretization of linear parabolic PDEs, demonstrate that the Chebyshev method can be an effective alternative to Krylov techniques, especially when memory bounds do not allow the storage of all Ritz vectors. We discuss also sensitivity of Chebyshev convergence to extreme eigenvalue approximation, as well as reliability of various a priori and a posteriori error estimates for both methods.","2000-01","2023-01-23 20:58:20","2024-07-30 14:16:04","2023-01-23 20:58:20","27-45","","1","7","","Numer. Linear Algebra Appl.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\5HP78YEJ\Bergamaschi en Vianello - 2000 - Efficient computation of the exponential operator .pdf","","exponential integrators; matrix exponential; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGQ3JN9R","encyclopediaArticle","2022","","Padé approximant","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Pad%C3%A9_approximant&oldid=1123396275","In mathematics, a Padé approximant is the ""best"" approximation of a function near a specific point by a rational function of given order. Under this technique, the approximant's power series agrees with the power series of the function it is approximating.  The technique was developed around 1890 by Henri Padé, but goes back to Georg Frobenius, who introduced the idea and investigated the features of rational approximations of power series. The Padé approximant often gives better approximation of the function than truncating its Taylor series, and it may still work where the Taylor series does not converge. For these reasons Padé approximants are used extensively in computer calculations. They have also been used as auxiliary functions in Diophantine approximation and transcendental number theory, though for sharp results ad hoc methods— in some sense inspired by the Padé theory— typically replace them. Since Padé approximant is a rational function, an artificial singular point may occur as an approximation, but this can be avoided by Borel–Padé analysis. The reason why the Padé approximant tends to be a better approximation than a truncating Taylor series is clear from the viewpoint of the multi-point summation method. Since there are many cases in which the asymptotic expansion at infinity becomes 0 or a constant, it can be interpreted as the ""incomplete two-point Padé approximation"", in which the ordinary Padé approximation improves the method truncating a Taylor series.","2022-11-23","2023-01-23 21:13:48","2024-07-30 14:15:41","2023-01-23 21:13:48","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1123396275","","C:\Users\isido\Zotero\storage\KVILJA4H\Padé_approximant.html","","♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFXB57DW","journalArticle","2014","Sastre, J.; Ibáñez, J.; Ruiz, P.; Defez, E.","Accurate and efficient matrix exponential computation","International Journal of Computer Mathematics","","0020-7160, 1029-0265","10.1080/00207160.2013.791392","http://www.tandfonline.com/doi/abs/10.1080/00207160.2013.791392","This work gives a new formula for the forward relative error of matrix exponential Taylor approximation and proposes new bounds for it depending on the matrix size and the Taylor approximation order, providing a new efficient scaling and squaring Taylor algorithm for the matrix exponential. A Matlab version of the new algorithm is provided and compared with Pade  ́ state-of-the-art algorithms obtaining higher accuracy in the majority of tests at similar or even lower cost.","2014-01-02","2023-01-23 21:18:10","2024-07-30 14:22:16","2023-01-23 21:18:10","97-112","","1","91","","International Journal of Computer Mathematics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\QEY4VDFZ\Sastre e.a. - 2014 - Accurate and efficient matrix exponential computat.pdf","","exponential integrators; matrix exponential; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4MW9JWQK","encyclopediaArticle","2022","","Matrix exponential","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Matrix_exponential&oldid=1122134034","In mathematics, the matrix exponential is a matrix function on square matrices analogous to the ordinary exponential function. It is used to solve systems of linear differential equations. In the theory of Lie groups, the matrix exponential gives the exponential map between a matrix Lie algebra and the corresponding Lie group. Let X  be an n×n real or complex matrix. The exponential of X, denoted by eX or exp(X), is the n×n matrix given by the power series where                                    X                        0                                     {\displaystyle X^{0}}    is defined to be the identity matrix                         I                 {\displaystyle I}    with the same dimensions as                         X                 {\displaystyle X}   .The above series always converges, so the exponential of X is well-defined. If X is a 1×1 matrix the matrix exponential of X is a 1×1 matrix whose single element is the ordinary exponential of the single element of X.","2022-11-16","2023-01-23 21:25:25","2024-07-30 14:23:06","2023-01-23 21:25:25","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1122134034","","","","exponential integrators; ♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"325A7RN6","journalArticle","2022","Ermakov, S. M.; Surovikina, T. O.","Backward Iterations for Solving Integral Equations with Polynomial Nonlinearity","Vestnik St. Petersburg University, Mathematics","","1063-4541, 1934-7855","10.1134/S1063454122010046","https://link.springer.com/10.1134/S1063454122010046","The theory of adjoint operators is widely used in solving applied multidimensional problems with the Monte Carlo method. Efficient algorithms are constructed using the duality principle for many problems described in linear integral equations of the second kind. On the other hand, important applications of adjoint equations for designing experiments were suggested by G.I. Marchuk and his colleagues in their respective works. Some results obtained in these fields are also generalized to the case of nonlinear operators. Linearization methods are mostly used for that purpose. The results for Lyapunov–Schmidt nonlinear polynomial equations are obtained in the theory of Monte Carlo methods. However, many interesting questions in this subject area remain open. New results about dual processes used for solving polynomial equations with the Monte Carlo method are presented. In particular, the adjoint Markov process for the branching process and corresponding unbiased estimate of the functional of the solution to the equation are constructed in the general form. The possibility of constructing an adjoint operator to a nonlinear one is discussed.","2022-03","2023-01-24 15:09:45","2024-07-30 14:24:20","2023-01-24 15:09:45","16-26","","1","55","","Vestnik St.Petersb. Univ.Math.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\MGBUUSKA\Ermakov en Surovikina - 2022 - Backward Iterations for Solving Integral Equations.pdf","","integral equations; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBNSUT7K","preprint","2023","Li, Yongming; Neufeld, Ariel","Quantum Monte Carlo algorithm for solving Black-Scholes PDEs for high-dimensional option pricing in finance and its proof of overcoming the curse of dimensionality","","","","","http://arxiv.org/abs/2301.09241","In this paper we provide a quantum Monte Carlo algorithm to solve high-dimensional Black-Scholes PDEs with correlation for high-dimensional option pricing. The payoﬀ function of the option is of general form and is only required to be continuous and piece-wise aﬃne (CPWA), which covers most of the relevant payoﬀ functions used in ﬁnance. We provide a rigorous error analysis and complexity analysis of our algorithm. In particular, we prove that the computational complexity of our algorithm is bounded polynomially in the space dimension d of the PDE and the reciprocal of the prescribed accuracy ε and so demonstrate that our quantum Monte Carlo algorithm does not suﬀer from the curse of dimensionality.","2023-01-22","2023-01-24 15:53:34","2024-07-30 14:36:12","2023-01-24 15:53:34","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.09241 [quant-ph, q-fin]","","C:\Users\isido\Zotero\storage\2UE3LKA6\Li en Neufeld - 2023 - Quantum Monte Carlo algorithm for solving Black-Sc.pdf","","monte carlo; ODE; ♥; quantum computing","","","","","","","","","","","","","","","","","","","","arXiv:2301.09241","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZH2DB9F2","encyclopediaArticle","2022","","Stochastic partial differential equation","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Stochastic_partial_differential_equation&oldid=1129102419","Stochastic partial differential equations (SPDEs) generalize partial differential equations via random force terms and coefficients, in the same way ordinary stochastic differential equations generalize ordinary differential equations. They have relevance to quantum field theory, statistical mechanics, and spatial modeling.","2022-12-23","2023-01-24 16:02:55","2024-07-30 14:28:50","2023-01-24 16:02:55","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1129102419","","C:\Users\isido\Zotero\storage\KXX22C9Y\Stochastic_partial_differential_equation.html","","PDE; random PDE; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HL55QPUC","preprint","2022","Becker, Sebastian; Jentzen, Arnulf; Müller, Marvin S.; von Wurstemberger, Philippe","Learning the random variables in Monte Carlo simulations with stochastic gradient descent: Machine learning for parametric PDEs and financial derivative pricing","","","","","http://arxiv.org/abs/2202.02717","In financial engineering, prices of financial products are computed approximately many times each trading day with (slightly) different parameters in each calculation. In many financial models such prices can be approximated by means of Monte Carlo (MC) simulations. To obtain a good approximation the MC sample size usually needs to be considerably large resulting in a long computing time to obtain a single approximation. In this paper we introduce a new approximation strategy for parametric approximation problems including the parametric financial pricing problems described above. A central aspect of the approximation strategy proposed in this article is to combine MC algorithms with machine learning techniques to, roughly speaking, learn the random variables (LRV) in MC simulations. In other words, we employ stochastic gradient descent (SGD) optimization methods not to train parameters of standard artificial neural networks (ANNs) but to learn random variables appearing in MC approximations. We numerically test the LRV strategy on various parametric problems with convincing results when compared with standard MC simulations, Quasi-Monte Carlo simulations, SGD-trained shallow ANNs, and SGD-trained deep ANNs. Our numerical simulations strongly indicate that the LRV strategy might be capable to overcome the curse of dimensionality in the $L^\infty$-norm in several cases where the standard deep learning approach has been proven not to be able to do so. This is not a contradiction to lower bounds established in the scientific literature because this new LRV strategy is outside of the class of algorithms for which lower bounds have been established in the scientific literature. The proposed LRV strategy is of general nature and not only restricted to the parametric financial pricing problems described above, but applicable to a large class of approximation problems.","2022-02-06","2023-01-24 16:21:57","2024-07-30 15:21:11","2023-01-24 16:21:57","","","","","","","Learning the random variables in Monte Carlo simulations with stochastic gradient descent","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2202.02717 [cs, math]","<div data-schema-version=""8""><p>love the idea, have read the details yet 30-07-24</p> </div>","C:\Users\isido\Zotero\storage\ZT28NZYQ\Becker e.a. - 2022 - Learning the random variables in Monte Carlo simul.pdf","","monte carlo; PDE; optimization; machine learning; SGD; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2202.02717","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DEIPCR4","preprint","2022","Nguwi, Jiang Yu; Penent, Guillaume; Privault, Nicolas","A fully nonlinear Feynman-Kac formula with derivatives of arbitrary orders","","","","","http://arxiv.org/abs/2201.03882","We present an algorithm for the numerical solution of nonlinear parabolic partial diﬀerential equations. This algorithm extends the classical Feynman-Kac formula to fully nonlinear partial diﬀerential equations, by using random trees that carry information on nonlinearities on their branches. It applies to functional, non-polynomial nonlinearities that are not treated by standard branching arguments, and deals with derivative terms of arbitrary orders. A Monte Carlo numerical implementation is provided.","2022-12-14","2023-01-24 16:29:09","2024-07-30 20:26:55","2023-01-24 16:29:09","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2201.03882 [math]","<div data-schema-version=""8""><p>dont understand it but looks important 30-07-24</p> </div>","C:\Users\isido\Zotero\storage\XRMIKRSC\Nguwi e.a. - 2022 - A fully nonlinear Feynman-Kac formula with derivat.pdf","","monte carlo; PDE; ♥♥♥; branching","","","","","","","","","","","","","","","","","","","","arXiv:2201.03882","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYJ9QE2D","encyclopediaArticle","2023","","Branching process","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Branching_process&oldid=1131945188","In probability theory, a branching process is a type of mathematical object known as a stochastic process, which consists of collections of random variables. The random variables of a stochastic process are indexed by the natural numbers. The original purpose of branching processes was to serve as a mathematical model of a population in which each individual in generation                         n                 {\displaystyle n}    produces some random number of individuals in generation                         n         +         1                 {\displaystyle n+1}   , according, in the simplest case, to a fixed probability distribution that does not vary from individual to individual. Branching processes are used to model reproduction; for example, the individuals might correspond to bacteria, each of which generates 0, 1, or 2 offspring with some probability in a single time unit.  Branching processes can also be used to model other systems with similar dynamics, e.g., the spread of surnames in genealogy or the propagation of neutrons in a nuclear reactor. A central question in the theory of branching processes is the probability of ultimate extinction, where no individuals exist after some finite number of generations.  Using Wald's equation, it can be shown that starting with one individual in generation zero, the expected size of generation n equals μn where μ is the expected number of children of each individual.  If μ < 1, then the expected number of individuals goes rapidly to zero, which implies ultimate extinction with probability 1 by Markov's inequality.  Alternatively, if μ > 1, then the probability of ultimate extinction is less than 1 (but not necessarily zero; consider a process where each individual either has 0 or 100 children with equal probability. In that case, μ = 50, but probability of ultimate extinction is greater than 0.5, since that's the probability that the first individual has 0 children).  If μ = 1, then ultimate extinction occurs with probability 1 unless each individual always has exactly one child. In theoretical ecology, the parameter μ of a branching process is called the basic reproductive rate.","2023-01-06","2023-01-24 16:43:50","2024-07-30 16:22:06","2023-01-24 16:43:50","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1131945188","","C:\Users\isido\Zotero\storage\NDK5UH98\Branching_process.html","","♥; branching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGVPH3T5","preprint","2016","Henry-Labordere, Pierre; Oudjane, Nadia; Tan, Xiaolu; Touzi, Nizar; Warin, Xavier","Branching diffusion representation of semilinear PDEs and Monte Carlo approximation","","","","","http://arxiv.org/abs/1603.01727","We provide a representation result of parabolic semi-linear PD-Es, with polynomial nonlinearity, by branching diﬀusion processes. We extend the classical representation for KPP equations, introduced by Skorokhod [23], Watanabe [27] and McKean [18], by allowing for polynomial nonlinearity in the pair (u, Du), where u is the solution of the PDE with space gradient Du. Similar to the previous literature, our result requires a non-explosion condition which restrict to “small maturity” or “small nonlinearity” of the PDE. Our main ingredient is the automatic diﬀerentiation technique as in [15], based on the Malliavin integration by parts, which allows to account for the nonlinearities in the gradient. As a consequence, the particles of our branching diﬀusion are marked by the nature of the nonlinearity. This new representation has very important numerical implications as it is suitable for Monte Carlo simulation. Indeed, this provides the ﬁrst numerical method for high dimensional nonlinear PDEs with error estimate induced by the dimension-free Central limit theorem. The complexity is also easily seen to be of the order of the squared dimension. The ﬁnal section of this paper illustrates the eﬃciency of the algorithm by some high dimensional numerical experiments.","2016-03-05","2023-01-24 16:53:08","2024-07-30 16:22:11","2023-01-24 16:53:08","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1603.01727 [math]","","C:\Users\isido\Zotero\storage\6HC4YVGI\Henry-Labordere e.a. - 2016 - Branching diffusion representation of semilinear P.pdf","","monte carlo; PDE; ♥; branching","","","","","","","","","","","","","","","","","","","","arXiv:1603.01727","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MV22E3D5","preprint","2022","Hout, Karel in 't; Lamotte, Pieter","Efficient numerical valuation of European options under the two-asset Kou jump-diffusion model","","","","","http://arxiv.org/abs/2207.10060","This paper concerns the numerical solution of the two-dimensional time-dependent partial integro-diﬀerential equation (PIDE) that holds for the values of European-style options under the two-asset Kou jump-diﬀusion model. A main feature of this equation is the presence of a nonlocal double integral term. For its numerical evaluation, we extend a highly eﬃcient algorithm derived by Toivanen [30] in the case of the one-dimensional Kou integral. The acquired algorithm for the two-dimensional Kou integral has optimal computational cost: the number of basic arithmetic operations is directly proportional to the number of spatial grid points in the semidiscretization. For the eﬀective discretization in time, we study seven contemporary operator splitting schemes of the implicit-explicit (IMEX) and the alternating direction implicit (ADI) kind. All these schemes allow for a convenient, explicit treatment of the integral term. By ample numerical experiments for put-on-the-average option values, the stability and convergence behaviour as well as the mutual performance of the seven operator splitting schemes are investigated. Moreover, the Greeks Delta and Gamma are considered.","2022-07-20","2023-01-24 17:20:25","2024-07-30 16:23:25","2023-01-24 17:20:25","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2207.10060 [cs, math, q-fin]","Comment: arXiv admin note: text overlap with arXiv:1901.03839","C:\Users\isido\Zotero\storage\65FUB6WN\Hout en Lamotte - 2022 - Efficient numerical valuation of European options .pdf","","PDE; ♥; IMEX; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2207.10060","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4J4JQIU6","journalArticle","2021","Černý, Aleš; Ruf, Johannes","Simplified stochastic calculus with applications in Economics and Finance","European Journal of Operational Research","","03772217","10.1016/j.ejor.2020.12.037","http://arxiv.org/abs/1912.03651","The paper introduces a simple way of recording and manipulating general stochastic processes without explicit reference to a probability measure. In the new calculus, operations traditionally presented in a measure-speciﬁc way are instead captured by tracing the behaviour of jumps (also when no jumps are physically present). The calculus is fail-safe in that, under minimal assumptions, all informal calculations yield mathematically well-deﬁned stochastic processes. The calculus is also intuitive as it allows the user to pretend all jumps are of compound Poisson type. The new calculus is very eﬀective when it comes to computing drifts and expected values that possibly involve a change of measure. Such drift calculations yield, for example, partial integro–diﬀerential equations, Hamilton–Jacobi–Bellman equations, Feynman–Kac formulae, or exponential moments needed in numerous applications. We provide several illustrations of the new technique, among them a novel result on the Margrabe option to exchange one defaultable asset for another.","2021-09","2023-01-24 17:55:38","2024-07-31 13:02:10","2023-01-24 17:55:38","547-560","","2","293","","European Journal of Operational Research","","","","","","","","en","","","","","arXiv.org","","arXiv:1912.03651 [math, q-fin]","","C:\Users\isido\Zotero\storage\7NICUN3Z\Černý en Ruf - 2021 - Simplified stochastic calculus with applications i.pdf","","SDE; ♥♥♥♥♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7FY9MKB5","preprint","2021","Blechschmidt, Jan; Ernst, Oliver G.","Three Ways to Solve Partial Differential Equations with Neural Networks -- A Review","","","","","http://arxiv.org/abs/2102.11802","Neural networks are increasingly used to construct numerical solution methods for partial diﬀerential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman-Kac formula and methods based on the solution of backward stochastic diﬀerential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.","2021-04-14","2023-01-24 17:59:17","2024-07-30 19:55:59","2023-01-24 17:59:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2102.11802 [cs, math]","Comment: 32 pages; for associated Jupyter notebooks, see https://github.com/janblechschmidt/PDEsByNNs","C:\Users\isido\Zotero\storage\3CQBUAIB\Blechschmidt en Ernst - 2021 - Three Ways to Solve Partial Differential Equations.pdf","","PDE; machine learning; deep learning; ♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2102.11802","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SBPV3MJC","journalArticle","2019","E, Weinan; Hutzenthaler, Martin; Jentzen, Arnulf; Kruse, Thomas","On multilevel Picard numerical approximations for high-dimensional nonlinear parabolic partial differential equations and high-dimensional nonlinear backward stochastic differential equations","Journal of Scientific Computing","","0885-7474, 1573-7691","10.1007/s10915-018-00903-0","http://arxiv.org/abs/1708.03223","Parabolic partial diﬀerential equations (PDEs) and backward stochastic diﬀerential equations (BSDEs) are key ingredients in a number of models in physics and ﬁnancial engineering. In particular, parabolic PDEs and BSDEs are fundamental tools in the state-of-the-art pricing and hedging of ﬁnancial derivatives. The PDEs and BSDEs appearing in such applications are often high-dimensional and nonlinear. Since explicit solutions of such PDEs and BSDEs are typically not available, it is a very active topic of research to solve such PDEs and BSDEs approximately. In the recent article [E, W., Hutzenthaler, M., Jentzen, A., & Kruse, T. Linear scaling algorithms for solving high-dimensional nonlinear parabolic diﬀerential equations. arXiv:1607.03295 (2017)] we proposed a family of approximation methods based on Picard approximations and multilevel Monte Carlo methods and showed under suitable regularity assumptions on the exact solution for semilinear heat equations that the computational complexity is bounded by O(d ε−(4+δ)) for any δ ∈ (0, ∞), where d is the dimensionality of the problem and ε ∈ (0, ∞) is the prescribed accuracy. In this paper, we test the applicability of this algorithm on a variety of 100-dimensional nonlinear PDEs that arise in physics and ﬁnance by means of numerical simulations presenting approximation accuracy against runtime. The simulation results for these 100-dimensional example PDEs are very satisfactory in terms of accuracy and speed. In addition, we also provide a review of other approximation methods for nonlinear PDEs and BSDEs from the literature.","2019-06","2023-01-24 21:08:08","2024-07-30 20:18:57","2023-01-24 21:08:08","1534-1571","","3","79","","J Sci Comput","","","","","","","","en","","","","","arXiv.org","","arXiv:1708.03223 [math]","","C:\Users\isido\Zotero\storage\GNSPNCPT\E e.a. - 2019 - On multilevel Picard numerical approximations for .pdf","","monte carlo; PDE; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6FCILPQL","preprint","2017","Warin, Xavier","Variations on branching methods for non linear PDEs","","","","","http://arxiv.org/abs/1701.07660","The branching methods developed in [9], [11] are eﬀective methods to solve some semi linear PDEs and are shown numerically to be able to solve some full non linear PDEs. These methods are however restricted to some small coeﬃcients in the PDE and small maturities. This article shows numerically that these methods can be adapted to solve the problems with longer maturities in the semi-linear case by using a new derivation scheme and some nested method. As for the case of full non linear PDEs, we introduce new schemes and we show numerically that they provide an eﬀective alternative to the schemes previously developed.","2017-01-26","2023-01-24 21:08:46","2024-07-30 20:19:48","2023-01-24 21:08:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1701.07660 [math]","Comment: 25 pages","C:\Users\isido\Zotero\storage\CK46H39I\Warin - 2017 - Variations on branching methods for non linear PDE.pdf","","monte carlo; PDE; ♥; branching","","","","","","","","","","","","","","","","","","","","arXiv:1701.07660","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"434UNRWM","preprint","2018","Warin, Xavier","Nesting Monte Carlo for high-dimensional Non Linear PDEs","","","","","http://arxiv.org/abs/1804.08432","A new method based on nesting Monte Carlo is developed to solve highdimensional semi-linear PDEs. Convergence of the method is proved and its convergence rate studied. Results in high dimension for diﬀerent kind of non-linearities show its eﬃciency.","2018-05-14","2023-01-24 21:09:57","2024-07-30 20:21:21","2023-01-24 21:09:57","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1804.08432 [math]","Comment: 35 pages","C:\Users\isido\Zotero\storage\LV4I2MV7\Warin - 2018 - Nesting Monte Carlo for high-dimensional Non Linea.pdf","","monte carlo; PDE; ♥; branching","","","","","","","","","","","","","","","","","","","","arXiv:1804.08432","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KE5VI8Y4","preprint","2018","Agarwal, Ankush; Claisse, Julien","Branching diffusion representation of semi-linear elliptic PDEs and estimation using Monte Carlo method","","","","","http://arxiv.org/abs/1704.00328","We study semi-linear elliptic PDEs with polynomial non-linearity and provide a probabilistic representation of their solution using branching diﬀusion processes. When the non-linearity involves the unknown function but not its derivatives, we extend previous results in the literature by showing that our probabilistic representation provides a solution to the PDE without assuming its existence. In the general case, we derive a new representation of the solution by using marked branching diﬀusion processes and automatic diﬀerentiation formulas to account for the non-linear gradient term. In both cases, we develop new theoretical tools to provide explicit suﬃcient conditions under which our probabilistic representations hold. As an application, we consider several examples including multi-dimensional semi-linear elliptic PDEs and estimate their solution by using the Monte Carlo method.","2018-02-14","2023-01-24 21:10:43","2024-07-30 20:21:40","2023-01-24 21:10:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1704.00328 [math]","","C:\Users\isido\Zotero\storage\A5D35SX4\Agarwal en Claisse - 2018 - Branching diffusion representation of semi-linear .pdf","","monte carlo; PDE; ♥; branching","","","","","","","","","","","","","","","","","","","","arXiv:1704.00328","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AAPRP6B9","encyclopediaArticle","2020","","Predictor–corrector method","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Predictor%E2%80%93corrector_method&oldid=955920527","In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equations – to find an unknown function that satisfies a given differential equation.  All such algorithms proceed in two steps:  The initial, ""prediction"" step, starts from a function fitted to the function-values and derivative-values at a preceding set of points to extrapolate (""anticipate"") this function's value at a subsequent, new point. The next, ""corrector"" step refines the initial approximation by using the predicted value of the function and another method to interpolate that unknown function's value at the same subsequent point.","2020-05-10","2023-01-25 10:02:54","2024-07-30 20:23:14","2023-01-25 10:02:54","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 955920527","","C:\Users\isido\Zotero\storage\8KLI757A\Predictor–corrector_method.html","","ODE; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLZHXC86","encyclopediaArticle","2023","","Spectral method","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Spectral_method&oldid=1134134983","Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations. The idea is to write the solution of the differential equation as a sum of certain ""basis functions"" (for example, as a Fourier series which is a sum of sinusoids) and then to choose the coefficients in the sum in order to satisfy the differential equation as well as possible. Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are generally nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains (compact support). Consequently, spectral methods connect variables globally while finite elements do so locally. Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth. However, there are no known three-dimensional single domain spectral shock capturing results (shock waves are not smooth). In the finite element community, a method where the degree of the elements is very high or increases as the grid parameter h increases is sometimes called a spectral element method. Spectral methods can be used to solve differential equations (PDEs, ODEs, eigenvalue, etc) and optimization problems. When applying spectral methods to time-dependent PDEs, the solution is typically written as a sum of basis functions with time-dependent coefficients; substituting this in the PDE yields a system of ODEs in the coefficients which can be solved using any numerical method for ODEs. Eigenvalue problems for ODEs are similarly converted to matrix eigenvalue problems. Spectral methods were developed in a long series of papers by Steven Orszag starting in 1969 including, but not limited to, Fourier series methods for periodic geometry problems, polynomial spectral methods for finite and unbounded geometry problems, pseudospectral methods for highly nonlinear problems, and spectral iteration methods for fast solution of steady-state problems. The implementation of the spectral method is normally accomplished either with collocation or a Galerkin or a Tau approach . For very small problems, the spectral method is unique in that solutions may be written out symbolically, yielding a practical alternative to series solutions for differential equations. Spectral methods can be computationally less expensive and easier to implement than finite element methods; they shine best when high accuracy is sought in simple domains with smooth solutions. However, because of their global nature, the matrices associated with step computation are dense and computational efficiency will quickly suffer when there are many degrees of freedom (with some exceptions, for example if matrix applications can be written as Fourier transforms). For larger problems and nonsmooth solutions, finite elements will generally work better due to sparse matrices and better modelling of discontinuities and sharp bends.","2023-01-17","2023-01-25 10:23:12","2024-07-30 20:24:01","2023-01-25 10:23:12","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1134134983","","C:\Users\isido\Zotero\storage\ZM4CLCDZ\Spectral_method.html","","PDE; ODE; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3PJPR7K","encyclopediaArticle","2023","","Galerkin method","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Galerkin_method&oldid=1130900253","In mathematics, in the area of numerical analysis, Galerkin methods, named after the Russian mathematician Boris Galerkin, convert a continuous operator problem, such as a differential equation, commonly in a weak formulation, to a discrete problem by applying linear constraints determined by finite sets of basis functions. Often when referring to a Galerkin method, one also gives the name along with typical assumptions and approximation methods used:  Ritz–Galerkin method (after Walther Ritz) typically assumes symmetric and positive definite bilinear form in the weak formulation, where the differential equation for a physical system can be formulated via minimization of a quadratic function representing the system energy and the approximate solution is a linear combination of the given set of the basis functions. Bubnov–Galerkin method (after Ivan Bubnov) does not require the bilinear form to be symmetric and substitutes the energy minimization with orthogonality constraints determined by the same basis functions that are used to approximate the solution. In an operator formulation of the differential equation, Bubnov–Galerkin method can be viewed as applying an orthogonal projection to the operator. Petrov–Galerkin method (after Georgii I. Petrov) allows using basis functions for orthogonality constraints (called test basis functions) that are different from the basis functions used to approximate the solution. Petrov–Galerkin method can be viewed as an extension of Bubnov–Galerkin method, applying a projection that is not necessarily orthogonal in the operator formulation of the differential equation.Examples of Galerkin methods are: the Galerkin method of weighted residuals, the most common method of calculating the global stiffness matrix in the finite element method, the boundary element method for solving integral equations, Krylov subspace methods.","2023-01-01","2023-01-25 15:17:57","2024-07-30 20:23:57","2023-01-25 15:17:57","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1130900253","","","","PDE; ODE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZZJ97S8","encyclopediaArticle","2022","","Collocation method","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Collocation_method&oldid=1101629556","In mathematics, a collocation method is a method for the numerical solution of ordinary differential equations, partial differential equations and integral equations. The idea is to choose a finite-dimensional space of candidate solutions (usually polynomials up to a certain degree) and a number of points in the domain (called collocation points), and to select that solution which satisfies the given equation at the collocation points.","2022-08-01","2023-01-25 15:18:37","2024-07-30 20:24:20","2023-01-25 15:18:37","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1101629556","","C:\Users\isido\Zotero\storage\GENR5CET\Collocation_method.html","","ODE; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BH37JDJ","journalArticle","2004","Gobet, Emmanuel; Maire, Sylvain","A spectral Monte Carlo method for the Poisson equation","Monte Carlo Methods and Applications","","1569-3961, 0929-9629","10.1515/mcma.2004.10.3-4.275","https://www.degruyter.com/document/doi/10.1515/mcma.2004.10.3-4.275/html","Using a sequential Monte Carlo algorithm, we compute a spectral approximation of the solution of the Poisson equation in dimension 1 and 2. The Feyman-Kac computation of the pointwise solution is achieved using either an integral representation or a modiﬁed walk on spheres method. The variances decrease geometrically with the number of steps. A global solution is obtained, accurate up to the interpolation error. Surprisingly, the accuracy depends very little on the absorption layer thickness of the walk on spheres.","2004-01","2023-01-25 16:00:41","2024-07-30 20:33:18","2023-01-25 16:00:41","","","3-4","10","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>like the idea but doesnt work</p> </div>","C:\Users\isido\Zotero\storage\I5NQN5MH\Gobet en Maire - 2004 - A spectral Monte Carlo method for the Poisson equa.pdf","","monte carlo; PDE; walk on spheres; SALT; ♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGQPJTAJ","preprint","2013","Maire, Sylvain; Tanré, Etienne","Monte Carlo approximations of the Neumann problem","","","","","http://arxiv.org/abs/1203.4910","We introduce Monte Carlo methods to compute the solution of elliptic equations with pure Neumann boundary conditions. We ﬁrst prove that the solution obtained by the stochastic representation has a zero mean value with respect to the invariant measure of the stochastic process associated to the equation. Pointwise approximations are computed by means of standard and new simulation schemes especially devised for local time approximation on the boundary of the domain. Global approximations are computed thanks to a stochastic spectral formulation taking into account the property of zero mean value of the solution. This stochastic formulation is asymptotically perfect in terms of conditioning. Numerical examples are given on the Laplace operator on a square domain with both pure Neumann and mixed Dirichlet-Neumann boundary conditions. A more general convection-diﬀusion equation is also numerically studied.","2013-08-27","2023-01-25 16:12:15","2024-07-30 20:28:07","2023-01-25 16:12:15","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1203.4910 [math]","","C:\Users\isido\Zotero\storage\IAB5W5EV\Maire en Tanré - 2013 - Monte Carlo approximations of the Neumann problem.pdf","","monte carlo; PDE; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1203.4910","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NLXV5UYY","bookSection","2009","Maire, Sylvain; Tanré, Etienne","Stochastic Spectral Formulations for Elliptic Problems","Monte Carlo and Quasi-Monte Carlo Methods 2008","978-3-642-04106-8 978-3-642-04107-5","","","http://link.springer.com/10.1007/978-3-642-04107-5_33","We describe new stochastic spectral formulations with very good properties in terms of conditioning. These formulations are built by combining Monte Carlo approximations of the Feynman-Kac formula and standard deterministic approximations on basis functions. We give error bounds on the solutions obtained using these formulations in the case of linear approximations. Some numerical tests are made on an anisotropic diﬀusion equation using a tensor product Tchebychef polynomial basis and one random point schemes quantiﬁed or not.","2009","2023-01-25 16:13:45","2024-07-30 20:32:07","2023-01-25 16:13:45","513-528","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-04107-5_33","","C:\Users\isido\Zotero\storage\N6IB2UR9\Maire en Tanré - 2009 - Stochastic Spectral Formulations for Elliptic Prob.pdf","","monte carlo; PDE; ♥","","L' Ecuyer, Pierre; Owen, Art B.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTTETC2G","journalArticle","2008","Maire, Sylvain; Tanré, Etienne","Some new simulations schemes for the evaluation of Feynman–Kac representations","Monte Carlo Methods and Applications","","0929-9629, 1569-3961","10.1515/MCMA.2008.002","https://www.degruyter.com/document/doi/10.1515/MCMA.2008.002/html","We describe new variants of the Euler scheme and of the walk on spheres method for the Monte Carlo computation of Feynman-Kac representations. We optimize these variants using quantization for both source and boundary terms. Numerical tests are given on basic examples and on Monte Carlo versions of spectral methods for the Poisson equation. We especially introduce a new stochastic spectral formulation with very good properties in terms of conditioning.","2008-01","2023-01-25 16:14:04","2024-07-30 20:32:48","2023-01-25 16:14:04","","","1","14","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\P47ARGF3\Maire en Tanré - 2008 - Some new simulations schemes for the evaluation of.pdf","","monte carlo; PDE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGJAN8KL","preprint","2018","Meester, Ludolf E.","Exponential convergence of adaptive importance sampling estimators for Markov chain expectations","","","","","http://arxiv.org/abs/1806.03029","In this paper it is shown that adaptive importance sampling algorithms converge at exponential rate for Markov chain expectation problems that admit a combination of a ﬁltered estimator and a Markov zero-variance measure. It extends a chain of results—special purpose proofs were already known for several cases [8, 2, 6]. A recent paper [1] provides a complete description of the class of combinations of Markov process expectations of path functionals and ﬁltered estimators that admit zero-variance importance measures that retain the Markov property. In a way, this is the maximal class for which adaptive importance sampling algorithms might exhibit exponential convergence. The main purpose of this paper is to prove that this is the case: for (most of) those combinations the natural adaptive importance sampling algorithm converges at exponential rate. In addition, the applicability of general Markov chain theory for this purpose is discussed through the analysis of a counterexample presented in [7].","2018-07-10","2023-01-25 16:16:55","2024-07-31 13:06:55","2023-01-25 16:16:55","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1806.03029 [math]","","C:\Users\isido\Zotero\storage\8BJ5II9Q\Meester - 2018 - Exponential convergence of adaptive importance sam.pdf","","monte carlo; importance sampling; ♥; variance reduction; MCMC","","","","","","","","","","","","","","","","","","","","arXiv:1806.03029","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYX88DA3","journalArticle","2014","Gobet, Emmanuel; Surana, Khushboo","A new sequential algorithm for L2-approximation and application to Monte-Carlo integration","","","","","","We design a new stochastic algorithm (called SALT) that sequentially approximates a given function in L2 w.r.t. a probability measure, using a ﬁnite sample of the distribution. By increasing the sets of approximating functions and the simulation eﬀort, we compute a L2-approximation with higher and higher accuracy. The simulation eﬀort is tuned in a robust way that ensures the convergence under rather general conditions. Then, we apply SALT to build eﬃcient control variates for accurate numerical integration. Examples and numerical experiments support the mathematical analysis.","2014","2023-01-25 16:19:32","2024-07-30 20:33:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","<div data-schema-version=""8""><p>like the idea, idk how to make it work</p> </div>","C:\Users\isido\Zotero\storage\MFJ99N7L\Gobet en Surana - A new sequential algorithm for L2-approximation an.pdf","","monte carlo; SALT; ♥♥♥♥; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XEKP5XNX","encyclopediaArticle","2022","","Eigenfunction","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Eigenfunction&oldid=1115272845","In mathematics, an eigenfunction of a linear operator D defined on some function space is any non-zero function                         f                 {\displaystyle f}    in that space that, when acted upon by D, is only multiplied by some scaling factor called an eigenvalue. As an equation, this condition can be written as for some scalar eigenvalue                         λ         .                 {\displaystyle \lambda .}    The solutions to this equation may also be subject to boundary conditions that limit the allowable eigenvalues and eigenfunctions. An eigenfunction is a type of eigenvector.","2022-10-10","2023-01-30 13:55:22","2024-07-30 20:38:52","2023-01-30 13:55:22","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1115272845","","","","♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R425XTU2","preprint","2015","Hout, Karel in 't; Toivanen, Jari","Application of Operator Splitting Methods in Finance","","","","","http://arxiv.org/abs/1504.01022","Financial derivatives pricing aims to ﬁnd the fair value of a ﬁnancial contract on an underlying asset. Here we consider option pricing in the partial differential equations framework. The contemporary models lead to one-dimensional or multidimensional parabolic problems of the convection-diffusion type and generalizations thereof. An overview of various operator splitting methods is presented for the efﬁcient numerical solution of these problems.","2015-04-04","2023-01-30 16:11:12","2024-07-30 20:39:12","2023-01-30 16:11:12","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1504.01022 [q-fin]","","C:\Users\isido\Zotero\storage\P4MV65GF\Hout en Toivanen - 2015 - Application of Operator Splitting Methods in Finan.pdf","","ODE; ♥; IMEX; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1504.01022","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NEHW9EQA","journalArticle","2008","Giles, Mike","Smoking adjoints, part II: fast Monte Carlo Greeks","","","","","","The efficient calculation of price sensitivities continues to be among the greatest practical challenges facing users of Monte Carlo methods in the derivatives industry. Computing Greeks is essential to hedging and risk management, but typically requires substantially more computing time than pricing a derivative. This article shows how an adjoint formula- tion can be used to accelerate the calculation of the Greeks. This method is particularly well suited to applications requiring sensitivities to a large number of parameters. Examples include interest rate derivatives requir- ing sensitivities to all initial forward rates and equity derivatives requiring sensitivities to all points on a volatility surface. The simplest methods for estimating Greeks are based on finite differ- ence approximations, in which a Monte Carlo pricing routine is rerun mul- tiple times at different settings of the input parameters in order to estimate sensitivities to the parameters. In the fixed-income setting, for example, this would mean perturbing each initial forward rate and then rerunning the Monte Carlo simulation to re-price a security or a whole book. The main virtues of this method are that it is straightforward to understand and requires no additional programming. But the bias and variance properties of finite difference estimates can be rather poor, and their computing time requirements grow with the number of input parameters. Better estimates of price sensitivities can often be derived by using in- formation about model dynamics in a Monte Carlo simulation. Techniques for doing this include the pathwise method and likelihood ratio method, both of which are reviewed in chapter 7 of Glasserman (2004). When ap- plicable, these methods produce unbiased estimates of price sensitivities from a single set of simulated paths, that is, without perturbing any para- meters. The pathwise method accomplishes this by differentiating the evo- lution of the underlying assets or state variables along each path; the likelihood ratio method instead differentiates the transition density of the underlying assets or state variables. In comparison with finite difference estimates, these methods require additional model analysis and program- ming, but the additional effort is often justified by the improvement in the quality of calculated Greeks. The adjoint method we develop here applies ideas used in computa- tional fluid dynamics (Giles & Pierce, 2000) to the calculation of pathwise estimates of Greeks. The estimate calculated using the adjoint method is identical to the ordinary pathwise estimate; its potential advantage is there- fore computational, rather than statistical. The relative merits of the ordi- nary (forward) calculation of pathwise Greeks and the adjoint calculation can be summarised as follows: a) the adjoint method is advantageous for calculating the sensitivities of a small number of securities with respect to a large number of parameters; and b) the forward method is advantageous for calculating the sensitivities of many securities with respect to a small number of parameters. The ‘small number of securities’ in this dichotomy could be an entire book, consisting of many individual securities, so long as the sensitivities to be calculated are for the book as a whole and not for the constituent securities. The rest of this article is organised as follows. The next section reviews the usual forward calculation of pathwise Greeks and the subsequent sec- tion illustrates its application in the Libor market model. We then develop the adjoint method for delta estimates, and extend it to applications such as vega estimation requiring sensitivities to parameters of model dynam- ics, rather than just sensitivities to initial conditions. We then extend it to gamma estimation. We use the Libor market model as an illustrative ex- ample in both settings. Lastly, we present numerical results that illustrate the computational savings offered by the adjoint method.","2008","2023-01-30 16:57:01","2024-08-12 14:21:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\D4MR342G\Giles - Smoking adjoints, part II fast Monte Carlo Greeks.pdf","","♥♥♥♥; autodiff; greeks; monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U9DGX66I","preprint","2021","Maran, Andrea; Pallavicini, Andrea; Scoleri, Stefano","Chebyshev Greeks: Smoothing Gamma without Bias","","","","","http://arxiv.org/abs/2106.12431","The computation of Greeks is a fundamental task for risk managing of ﬁnancial instruments. The standard approach to their numerical evaluation is via ﬁnite diﬀerences. Most exotic derivatives are priced via Monte Carlo simulation: in these cases, it is hard to ﬁnd a fast and accurate approximation of Greeks, mainly because of the need of a tradeoﬀ between bias and variance. Recent improvements in Greeks computation, such as Adjoint Algorithmic Diﬀerentiation, are unfortunately uneﬀective on second order Greeks (such as Gamma), which are plagued by the most signiﬁcant instabilities, so that a viable alternative to standard ﬁnite diﬀerences is still lacking. We apply Chebyshev interpolation techniques to the computation of spot Greeks, showing how to improve the stability of ﬁnite diﬀerence Greeks of arbitrary order, in a simple and general way. The increased performance of the proposed technique is analyzed for a number of real payoﬀs commonly traded by ﬁnancial institutions.","2021-06-23","2023-01-30 17:59:14","2024-07-30 20:39:52","2023-01-30 17:59:14","","","","","","","Chebyshev Greeks","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.12431 [q-fin]","Comment: 15 pages, 4 figures; Comment: 15 pages, 4 figures","C:\Users\isido\Zotero\storage\NAD6EA6C\Maran e.a. - 2021 - Chebyshev Greeks Smoothing Gamma without Bias.pdf","","chebychev; greeks; ♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2106.12431","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQMNHDI6","journalArticle","2018","Trefethen, Lloyd N","Approximation Theory and Approximation Practice","","","","","","There are quite a number of excellent books on approximation theory. Three classics are [Cheney 1966], [Davis 1975], and [Meinardus 1967], and a slightly more recent computationally oriented classic is [Powell 1981]. Perhaps the first approximation theory text was [Borel 1905]. A good deal of my emphasis will be on ideas related to Chebyshev points and polynomials, whose origins go back more than a century to mathematicians including Chebyshev (1821–1894), de la Valle  ́e Poussin (1866–1962), Bernstein (1880–1968), and Jackson (1888–1946). In the computer era, some of the early figures who developed “Chebyshev technology,” in approximately chronological order, were Lanczos, Clenshaw, Good, Fox, Elliott, Mason, Orszag, Paszkowski, and V. I. Lebedev. Five books on Chebyshev polynomials are by Snyder [1966], Paszkowski [1975], Fox and Parker [1968], Rivlin [1990], and Mason and Handscomb [2003]. One reason we emphasize Chebyshev technology so much is that in practice, for working with functions on intervals, these methods are unbeatable. For example, we shall see in Chapter 16 that the difference in approximation power between Chebyshev and “optimal” interpolation points is utterly negligible. Another reason is that if you know the Chebyshev material well, this is the best possible foundation for work on other approximation topics, and for understanding the links with Fourier analysis.","2018-01-30","2023-01-30 18:24:38","2024-08-12 14:39:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\MREQLLLP\Trefethen - Approximation Theory and Approximation Practice.pdf","","♥♥♥♥♥; chebychev; function approximation; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DEBDAGGJ","preprint","2018","Gilles, Marc Aurèle; Townsend, Alex","Continuous analogues of Krylov methods for differential operators","","","","","http://arxiv.org/abs/1803.11049","Analogues of the conjugate gradient method, MINRES, and GMRES are derived for solving boundary value problems (BVPs) involving second-order diﬀerential operators. Two challenges arise: imposing the boundary conditions on the solution while building up a Krylov subspace, and guaranteeing convergence of the Krylov-based method on unbounded operators. Our approach employs projection operators to guarantee that the boundary conditions are satisﬁed, and we develop an operator preconditioner that ensures that an approximate solution is computed after a ﬁnite number of iterations. The developed Krylov methods are practical iterative BVP solvers that are particularly eﬃcient when a fast operator-function product is available.","2018-04-19","2023-01-30 21:16:43","2024-07-31 13:08:59","2023-01-30 21:16:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1803.11049 [math]","","C:\Users\isido\Zotero\storage\K2ZL49ZL\Gilles en Townsend - 2018 - Continuous analogues of Krylov methods for differe.pdf","","krylov; ♥","Mathematics - Numerical Analysis","","","","","","","","","","","","","","","","","","","arXiv:1803.11049","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GQI67YA","preprint","2021","Lavagnini, Silvia","Pricing Asian Options with Correlators","","","","","http://arxiv.org/abs/2104.11684","We derive a series expansion by Hermite polynomials for the price of an arithmetic Asian option. This series requires the computation of moments and correlators of the underlying price process, but for a polynomial jump-diﬀusion, these are given in closed form, hence no numerical simulation is required to evaluate the series. This allows, for example, for the explicit computation of Greeks. The weight function deﬁning the Hermite polynomials is a Gaussian density with scale b. We ﬁnd that the rate of convergence for the series depends on b, for which we prove a lower bound to guarantee convergence. Numerical examples show that the series expansion is accurate but unstable for initial values of the underlying process far from zero, mainly due to rounding errors.","2021-04-23","2023-01-30 22:44:59","2024-07-30 20:45:30","2023-01-30 22:44:59","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2104.11684 [q-fin]","","C:\Users\isido\Zotero\storage\AJNEHDQB\Lavagnini - 2021 - Pricing Asian Options with Correlators.pdf","","chebychev; ♥; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2104.11684","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZ7KHXT4","preprint","2022","Bilokon, Paul; Kucherenko, Sergei; Williams, Casey","Quasi-Monte Carlo methods for calculating derivatives sensitivities on the GPU","","","","","http://arxiv.org/abs/2209.11337","The calculation of option Greeks is vital for risk management. Traditional pathwise and ﬁnitedifference methods work poorly for higher-order Greeks and options with discontinuous payoff functions. The Quasi-Monte Carlo-based conditional pathwise method (QMC-CPW) for options Greeks allows the payoff function of options to be effectively smoothed, allowing for increased efﬁciency when calculating sensitivities. Also demonstrated in literature is the increased computational speed gained by applying GPUs to highly parallelisable ﬁnance problems such as calculating Greeks. We pair QMC-CPW with simulation on the GPU using the CUDA platform. We estimate the delta, vega and gamma Greeks of three exotic options: arithmetic Asian, binary Asian, and lookback. Not only are the beneﬁts of QMC-CPW shown through variance reduction factors of up to 1.0 × 1018, but the increased computational speed through usage of the GPU is shown as we achieve speedups over sequential CPU implementations of more than 200x for our most accurate method.","2022-09-22","2023-01-30 22:45:17","2024-07-30 20:54:19","2023-01-30 22:45:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.11337 [q-fin]","Comment: 26 pages, 12 figures","C:\Users\isido\Zotero\storage\6BCIKW77\Bilokon e.a. - 2022 - Quasi-Monte Carlo methods for calculating derivati.pdf","","monte carlo; greeks; ♥♥; quasi monte carlo; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2209.11337","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HS7XJKW","journalArticle","2016","Christara, Christina C; Leung, Nat Chun-Ho","Option pricing in jump diffusion models with quadratic spline collocation","","","","","","In this paper, we develop a robust numerical method in pricing options, when the underlying asset follows a jump diffusion model. We demonstrate that, with the quadratic spline collocation method, the integral approximation in the pricing PIDE is intuitively simple, and comes down to the evaluation of the probabilistic moments of the jump density. When combined with a Picard iteration scheme, the pricing problem can be solved efﬁciently. We present the method and the numerical results from pricing European and American options with Merton’s and Kou’s models.","2016-04-10","2023-01-30 22:45:35","2024-08-12 13:31:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\Z8AA6EGL\Christara en Leung - Option pricing in jump diffusion models with quadr.pdf","","♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FWVFBI9C","preprint","2021","Hashemi, Behnam; Nakatsukasa, Yuji; Trefethen, Lloyd N.","Rectangular eigenvalue problems","","","","","http://arxiv.org/abs/2112.13698","Often the easiest way to discretize an ordinary or partial diﬀerential equation is by a rectangular numerical method, in which n basis functions are sampled at m ≫ n collocation points. We show how eigenvalue problems can be solved in this setting by QR reduction to square matrix generalized eigenvalue problems. The method applies equally in the limit “m = ∞” of eigenvalue problems for quasimatrices. Numerical examples are presented as well as pointers to some related literature.","2021-12-27","2023-01-31 09:40:37","2024-07-31 13:12:59","2023-01-31 09:40:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2112.13698 [cs, math]","","C:\Users\isido\Zotero\storage\MWM8W82T\Hashemi e.a. - 2021 - Rectangular eigenvalue problems.pdf","","PDE; chebychev; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2112.13698","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QS5IWQFR","preprint","2021","Trefethen, Lloyd N.","Exactness of quadrature formulas","","","","","http://arxiv.org/abs/2101.09501","The standard design principle for quadrature formulas is that they should be exact for integrands of a given class, such as polynomials of a ﬁxed degree. We show how this principle fails to predict the actual behavior in four cases: Newton–Cotes, Clenshaw–Curtis, Gauss–Legendre, and Gauss–Hermite quadrature. Three further examples are mentioned more brieﬂy.","2021-01-23","2023-01-31 09:48:54","2024-07-31 13:13:43","2023-01-31 09:48:54","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2101.09501 [cs, math]","","C:\Users\isido\Zotero\storage\UVIPUHJ5\Trefethen - 2021 - Exactness of quadrature formulas.pdf","","chebychev; ♥♥♥; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2101.09501","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5L4YEJF4","preprint","2019","Nakatsukasa, Yuji; Trefethen, Lloyd N.","An algorithm for real and complex rational minimax approximation","","","","","http://arxiv.org/abs/1908.06001","Rational minimax approximation of real functions on real intervals is an established topic, but when it comes to complex functions or domains, there appear to be no algorithms currently in use. Such a method is introduced here, the AAA-Lawson algorithm, available in Chebfun. The new algorithm solves a wide range of problems on arbitrary domains in a fraction of a second of laptop time by a procedure consisting of two steps. First, the standard AAA algorithm is run to obtain a near-best approximation and a set of support points for a barycentric representation of the rational approximant. Then a “Lawson phase” of iteratively reweighted least-squares adjustment of the barycentric coeﬃcients is carried out to improve the approximation to minimax.","2019-08-16","2023-01-31 10:07:56","2024-08-01 15:10:42","2023-01-31 10:07:56","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1908.06001 [cs, math]","","C:\Users\isido\Zotero\storage\M6YJ3J87\Nakatsukasa en Trefethen - 2019 - An algorithm for real and complex rational minimax.pdf","","♥♥♥♥; function approximation; rational functions","","","","","","","","","","","","","","","","","","","","arXiv:1908.06001","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZZIL8X4","preprint","2016","Trefethen, Lloyd N.","Multivariate polynomial approximation in the hypercube","","","","","http://arxiv.org/abs/1608.02216","A theorem is proved concerning approximation of analytic functions by multivariate polynomials in the s-dimensional hypercube. The geometric convergence rate is determined not by the usual notion of degree of a multivariate polynomial, but by the Euclidean degree, deﬁned in terms of the 2-norm rather than the 1-norm of the exponent vector k of a monomial xk11 · · · xkss .","2016-08-07","2023-01-31 10:13:22","2024-08-04 19:37:46","2023-01-31 10:13:22","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1608.02216 [math]","","C:\Users\isido\Zotero\storage\HSBRRHS8\Trefethen - 2016 - Multivariate polynomial approximation in the hyper.pdf","","chebychev; ♥; function approximation","","","","","","","","","","","","","","","","","","","","arXiv:1608.02216","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VC8H5PMH","preprint","2015","Aurentz, Jared L.; Trefethen, Lloyd N.","Chopping a Chebyshev Series","","","","","http://arxiv.org/abs/1512.01803","Chebfun and related software projects for numerical computing with functions are based on the idea that at each step of a computation, a function f (x) deﬁned on an interval [a, b] is “rounded” to a prescribed precision by constructing a Chebyshev series and chopping it at an appropriate point. Designing a chopping algorithm with the right properties proves to be a surprisingly complex and interesting problem. We describe the chopping algorithm introduced in Chebfun Version 5.3 in 2015 after many years of discussion and the considerations that led to this design.","2015-12-06","2023-01-31 10:24:31","2024-08-04 20:04:27","2023-01-31 10:24:31","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1512.01803 [math]","","C:\Users\isido\Zotero\storage\Q3HLDCCX\Aurentz en Trefethen - 2015 - Chopping a Chebyshev Series.pdf","","chebychev; ♥♥; function approximation","","","","","","","","","","","","","","","","","","","","arXiv:1512.01803","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2INF43H","journalArticle","2015","Wright, Grady B.; Javed, Mohsin; Montanelli, Hadrien; Trefethen, Lloyd N.","Extension of Chebfun to periodic functions","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/141001007","http://arxiv.org/abs/1511.00166","Algorithms and underlying mathematics are presented for numerical computation with periodic functions via approximations to machine precision by trigonometric polynomials, including the solution of linear and nonlinear periodic ordinary diﬀerential equations. Diﬀerences from the nonperiodic Chebyshev case are highlighted.","2015-01","2023-01-31 10:25:47","2024-08-04 20:05:24","2023-01-31 10:25:47","C554-C573","","5","37","","SIAM J. Sci. Comput.","","","","","","","","en","","","","","arXiv.org","","arXiv:1511.00166 [math]","Comment: 20 pages","C:\Users\isido\Zotero\storage\UEQFA5S5\Wright e.a. - 2015 - Extension of Chebfun to periodic functions.pdf","","chebychev; ♥; function approximation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CGAZGF6","webpage","2023","","Continuous analogues of matrix factorizations","","","","","https://royalsocietypublishing.org/doi/epdf/10.1098/rspa.2014.0585","","2023-01-31","2023-01-31 10:26:43","2024-08-12 14:54:43","2023-01-31 10:26:43","","","","","","","","","","","","","","en","","","","","","","DOI: 10.1098/rspa.2014.0585","","C:\Users\isido\Zotero\storage\24VM589B\Continuous analogues of matrix factorizations.pdf","","♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AWVWCBNZ","journalArticle","2013","Townsend, Alex; Trefethen, Lloyd N.","An Extension of Chebfun to Two Dimensions","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/130908002","http://epubs.siam.org/doi/10.1137/130908002","An object-oriented Matlab system is described that extends the capabilities of Chebfun to smooth functions of two variables deﬁned on rectangles. Functions are approximated to essentially machine precision by using iterative Gaussian elimination with complete pivoting to form “chebfun2” objects representing low rank approximations. Operations such as integration, diﬀerentiation, function evaluation, and transforms are particularly eﬃcient. Global optimization, the singular value decomposition, and rootﬁnding are also extended to chebfun2 objects. Numerical applications are presented.","2013-01","2023-01-31 10:46:53","2024-08-04 20:12:33","2023-01-31 10:46:53","C495-C518","","6","35","","SIAM J. Sci. Comput.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\X7345UNU\Townsend en Trefethen - 2013 - An Extension of Chebfun to Two Dimensions.pdf","","chebychev; ♥♥♥; function approximation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPP4UJM9","journalArticle","2015","Rivaz, Azim","Two-dimensional Chebyshev Polynomials for Solving Two-dimensional Integro-Differential Equations","","","","","","In this paper, we present a new approach to obtain the numerical solution of the linear twodimensional Fredholm and Volterra integro-differential equations (2D-FIDE and 2D-VIDE). First, we introduce the two-dimensional Chebyshev polynomials and construct their operational matrices of integration. Then, both of them, two-dimensional Chebyshev polynomials and their operational matrix of integration, are used to represent the matrix form of 2D-FIDE and 2D-VIDE. The main characteristic of this approach is that it reduces 2D-FIDE and 2D-VIDE to a system of linear algebraic equations. Illustrative examples are included to demonstrate the validity and applicability of the presented technique.","2015","2023-01-31 15:43:47","2024-08-05 13:20:12","","","","2","","","","","","","","","","","en","","","","","Zotero","","","<div data-schema-version=""8""><p>some identities of chebychev polynomials that we thought we needed</p> </div>","C:\Users\isido\Zotero\storage\SPZJSJ2L\Rivaz - 2015 - Two-dimensional Chebyshev Polynomials for Solving .pdf","","chebychev; integral equations; ♥♥; function approximation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFHJWFII","journalArticle","2017","Sajikumar, S","IMAGE COMPRESSION USING CHEBYSHEV POLYNOMIAL SURFACE FIT","","","","","","A lossy image compression method based on block-by-block surface fit using bivariate polynomial is proposed. Chebyshev polynomials of first kind are used to generate the surface for each block. Data compression is achieved by representing the gray level variations across any block by the parameters of the fitted surface and these parameters are stored instead of the pixel values. Compression with three coefficients and four coefficients in the fit model is proposed. In standard lossy compression techniques compression is achieved by exploiting spatial redundancies of the input data. In this proposed method compression does not depends on redundant information but depends on the block size that can be predefined. The method is best suited for high compression with reasonable reconstructed image quality. Performance was tested on number of test images using Chebyshev polynomials of different orders.","2017","2023-01-31 16:19:38","2024-08-12 14:37:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\Z7XR22R5\Sajikumar - IMAGE COMPRESSION USING CHEBYSHEV POLYNOMIAL SURFA.pdf","","♥; chebychev; function approximation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D36A8RLK","encyclopediaArticle","2022","","Clenshaw algorithm","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Clenshaw_algorithm&oldid=1089015914","In numerical analysis, the Clenshaw algorithm, also called Clenshaw summation, is a recursive method to evaluate a linear combination of Chebyshev polynomials.  The method was published by Charles William Clenshaw in 1955. It is a generalization of Horner's method for evaluating a linear combination of monomials. It generalizes to more than just Chebyshev polynomials; it applies to any class of functions that can be defined by a three-term recurrence relation.","2022-05-21","2023-01-31 21:22:51","2024-08-05 13:26:41","2023-01-31 21:22:51","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1089015914","","C:\Users\isido\Zotero\storage\HTUIQKE9\Clenshaw_algorithm.html","","chebychev; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PV2HLZTH","forumPost","2017","Kirill","Answer to ""Clenshaw-type recurrence for derivative of Chebyshev series""","Computational Science Stack Exchange","","","","https://scicomp.stackexchange.com/a/27866","","2017-09-17","2023-01-31 21:25:09","2024-08-06 08:59:23","2023-01-31 21:25:09","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\PMJIJDW5\clenshaw-type-recurrence-for-derivative-of-chebyshev-series.html","","chebychev; ♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMTIPEUX","forumPost","2012","Pedro","Answer to ""Fast (approximate) evaluation of Chebyshev polynomial""","Computational Science Stack Exchange","","","","https://scicomp.stackexchange.com/a/3416","","2012-10-03","2023-01-31 21:29:55","2024-08-06 09:25:03","2023-01-31 21:29:54","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\FX7IQLE7\fast-approximate-evaluation-of-chebyshev-polynomial.html","","chebychev; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4V8UR4J","preprint","2022","Pan, Victor Y.; Go, Soo; Luan, Qi; Zhao, Liang","Fast Approximation of Polynomial Zeros and Matrix Eigenvalues","","","","","http://arxiv.org/abs/2301.11268","Given a black box (oracle) for the evaluation of a univariate polynomial p(x) of a degree d, we seek its zeros, that is, the roots of the equation p(x)=0. At FOCS 2016 Louis and Vempala approximated a largest zero of such a real-rooted polynomial within $1/2^b$, by performing at NR cost of the evaluation of Newton's ratio p(x)/p'(x) at O(b\log(d)) points x. They readily extended this root-finder to record fast approximation of a largest eigenvalue of a symmetric matrix under the Boolean complexity model. We apply distinct approach and techniques to obtain more general results at the same computational cost.","2022-12-31","2023-02-01 08:15:35","2024-08-06 06:13:15","2023-02-01 08:15:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.11268 [cs, math]","Comment: 19 pages, 5 figures","C:\Users\isido\Zotero\storage\WNNTR8QA\Pan e.a. - 2022 - Fast Approximation of Polynomial Zeros and Matrix .pdf","","♥","","","","","","","","","","","","","","","","","","","","arXiv:2301.11268","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LJ6QF2CQ","preprint","2016","Dumas, Jean-Guillaume; Pan, Victor","Fast Matrix Multiplication and Symbolic Computation","","","","","http://arxiv.org/abs/1612.05766","The complexity of matrix multiplication (hereafter MM) has been intensively studied since 1969, when Strassen surprisingly decreased the exponent 3 in the cubic cost of the straightforward classical MM to log2(7) ≈ 2.8074. Applications to some fundamental problems of Linear Algebra and Computer Science have been immediately recognized, but the researchers in Computer Algebra keep discovering more and more applications even today, with no sign of slowdown. We survey the unﬁnished history of decreasing the exponent towards its information lower bound 2, recall some important techniques discovered in this process and linked to other ﬁelds of computing, reveal sample surprising applications to fast computation of the inner products of two vectors and summation of integers, and discuss the curse of recursion, which separates the progress in fast MM into its most acclaimed and purely theoretical part and into valuable acceleration of MM of feasible sizes. Then, in the second part of our paper, we cover fast MM in realistic symbolic computations and discuss applications and implementation of fast exact matrix multiplication. We ﬁrst review how most of exact linear algebra can be reduced to matrix multiplication over small ﬁnite ﬁelds. Then we highlight the diﬀerences in the design of approximate and exact implementations of fast MM, taking into account nowadays processor and memory hierarchies. In the concluding section we comment on current perspectives of the study of fast MM.","2016-12-17","2023-02-01 08:23:53","2024-08-06 06:25:04","2023-02-01 08:23:53","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1612.05766 [cs]","","C:\Users\isido\Zotero\storage\SJLTRMVL\Dumas en Pan - 2016 - Fast Matrix Multiplication and Symbolic Computatio.pdf","","♥","","","","","","","","","","","","","","","","","","","","arXiv:1612.05766","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"77YIF8YL","encyclopediaArticle","2023","","Stochastic gradient descent","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Stochastic_gradient_descent&oldid=1136419553","Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.While the basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning.","2023-01-30","2023-02-01 10:06:17","2024-08-06 09:26:22","2023-02-01 10:06:17","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1136419553","","C:\Users\isido\Zotero\storage\C8B3CMLS\Stochastic_gradient_descent.html","","SGD; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBPJ52BH","encyclopediaArticle","2022","","Augmented Lagrangian method","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Augmented_Lagrangian_method&oldid=1130093461#Alternating_direction_method_of_multipliers","Augmented Lagrangian methods are a certain class of algorithms for solving constrained optimization problems.  They have similarities to penalty methods in that they replace a constrained optimization problem by a series of unconstrained problems and add a penalty term to the objective; the difference is that the augmented Lagrangian method adds yet another term, designed to mimic a Lagrange multiplier.  The augmented Lagrangian is related to, but not identical with the method of Lagrange multipliers. Viewed differently, the unconstrained objective is the Lagrangian of the constrained problem, with an additional penalty term (the augmentation). The method was originally known as the method of multipliers, and was studied much in the 1970 and 1980s as a good alternative to penalty methods. It was first discussed by Magnus Hestenes, and by Michael Powell in 1969. The method was studied by R. Tyrrell Rockafellar in relation to Fenchel duality, particularly in relation to proximal-point methods, Moreau–Yosida regularization, and maximal monotone operators: These methods were used in structural optimization.  The method was also studied by Dimitri Bertsekas, notably in his 1982 book, together with extensions involving nonquadratic regularization functions, such as entropic regularization, which gives rise to the ""exponential method of multipliers,"" a method that handles inequality constraints with a twice differentiable augmented Lagrangian function. Since the 1970s, sequential quadratic programming (SQP) and interior point methods (IPM) have had increasing attention, in part because they more easily use sparse matrix subroutines from numerical software libraries, and in part because IPMs have proven complexity results via the theory of self-concordant functions. The augmented Lagrangian method was rejuvenated by the optimization systems LANCELOT, ALGENCAN and AMPL, which allowed sparse matrix techniques to be used on seemingly dense but ""partially separable"" problems. The method is still useful for some problems. Around 2007, there was a resurgence of augmented Lagrangian methods in fields such as total-variation denoising and compressed sensing. In particular, a variant of the standard augmented Lagrangian method that uses partial updates (similar to the Gauss–Seidel method for solving linear equations) known as the alternating direction method of multipliers or ADMM gained some attention.","2022-12-28","2023-02-01 11:34:05","2023-02-01 11:34:05","2023-02-01 11:34:05","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1130093461","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVCP6M5T","journalArticle","2019","Yuan, Di; Zhang, Xinming","An overview of numerical methods for the first kind Fredholm integral equation","SN Applied Sciences","","2523-3963, 2523-3971","10.1007/s42452-019-1228-3","http://link.springer.com/10.1007/s42452-019-1228-3","In the field of engineering technology, many problems can be transformed into the first kind Fredholm integral equation, which has a prominent feature called “ill-posedness”. This property makes it difficult to find the analytical solution of first kind Fredholm integral equation. Therefore, how to find the numerical solution of first kind Fredholm integral equation has been a common concern of domestic and overseas scholars in recent years. In this article, various numerical solution methods of first kind Fredholm integral equation are introduced in detail. First, the existence and convergence of the solution of the integral equation are given. Second, the current mainstream numerical methods, such as regularization method, wavelet analysis method and multilevel iteration method are introduced in detail. Finally, we presented a concise overview of the numerical method of first kind Fredholm integral equation.","2019-10","2023-02-01 16:47:20","2024-08-06 08:52:41","2023-02-01 16:47:20","1178","","10","1","","SN Appl. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\FTSQCFRP\Yuan en Zhang - 2019 - An overview of numerical methods for the first kin.pdf","","integral equations; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCQPFSAY","journalArticle","2022","Almousa, Salah Al-Deen; Horv´ath, G´abor; Horv´ath, Ill ´es; M´esz´aros, Andr´as; Telek, Mikl ´os","The CME method: Efficient numerical inverse Laplace transformation with Concentrated Matrix Exponential distribution","ACM SIGMETRICS Performance Evaluation Review","","0163-5999","10.1145/3543146.3543155","https://dl.acm.org/doi/10.1145/3543146.3543155","Numerical inverse Laplace transformation (NILT) is an important tool in the ﬁeld of system modelling and performance analysis. The recently introduced CME method has many important advantages over the alternative numerical inverse Laplace transformation (NILT) methods. It avoids Gibbs oscillation (i.e., does not generate overshoot and undershoot), preserves the monotonicity of functions, its accuracy is gradually improving with the order, and it is numerically more stable than the alternative methods. In this paper we demonstrate these advantages and introduce our tool which implements the CME method and other popular NILT methods.","2022-06-02","2023-02-02 10:54:13","2024-08-06 08:54:13","2023-02-02 10:54:13","29-34","","4","49","","SIGMETRICS Perform. Eval. Rev.","The CME method","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BXPJ7LAI\Almousa e.a. - 2022 - The CME method Efficient numerical inverse Laplac.pdf","","♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKSTIAW2","journalArticle","2006","Chatterjee, K; Yu, C","A Monte Carlo Algorithm for the Solution of the One-Dimensional Wave Equation","","","","","","This paper presents a 1D Monte Carlo (MC) algorithm for the solution of the wave equation. Historically, the MC method has not been applied successfully to the solution of wave problems. This can primarily be attributed to the problem of resonance in the frequency-domain Green’s function for finite geometries at length scales greater than half a wavelength. In our previously published work, we have been successful in obtaining a frequency-domain solution at multiple-wavelength length scales through the use of infinite-domain Green’s functions. In this work, we extend the algorithm to problems in the time-domain. The MC method does not require any discretization, and hence the memory requirements are lower than approaches based on discretization. Another advantage of the MC method is that the computational procedure is inherently parallelizable and an almost linear increase in computational speed can be obtained with an increase in the number of processors. The application area of our interest is in the full-wave analysis of IC interconnect structures at multi-GHz frequencies.","2006-11-20","2023-02-09 08:59:19","2024-08-12 13:26:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\Z59LHDC5\Chatterjee en Yu - A Monte Carlo Algorithm for the Solution of the On.pdf","","♥; green function; monte carlo; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZRBKHQ8","journalArticle","2006","Barnett, Alex H","Greens Functions for the Wave Equation","","","","","","I gather together known results on fundamental solutions to the wave equation in free space, and Greens functions in tori, boxes, and other domains. From this the corresponding fundamental solutions for the Helmholtz equation are derived, and, for the 2D case the semiclassical approximation interpreted back in the time-domain. Utility: scarring via time-dependent propagation in cavities; Math 46 course ideas.","2006-12-28","2023-02-09 09:17:12","2024-08-12 13:23:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\P38YMD3D\Barnett - Greens Functions for the Wave Equation.pdf","","♥; green function; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83Z69CK3","journalArticle","2022","Rath, Alexander; Grittmann, Pascal; Herholz, Sebastian; Weier, Philippe; Slusallek, Philipp","EARS: efficiency-aware russian roulette and splitting","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3528223.3530168","https://dl.acm.org/doi/10.1145/3528223.3530168","Russian roulette and splitting are widely used techniques to increase the efficiency of Monte Carlo estimators. But, despite their popularity, there is little work on how to best apply them. Most existing approaches rely on simple heuristics based on, e.g., surface albedo and roughness. Their efficiency often hinges on user-controlled parameters. We instead iteratively learn optimal Russian roulette and splitting factors during rendering, using a simple and lightweight data structure. Given perfect estimates of variance and cost, our fixed-point iteration provably converges to the optimal Russian roulette and splitting factors that maximize the rendering efficiency. In our application to unidirectional path tracing, we achieve consistent and significant speed-ups over the state of the art.","2022-07","2023-02-10 11:06:54","2024-08-06 09:30:36","2023-02-10 11:06:54","1-14","","4","41","","ACM Trans. Graph.","EARS","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>has a youtube video</p> </div>","C:\Users\isido\Zotero\storage\SX8TXAMP\Rath e.a. - 2022 - EARS efficiency-aware russian roulette and splitt.pdf","","monte carlo; rendering; ♥♥♥; variance reduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZTYUYV8","journalArticle","2020","West, Rex; Georgiev, Iliyan; Gruson, Adrien; Hachisuka, Toshiya","Continuous multiple importance sampling","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3386569.3392436","https://dl.acm.org/doi/10.1145/3386569.3392436","Multiple importance sampling (MIS) is a provably good way to combine a finite set of sampling techniques to reduce variance in Monte Carlo integral estimation. However, there exist integration problems for which a continuum of sampling techniques is available. To handle such cases we establish a continuous MIS (CMIS) formulation as a generalization of MIS to uncountably infinite sets of techniques. Our formulation is equipped with a base estimator that is coupled with a provably optimal balance heuristic and a practical stochastic MIS (SMIS) estimator that makes CMIS accessible to a broad range of problems. To illustrate the effectiveness and utility of our framework, we apply it to three different light transport applications, showing improved performance over the prior state-of-the-art techniques.","2020-08-31","2023-02-10 11:12:54","2024-08-06 16:29:45","2023-02-10 11:12:54","","","4","39","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\T968PF33\West e.a. - 2020 - Continuous multiple importance sampling.pdf","","monte carlo; rendering; importance sampling; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UP4ASYSQ","preprint","2021","He, Shengyi; Jiang, Guangxin; Lam, Henry; Fu, Michael C.","Adaptive Importance Sampling for Efficient Stochastic Root Finding and Quantile Estimation","","","","","http://arxiv.org/abs/2102.10631","In solving simulation-based stochastic root-ﬁnding or optimization problems that involve rare events, such as in extreme quantile estimation, running crude Monte Carlo can be prohibitively ineﬃcient. To address this issue, importance sampling can be employed to drive down the sampling error to a desirable level. However, selecting a good importance sampler requires knowledge of the solution to the problem at hand, which is the goal to begin with and thus forms a circular challenge. We investigate the use of adaptive importance sampling to untie this circularity. Our procedure sequentially updates the importance sampler to reach the optimal sampler and the optimal solution simultaneously, and can be embedded in both sample average approximation and stochastic approximation-type algorithms. Our theoretical analysis establishes strong consistency and asymptotic normality of the resulting estimators. We also demonstrate, via a minimax perspective, the key role of using adaptivity in controlling asymptotic errors. Finally, we illustrate the eﬀectiveness of our approach via numerical experiments.","2021-02-21","2023-02-10 11:20:19","2024-08-06 16:38:45","2023-02-10 11:20:19","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2102.10631 [math, stat]","","C:\Users\isido\Zotero\storage\AJ5J334J\He e.a. - 2021 - Adaptive Importance Sampling for Efficient Stochas.pdf","","monte carlo; importance sampling; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2102.10631","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SV389DF","conferencePaper","2022","West, Rex; Georgiev, Iliyan; Hachisuka, Toshiya","Marginal Multiple Importance Sampling","SIGGRAPH Asia 2022 Conference Papers","978-1-4503-9470-3","","10.1145/3550469.3555388","https://dl.acm.org/doi/10.1145/3550469.3555388","Multiple importance sampling (MIS) is a powerful tool to combine different sampling techniques in a provably good manner. MIS requires that the techniques’ probability density functions (PDFs) are readily evaluable point-wise. However, this requirement may not be satisfied when (some of) those PDFs are marginals, i.e., integrals of other PDFs. We generalize MIS to combine samples from such marginal PDFs. The key idea is to consider each marginalization domain as a continuous space of sampling techniques with readily evaluable (conditional) PDFs. We stochastically select techniques from these spaces and combine the samples drawn from them into an unbiased estimator. Prior work has dealt with the special cases of multiple classical techniques or a single marginal one. Our formulation can handle mixtures of those. We apply our marginal MIS formulation to light-transport simulation to demonstrate its utility. We devise a marginal path sampling framework that makes previously intractable sampling techniques practical and significantly broadens the path-sampling choices beyond what is presently possible. We highlight results from two algorithms based on marginal MIS: a novel formulation of path-space filtering at multiple vertices along a camera path and a similar filtering method for photon-density estimation. CCS Concepts: • Computing methodologies → Rendering; Ray tracing.","2022-11-29","2023-02-10 11:23:04","2024-08-06 16:39:29","2023-02-10 11:23:04","1-8","","","","","","","","","","","ACM","Daegu Republic of Korea","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\HGCCANTP\West e.a. - 2022 - Marginal Multiple Importance Sampling.pdf","","monte carlo; rendering; importance sampling; ♥; variance reduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SA '22: SIGGRAPH Asia 2022","","","","","","","","","","","","","","",""
"YQU8X77K","preprint","2020","Crespo, Miguel; Bernal, Felix; Jarabo, Adrian; Muñoz, Adolfo","Primary-Space Adaptive Control Variates using Piecewise-Polynomial Approximations","","","","","http://arxiv.org/abs/2008.06722","We present an unbiased numerical integration algorithm that handles both low-frequency regions and high frequency details of multidimensional integrals. It combines quadrature and Monte Carlo integration, by using a quadrature-base approximation as a control variate of the signal. We adaptively build the control variate constructed as a piecewise polynomial, which can be analytically integrated, and accurately reconstructs the low frequency regions of the integrand. We then recover the high-frequency details missed by the control variate by using Monte Carlo integration of the residual. Our work leverages importance sampling techniques by working in primary space, allowing the combination of multiple mappings; this enables multiple importance sampling in quadrature-based integration. Our algorithm is generic, and can be applied to any complex multidimensional integral. We demonstrate its effectiveness with four applications with low dimensionality: transmittance estimation in heterogeneous participating media, low-order scattering in homogeneous media, direct illumination computation, and rendering of distributed effects. Finally, we show how our technique is extensible to integrands of higher dimensionality, by computing the control variate on Monte Carlo estimates of the high-dimensional signal, and accounting for such additional dimensionality on the residual as well. In all cases, we show accurate results and faster convergence compared to previous approaches.","2020-08-15","2023-02-10 11:32:58","2024-08-06 18:33:15","2023-02-10 11:32:58","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2008.06722 [cs]","Comment: 14 pages, 18 figures","C:\Users\isido\Zotero\storage\UYAUD2N9\Crespo e.a. - 2020 - Primary-Space Adaptive Control Variates using Piec.pdf","","monte carlo; rendering; control variates; ♥♥; variance reduction; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2008.06722","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3I3VLE8E","journalArticle","2017","Lfant, Fredrik Johansson; Bordeaux, Inria","Numerical integration in complex interval arithmetic","","","","","","","2017-12-11","2023-02-10 14:46:17","2024-08-12 14:30:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\WGXY3GTI\Lfant en Bordeaux - Numerical integration in complex interval arithmet.pdf","","♥; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJQNG7JJ","webpage","2023","","Fast Wavelet Transform (FWT) Algorithm - MATLAB & Simulink","","","","","https://www.mathworks.com/help/wavelet/ug/fast-wavelet-transform-fwt-algorithm.html?ue","","2023-02-11","2023-02-11 08:25:10","2024-08-12 14:47:19","2023-02-11 08:25:10","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\IZRA3VBE\fast-wavelet-transform-fwt-algorithm.html","","wavelets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCDH2DMJ","videoRecording","2020","The Abel Prize","Ingrid Daubechies: Wavelet bases: roots, surprises and applications","","","","","https://www.youtube.com/watch?v=tMV61BZCrhk","Yves Meyer's surprising construction of orthonormal bases consisting of dilates and translates of a single smooth function was followed soon after by the development of the Multiresolution Analysis framework in collaboration with Stephane Mallat. As already shown in the presentation by Stephane Mallat, this development was rooted in and used insights from a variety of fields -- ranging from pure harmonic analysis to statistics, quantum physics, geophysics and computer vision. The lecture will discuss some of those diverse roots in more detail, and also show how the new wavelet synthesis, sparked by Yves Meyer's seminal work, led to further progress in all those fields as well as others. Finally, hindsight shows that the new paradigm introduced by wavelet analysis was a first example of the power of  sparse decompositions -- and thus a prelude to another paradigm shift, that of Compressed Sensing, about which more will follow, in the presentation by Emmanuel Candès.","2020-02-03","2023-02-11 08:26:33","2024-08-06 18:37:25","2023-02-11 08:26:33","","","","","","","Ingrid Daubechies","","","","","","","","","","","","YouTube","","","","","","wavelets; ♥♥","","","","","","","","","","","","","","","","","","","","","","45:51","","","","","","","","","","","","","","","","","","","","","","","","",""
"4G9KSSQT","conferencePaper","1997","Unser, Michael A.","Ten good reasons for using spline wavelets","","","","10.1117/12.292801","http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=930676","The purpose of this note is to highlight some of the unique properties of spline wavelets. These wavelets can be classified in four categories: othogonal (Battle-Lemarié), semi-orthogonal (e.g., B-spline), shift-orthogonal, and biorthogonal (Cohen-DaubechiesFeauveau). Unlike most other wavelet bases, splines have explicit formulae in both the time and frequency domain, which greatly facilitates their manipulation. They allow for a progressive transition between the two extreme cases of a multiresolution: Haar's piecewise constant representation (spline of degree zero) versus Shannon's bandlimited model (which corresponds to a spline of infinite order). Spline wavelets are extremely regular and usually symmetric or anti-symmetric. They can be designed to have compact support and to achieve optimal time-frequency localization (B-spline wavelets). The underlying scaling functions are the B-splines, which are the shortest and most regular scaling functions of order L. Finally, splines have the best approximation properties among all known wavelets of a given order L. In other words, they are the best for approximating smooth functions.","1997-10-30","2023-02-11 13:03:20","2024-08-06 18:40:03","2023-02-11 13:03:20","422-431","","","","","","","","","","","","San Diego, CA","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\CQW7SJ6F\Unser - 1997 - Ten good reasons for using spline wavelets.pdf","","wavelets; ♥","","Aldroubi, Akram; Laine, Andrew F.; Unser, Michael A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Optical Science, Engineering and Instrumentation '97","","","","","","","","","","","","","","",""
"8R3ABQSM","preprint","2016","Toulis, Panos; Airoldi, Edoardo M.","Asymptotic and finite-sample properties of estimators based on stochastic gradients","","","","","http://arxiv.org/abs/1408.2923","Stochastic optimization procedures, such as stochastic gradient descent, have gained popularity for parameter estimation from large data sets. However, standard stochastic optimization procedures cannot eﬀectively combine numerical stability with statistical and computational eﬃciency. Here, we introduce an implicit stochastic gradient descent procedure, the iterates of which are implicitly deﬁned. Intuitively, implicit iterates shrink the standard iterates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed in practice, thus increasing stability without increasing the computational burden. When combined with averaging, the proposed procedure achieves statistical eﬃciency as well. We derive non-asymptotic bounds and characterize the asymptotic distribution of implicit procedures. Our analysis also reveals the asymptotic variance of a number of existing procedures. We demonstrate implicit stochastic gradient descent by further developing theory for generalized linear models, Cox proportional hazards, and M-estimation problems, and by carrying out extensive experiments. Our results suggest that the implicit stochastic gradient descent procedure is poised to become the workhorse of estimation with large data sets.","2016-09-28","2023-02-11 14:07:45","2024-02-07 20:11:59","2023-02-11 14:07:45","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1408.2923 [stat]","Comment: Annals of Statistics, 2016, forthcoming; 71 pages, 37-page main body; 9 figures; 6 tables","C:\Users\isido\Zotero\storage\IUAWENHV\Toulis en Airoldi - 2016 - Asymptotic and finite-sample properties of estimat.pdf","","optimization; gradient descent","","","","","","","","","","","","","","","","","","","","arXiv:1408.2923","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YY3EUM8L","journalArticle","2020","Rath, Alexander; Grittmann, Pascal; Herholz, Sebastian; Vévoda, Petr; Slusallek, Philipp; Křivánek, Jaroslav","Variance-aware path guiding","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3386569.3392441","https://dl.acm.org/doi/10.1145/3386569.3392441","Path guiding is a promising tool to improve the performance of path tracing algorithms. However, not much research has investigated what target densities a guiding method should strive to learn for optimal performance. Instead, most previous work pursues the zero-variance goal: The local decisions are guided under the assumption that all other decisions along the random walk will be sampled perfectly. In practice, however, many decisions are poorly guided, or not guided at all. Furthermore, learned distributions are often marginalized, e.g., by neglecting the BSDF. We present a generic procedure to derive theoretically optimal target densities for local path guiding. These densities account for variance in nested estimators, and marginalize provably well over, e.g., the BSDF. We apply our theory in two state-of-the-art rendering applications: a path guiding solution for unidirectional path tracing [Müller et al. 2017] and a guiding method for light source selection for the many lights problem [Vévoda et al. 2018]. In both cases, we observe significant improvements, especially on glossy surfaces. The implementations for both applications consist of trivial modifications to the original code base, without introducing any additional overhead.","2020-08-31","2023-02-11 15:09:58","2023-03-18 20:04:04","2023-02-11 15:09:58","","","4","39","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\X79P4T7Y\Rath e.a. - 2020 - Variance-aware path guiding.pdf","","monte carlo; rendering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHWPZTLY","journalArticle","2005","Xiu, Dongbin; Hesthaven, Jan S.","High-Order Collocation Methods for Differential Equations with Random Inputs","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/040615201","http://epubs.siam.org/doi/10.1137/040615201","Recently there has been a growing interest in designing eﬃcient methods for the solution of ordinary/partial diﬀerential equations with random inputs. To this end, stochastic Galerkin methods appear to be superior to other nonsampling methods and, in many cases, to several sampling methods. However, when the governing equations take complicated forms, numerical implementations of stochastic Galerkin methods can become nontrivial and care is needed to design robust and eﬃcient solvers for the resulting equations. On the other hand, the traditional sampling methods, e.g., Monte Carlo methods, are straightforward to implement, but they do not oﬀer convergence as fast as stochastic Galerkin methods. In this paper, a high-order stochastic collocation approach is proposed. Similar to stochastic Galerkin methods, the collocation methods take advantage of an assumption of smoothness of the solution in random space to achieve fast convergence. However, the numerical implementation of stochastic collocation is trivial, as it requires only repetitive runs of an existing deterministic solver, similar to Monte Carlo methods. The computational cost of the collocation methods depends on the choice of the collocation points, and we present several feasible constructions. One particular choice, based on sparse grids, depends weakly on the dimensionality of the random space and is more suitable for highly accurate computations of practical applications with large dimensional random inputs. Numerical examples are presented to demonstrate the accuracy and eﬃciency of the stochastic collocation methods.","2005-01","2023-02-13 11:13:00","2023-03-18 20:03:48","2023-02-13 11:13:00","1118-1139","","3","27","","SIAM J. Sci. Comput.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\PJ6XFAQG\Xiu en Hesthaven - 2005 - High-Order Collocation Methods for Differential Eq.pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7I26G9BD","preprint","2014","Teckentrup, Aretha L.; Jantsch, Peter; Webster, Clayton G.; Gunzburger, Max","A Multilevel Stochastic Collocation Method for Partial Differential Equations with Random Input Data","","","","","http://arxiv.org/abs/1404.2647","Stochastic collocation methods for approximating the solution of partial diﬀerential equations with random input data (e.g., coeﬃcients and forcing terms) suﬀer from the curse of dimensionality whereby increases in the stochastic dimension cause an explosion of the computational eﬀort. We propose and analyze a multilevel version of the stochastic collocation method that, as is the case for multilevel Monte Carlo (MLMC) methods, uses hierarchies of spatial approximations to reduce the overall computational complexity. In addition, our proposed approach utilizes, for approximation in stochastic space, a sequence of multi-dimensional interpolants of increasing ﬁdelity which can then be used for approximating statistics of the solution as well as for building highorder surrogates featuring faster convergence rates. A rigorous convergence and computational cost analysis of the new multilevel stochastic collocation method is provided, demonstrating its advantages compared to standard single-level stochastic collocation approximations as well as MLMC methods. Numerical results are provided that illustrate the theory and the eﬀectiveness of the new multilevel method.","2014-05-22","2023-02-13 11:13:55","2023-03-18 20:03:36","2023-02-13 11:13:55","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1404.2647 [math]","","C:\Users\isido\Zotero\storage\R98M8FWQ\Teckentrup e.a. - 2014 - A Multilevel Stochastic Collocation Method for Par.pdf","","random ODE","Mathematics - Numerical Analysis","","","","","","","","","","","","","","","","","","","arXiv:1404.2647","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5GPLP4C","preprint","2022","Deelstra, Griselda; Grzelak, Lech A.; Wolf, Felix","Accelerated Computations of Sensitivities for xVA","","","","","http://arxiv.org/abs/2211.17026","Exposure simulations are fundamental to many xVA calculations and are a nested expectation problem where repeated portfolio valuations create a signiﬁcant computational expense. Sensitivity calculations which require shocked and unshocked valuations in bump-and-revalue schemes exacerbate the computational load. A known reduction of the portfolio valuation cost is understood to be found in polynomial approximations, which we apply in this article to interest rate sensitivities of expected exposures. We consider a method based on the approximation of the shocked and unshocked valuation functions, as well as a novel approach in which the diﬀerence between these functions is approximated. Convergence results are shown, and we study the choice of interpolation nodes. Numerical experiments with interest rate derivatives are conducted to demonstrate the high accuracy and remarkable computational cost reduction. We further illustrate how the method can be extended to more general xVA models using the example of CVA with wrong-way risk.","2022-11-30","2023-02-13 11:15:17","2024-02-07 20:12:40","2023-02-13 11:15:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.17026 [q-fin]","","C:\Users\isido\Zotero\storage\GCGYS354\Deelstra e.a. - 2022 - Accelerated Computations of Sensitivities for xVA.pdf","","chebychev","","","","","","","","","","","","","","","","","","","","arXiv:2211.17026","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FZQ8RRU","preprint","2013","Steffes-lai, Daniela; Rosseel, Eveline; Clees, Tanja","Interpolation methods to compute statistics of a stochastic partial differential equation","","","","","http://arxiv.org/abs/1309.3853","This paper considers the analysis of partial diﬀerential equations (PDE) containing multiple random variables. Recently developed collocation methods enable the construction of high-order stochastic solutions by converting a stochastic PDE into a system of deterministic PDEs. This interpolation method requires that the probability distribution of all random input variables is known a priori, which is often not the case in industrially relevant applications. Additionally, this method suﬀers from a curse of dimensionality, i.e., the number of deterministic PDEs to be solved grows exponentially with respect to the number of random variables. This paper presents an alternative interpolation method, based on a radial basis function (RBF) metamodel, to compute statistics of the stochastic PDE. The RBF metamodel can be constructed even if the probability distribution of all random variables is not known. Then, a lot of statistic scenarios with diﬀerent probability distributions of the random variables can be computed with this single metamodel. In order to reduce the model complexity, we present a parameter screening technique which can be combined with an interpolation method to solve a reduced stochastic model. Numerical results of a model problem demonstrate that the RBF metamodel is as fast as a low order collocation approach and achieves a good accuracy. The parameter screening is able to reduce the dimension and, thus, to accelerate the computation of the stochastic solution.","2013-09-16","2023-02-13 11:19:53","2024-02-07 20:12:51","2023-02-13 11:19:53","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1309.3853 [math]","Comment: 26 pages","C:\Users\isido\Zotero\storage\DUFXWZYZ\Steffes-lai e.a. - 2013 - Interpolation methods to compute statistics of a s.pdf","","random PDE","","","","","","","","","","","","","","","","","","","","arXiv:1309.3853","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYXF8QHR","preprint","2016","Gaß, Maximilian; Glau, Kathrin; Mahlstedt, Mirco; Mair, Maximilian","Chebyshev Interpolation for Parametric Option Pricing","","","","","http://arxiv.org/abs/1505.04648","Recurrent tasks such as pricing, calibration and risk assessment need to be executed accurately and in real-time. Simultaneously we observe an increase in model sophistication on the one hand and growing demands on the quality of risk management on the other. To address the resulting computational challenges, it is natural to exploit the recurrent nature of these tasks. We concentrate on Parametric Option Pricing (POP) and show that polynomial interpolation in the parameter space promises to reduce run-times while maintaining accuracy. The attractive properties of Chebyshev interpolation and its tensorized extension enable us to identify criteria for (sub)exponential convergence and explicit error bounds. We show that these results apply to a variety of European (basket) options and aﬃne asset models. Numerical experiments conﬁrm our ﬁndings. Exploring the potential of the method further, we empirically investigate the eﬃciency of the Chebyshev method for multivariate and path-dependent options.","2016-07-08","2023-02-13 11:27:30","2024-02-07 20:12:54","2023-02-13 11:27:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1505.04648 [q-fin]","Comment: Multivariate Option Pricing, Complexity Reduction, (Tensorized) Chebyshev Polynomials, Polynomial Interpolation, Fourier Transform Methods, Monte Carlo, Affine Processes","C:\Users\isido\Zotero\storage\3RRJ3YJB\Gaß e.a. - 2016 - Chebyshev Interpolation for Parametric Option Pric.pdf","","chebychev; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1505.04648","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4L7IEST7","preprint","2017","Glau, Kathrin; Herold, Paul; Madan, Dilip B.; Pötz, Christian","The Chebyshev method for the implied volatility","","","","","http://arxiv.org/abs/1710.01797","The implied volatility is a crucial element of any ﬁnancial toolbox, since it is used for quoting and the hedging of options as well as for model calibration. In contrast to the BlackScholes formula its inverse, the implied volatility, is not explicitly available and numerical approximation is required. We propose a bivariate interpolation of the implied volatility surface based on Chebyshev polynomials. This yields a closed-form approximation of the implied volatility, which is easy to implement and to maintain. We prove a subexponential error decay. This allows us to obtain an accuracy close to machine precision with polynomials of a low degree. We compare the performance of the method in terms of runtime and accuracy to the most common reference methods. In contrast to existing interpolation methods, the proposed method is able to compute the implied volatility for all relevant option data. In this context, numerical experiments conﬁrm a considerable increase in eﬃciency, especially for large data sets.","2017-10-04","2023-02-13 11:29:32","2024-02-07 20:13:01","2023-02-13 11:29:32","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1710.01797 [q-fin]","","C:\Users\isido\Zotero\storage\3VZDVADR\Glau e.a. - 2017 - The Chebyshev method for the implied volatility.pdf","","chebychev; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1710.01797","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9C5TFE5F","preprint","2020","Glau, Kathrin; Wunderlich, Linus","The Deep Parametric PDE Method: Application to Option Pricing","","","","","http://arxiv.org/abs/2012.06211","We propose the deep parametric PDE method to solve high-dimensional parametric partial diﬀerential equations. A single neural network approximates the solution of a whole family of PDEs after being trained without the need of sample solutions. As a practical application, we compute option prices in the multivariate Black-Scholes model. After a single training phase, the prices for diﬀerent time, state and model parameters are available in milliseconds. We evaluate the accuracy in the price and a generalisation of the implied volatility with examples of up to 25 dimensions. A comparison with alternative machine learning approaches, conﬁrms the eﬀectiveness of the approach.","2020-12-11","2023-02-13 12:12:25","2024-02-07 20:13:33","2023-02-13 12:12:25","","","","","","","The Deep Parametric PDE Method","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2012.06211 [q-fin]","Comment: Some examples can be reproduced in our Jupyter Notebook: https://github.com/LWunderlich/DeepPDE/blob/main/TwoAssetsExample/DeepParametricPDEExample.ipynb","C:\Users\isido\Zotero\storage\3AXRVL4J\Glau en Wunderlich - 2020 - The Deep Parametric PDE Method Application to Opt.pdf","","PDE; deep learning; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:2012.06211","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GC5A9D9V","preprint","2019","Glau, Kathrin; Kressner, Daniel; Statti, Francesco","Low-rank tensor approximation for Chebyshev interpolation in parametric option pricing","","","","","http://arxiv.org/abs/1902.04367","Treating high dimensionality is one of the main challenges in the development of computational methods for solving problems arising in ﬁnance, where tasks such as pricing, calibration, and risk assessment need to be performed accurately and in real-time. Among the growing literature addressing this problem, Gass et al. [14] propose a complexity reduction technique for parametric option pricing based on Chebyshev interpolation. As the number of parameters increases, however, this method is aﬀected by the curse of dimensionality. In this article, we extend this approach to treat high-dimensional problems: Additionally exploiting low-rank structures allows us to consider parameter spaces of high dimensions. The core of our method is to express the tensorized interpolation in tensor train (TT) format and to develop an eﬃcient way, based on tensor completion, to approximate the interpolation coeﬃcients. We apply the new method to two model problems: American option pricing in the Heston model and European basket option pricing in the multi-dimensional Black-Scholes model. In these examples we treat parameter spaces of dimensions up to 25. The numerical results conﬁrm the low-rank structure of these problems and the eﬀectiveness of our method compared to advanced techniques.","2019-02-12","2023-02-13 12:16:17","2024-02-07 20:13:31","2023-02-13 12:16:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1902.04367 [q-fin]","","C:\Users\isido\Zotero\storage\5FSQELEL\Glau e.a. - 2019 - Low-rank tensor approximation for Chebyshev interp.pdf","","chebychev; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1902.04367","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGJUSC5B","preprint","2019","Glau, Kathrin; Pachon, Ricardo; Pötz, Christian","Speed-up credit exposure calculations for pricing and risk management","","","","","http://arxiv.org/abs/1912.01280","We introduce a new method to calculate the credit exposure of European and path-dependent options. The proposed method is able to calculate accurate expected exposure and potential future exposure proﬁles under the riskneutral and the real-world measure. Key advantage of is that it delivers an accuracy comparable to a full re-evaluation and at the same time it is faster than a regression-based method. Core of the approach is solving a dynamic programming problem by function approximation. This yields a closed form approximation along the paths together with the option’s delta and gamma. The simple structure allows for highly eﬃcient evaluation of the exposures, even for a large number of simulated paths. The approach is ﬂexible in the model choice, payoﬀ proﬁles and asset classes. We validate the accuracy of the method numerically for three diﬀerent equity products and a Bermudan interest rate swaption. Benchmarking against the popular least-squares Monte Carlo approach shows that our method is able to deliver a higher accuracy in a faster runtime.","2019-12-03","2023-02-13 12:18:55","2024-02-07 20:13:29","2023-02-13 12:18:55","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1912.01280 [q-fin]","Comment: arXiv admin note: substantial text overlap with arXiv:1905.00238","C:\Users\isido\Zotero\storage\5CIFA7XE\Glau e.a. - 2019 - Speed-up credit exposure calculations for pricing .pdf","","option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1912.01280","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXWYV3MR","preprint","2018","Glau, Kathrin; Mahlstedt, Mirco; Pötz, Christian","A new approach for American option pricing: The Dynamic Chebyshev method","","","","","http://arxiv.org/abs/1806.05579","We introduce a new method to price American options based on Chebyshev interpolation. In each step of a dynamic programming time-stepping we approximate the value function with Chebyshev polynomials. The key advantage of this approach is that it allows to shift the model-dependent computations into an oﬄine phase prior to the time-stepping. In the oﬄine part a family of generalised (conditional) moments is computed by an appropriate numerical technique such as a Monte Carlo, PDE or Fourier transform based method. Thanks to this methodological ﬂexibility the approach applies to a large variety of models. Online, the backward induction is solved on a discrete Chebyshev grid, and no (conditional) expectations need to be computed. For each time step the method delivers a closed form approximation of the price function along with the options’ delta and gamma. Moreover, the same family of (conditional) moments yield multiple outputs including the option prices for diﬀerent strikes, maturities and diﬀerent payoﬀ proﬁles. We provide a theoretical error analysis and ﬁnd conditions that imply explicit error bounds for a variety of stock price models. Numerical experiments conﬁrm the fast convergence of prices and sensitivities. An empirical investigation of accuracy and runtime also shows an eﬃciency gain compared with the least-square Monte-Carlo method introduced by Longstaﬀ and Schwartz (2001).","2018-06-14","2023-02-13 12:26:36","2024-02-07 20:13:27","2023-02-13 12:26:36","","","","","","","A new approach for American option pricing","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1806.05579 [q-fin]","","C:\Users\isido\Zotero\storage\5CD69T7N\Glau e.a. - 2018 - A new approach for American option pricing The Dy.pdf","","chebychev; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1806.05579","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XHY9MSE","journalArticle","2022","Nesterov, Yurii; Florea, Mihai I.","Gradient Methods with Memory","Optimization Methods and Software","","1055-6788, 1029-4937","10.1080/10556788.2020.1858831","http://arxiv.org/abs/2105.09241","In this paper, we consider gradient methods for minimizing smooth convex functions, which employ the information obtained at the previous iterations in order to accelerate the convergence towards the optimal solution. This information is used in the form of a piece-wise linear model of the objective function, which provides us with much better prediction abilities as compared with the standard linear model. To the best of our knowledge, this approach was never really applied in Convex Minimization to diﬀerentiable functions in view of the high complexity of the corresponding auxiliary problems. However, we show that all necessary computations can be done very eﬃciently. Consequently, we get new optimization methods, which are better than the usual Gradient Methods both in the number of oracle calls and in the computational time. Our theoretical conclusions are conﬁrmed by preliminary computational experiments.","2022-05-04","2023-02-14 11:11:38","2024-02-07 20:13:25","2023-02-14 11:11:38","936-953","","3","37","","Optimization Methods and Software","","","","","","","","en","","","","","arXiv.org","","arXiv:2105.09241 [math]","Comment: This is an Accepted Manuscript of an article published by Taylor \& Francis in Optimization Methods and Software on 13 Jan 2021, available at https://www.tandfonline.com/doi/10.1080/10556788.2020.1858831","C:\Users\isido\Zotero\storage\EZKMSTTQ\Nesterov en Florea - 2022 - Gradient Methods with Memory.pdf","","optimization; gradient descent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D9IG24US","journalArticle","2022","Anikin, Anton; Gasnikov, Alexander; Gornov, Alexander; Kamzolov, Dmitry; Maximov, Yury; Nesterov, Yurii","Efficient Numerical Methods to Solve Sparse Linear Equations with Application to PageRank","Optimization Methods and Software","","1055-6788, 1029-4937","10.1080/10556788.2020.1858297","http://arxiv.org/abs/1508.07607","Over the last two decades, the PageRank problem has received increased interest from the academic community as an efficient tool to estimate web-page importance in information retrieval. Despite numerous developments, the design of efficient optimization algorithms for the PageRank problem is still a challenge. This paper proposes three new algorithms with a linear-time complexity for solving the problem over a bounded-degree graph. The idea behind them is to set up the PageRank as a convex minimization problem over a unit simplex, and then solve it using iterative methods with small iteration complexity. Our theoretical results are supported by an extensive empirical justification using real-world and simulated data.","2022-05-04","2023-02-14 11:18:15","2024-02-07 20:13:37","2023-02-14 11:18:15","907-935","","3","37","","Optimization Methods and Software","","","","","","","","en","","","","","arXiv.org","","arXiv:1508.07607 [math]","Comment: 26 pages; Accepted to Optimization Methods and Software","C:\Users\isido\Zotero\storage\7AQ4PXSZ\Anikin e.a. - 2022 - Efficient Numerical Methods to Solve Sparse Linear.pdf","","linear systems; page rank","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JBYFX7Q","journalArticle","2021","Vicini, Delio; Speierer, Sébastien; Jakob, Wenzel","Path replay backpropagation: differentiating light paths using constant memory and linear time","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3450626.3459804","https://dl.acm.org/doi/10.1145/3450626.3459804","Differentiable physically-based rendering has become an indispensable tool for solving inverse problems involving light. Most applications in this area jointly optimize a large set of scene parameters to minimize an objective function, in which case reverse-mode differentiation is the method of choice for obtaining parameter gradients.             However, existing techniques that perform the necessary differentiation step suffer from either statistical bias or a prohibitive cost in terms of memory and computation time. For example, standard techniques for automatic differentiation based on program transformation or Wengert tapes lead to impracticably large memory usage when applied to physically-based rendering algorithms. A recently proposed adjoint method by Nimier-David et al. [2020] reduces this to a constant memory footprint, but the computation time for unbiased gradient estimates then becomes quadratic in the number of scattering events along a light path. This is problematic when the scene contains highly scattering materials like participating media.             In this paper, we propose a new unbiased backpropagation algorithm for rendering that only requires constant memory, and whose computation time is linear in the number of scattering events (i.e., just like path tracing). Our approach builds on the invertibility of the local Jacobian at scattering interactions to recover the various quantities needed for reverse-mode differentiation. Our method also extends to specular materials such as smooth dielectrics and conductors that cannot be handled by prior work.","2021-08-31","2023-02-17 21:29:58","2023-03-18 19:58:58","2023-02-17 21:29:58","1-14","","4","40","","ACM Trans. Graph.","Path replay backpropagation","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\CYDBTJMN\Vicini e.a. - 2021 - Path replay backpropagation differentiating light.pdf","","monte carlo; rendering; inverse problem","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXGSRH2D","preprint","2022","Sinha, Devang; Chakrabarty, Siddhartha P.","Multilevel Monte Carlo and its Applications in Financial Engineering","","","","","http://arxiv.org/abs/2209.14549","In this article, we present a review of the recent developments on the topic of Multilevel Monte Carlo (MLMC) algorithm, in the paradigm of applications in ﬁnancial engineering. We speciﬁcally focus on the recent studies conducted in two subareas, namely, option pricing and ﬁnancial risk management. For the former, the discussion involves incorporation of the importance sampling algorithm, in conjunction with the MLMC estimator, thereby constructing a hybrid algorithm in order to achieve reduction for the overall variance of the estimator. In case of the latter, we discuss the studies carried out in order to construct an efﬁcient algorithm in order to estimate the risk measures of Value-at-Risk (VaR) and Conditional Var (CVaR), in an efﬁcient manner. In this regard, we brieﬂy discuss the motivation and the construction of an adaptive sampling algorithm with an aim to efﬁciently estimate the nested expectation, which, in general is computationally expensive.","2022-09-29","2023-02-18 14:06:58","2024-02-07 20:14:06","2023-02-18 14:06:58","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.14549 [q-fin]","","C:\Users\isido\Zotero\storage\YZXZQ439\Sinha en Chakrabarty - 2022 - Multilevel Monte Carlo and its Applications in Fin.pdf","","monte carlo; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2209.14549","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MEA963SP","preprint","2020","Beck, Christian; Jentzen, Arnulf; Kruse, Thomas","Nonlinear Monte Carlo methods with polynomial runtime for high-dimensional iterated nested expectations","","","","","http://arxiv.org/abs/2009.13989","The approximative calculation of iterated nested expectations is a recurring challenging problem in applications. Nested expectations appear, for example, in the numerical approximation of solutions of backward stochastic diﬀerential equations (BSDEs), in the numerical approximation of solutions of semilinear parabolic partial diﬀerential equations (PDEs), in statistical physics, in optimal stopping problems such as the approximative pricing of American or Bermudan options, in risk measure estimation in mathematical ﬁnance, or in decisionmaking under uncertainty. Nested expectations which arise in the above named applications often consist of a large number of nestings. However, the computational eﬀort of standard nested Monte Carlo approximations for iterated nested expectations grows exponentially in the number of nestings and it remained an open question whether it is possible to approximately calculate multiply iterated high-dimensional nested expectations in polynomial time. In this article we tackle this problem by proposing and studying a new class of full-history recursive multilevel Picard (MLP) approximation schemes for iterated nested expectations. In particular, we prove under suitable assumptions that these MLP approximation schemes can approximately calculate multiply iterated nested expectations with a computational effort growing at most polynomially in the number of nestings K P N “ t1, 2, 3, . . .u, in the problem dimension d P N, and in the reciprocal 1{ε of the desired approximation accuracy ε P p0, 8q.","2020-09-29","2023-02-18 14:08:24","2024-02-07 20:14:09","2023-02-18 14:08:24","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2009.13989 [cs, math]","Comment: 47 pages","C:\Users\isido\Zotero\storage\DERMR4VB\Beck e.a. - 2020 - Nonlinear Monte Carlo methods with polynomial runt.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2009.13989","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3A4N44J","preprint","2019","Jacquier, Antoine; Malone, Emma R.; Oumgari, Mugad","Stacked Monte Carlo for option pricing","","","","","http://arxiv.org/abs/1903.10795","We introduce a stacking version of the Monte Carlo algorithm in the context of option pricing. Introduced recently for aeronautic computations, this simple technique, in the spirit of current machine learning ideas, learns control variates by approximating Monte Carlo draws with some speciﬁed function. We describe the method from ﬁrst principles and suggest appropriate ﬁts, and show its eﬃciency to evaluate European and Asian Call options in constant and stochastic volatility models.","2019-03-26","2023-02-18 14:10:45","2023-03-18 19:58:20","2023-02-18 14:10:45","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1903.10795 [math, q-fin]","Comment: 12 pages, 13 Figures","C:\Users\isido\Zotero\storage\XMLJ4K7G\Jacquier e.a. - 2019 - Stacked Monte Carlo for option pricing.pdf","","monte carlo","Mathematics - Probability; Quantitative Finance - Computational Finance","","","","","","","","","","","","","","","","","","","arXiv:1903.10795","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KA66DW9Q","encyclopediaArticle","2022","","Malliavin calculus","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Malliavin_calculus&oldid=1115107254","In probability theory and related fields, Malliavin calculus is a set of mathematical techniques and ideas that extend the mathematical field of calculus of variations from  deterministic functions to stochastic processes. In particular, it allows the computation of derivatives of random variables. Malliavin calculus is also called the stochastic calculus of variations. P. Malliavin first initiated the calculus on infinite dimensional space. Then, the significant contributors such as S. Kusuoka, D. Stroock, Bismut, S. Watanabe, I. Shigekawa, and so on finally completed the foundations. Malliavin calculus is named after Paul Malliavin whose ideas led to a proof that Hörmander's condition implies the existence and smoothness of a density for the solution of a stochastic differential equation; Hörmander's original proof was based on the theory of  partial differential equations. The calculus has been applied to stochastic partial differential equations as well. The calculus allows integration by parts with random variables; this operation is used in mathematical finance to compute the sensitivities of financial derivatives. The calculus has applications in, for example, stochastic filtering.","2022-10-09","2023-02-20 11:46:29","2023-02-20 11:46:29","2023-02-20 11:46:29","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1115107254","","C:\Users\isido\Zotero\storage\SJFH7GTJ\Malliavin_calculus.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FH7EVQT9","preprint","2022","Pentland, Kamran; Tamborrino, Massimiliano; Sullivan, T. J.","Error bound analysis of the stochastic parareal algorithm","","","","","http://arxiv.org/abs/2211.05496","Stochastic parareal (SParareal) is a probabilistic variant of the popular parallel-intime algorithm known as parareal. Similarly to parareal, it combines ﬁne- and coarse-grained solutions to an ordinary diﬀerential equation (ODE) using a predictor-corrector (PC) scheme. The key diﬀerence is that carefully chosen random perturbations are added to the PC to try to accelerate the location of a stochastic solution to the ODE. In this paper, we derive superlinear and linear mean-square error bounds for SParareal applied to nonlinear systems of ODEs using diﬀerent types of perturbations. We illustrate these bounds numerically on a linear system of ODEs and a scalar nonlinear ODE, showing a good match between theory and numerics.","2022-11-10","2023-02-25 20:00:14","2024-02-07 20:15:07","2023-02-25 20:00:14","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.05496 [cs, math, stat]","","C:\Users\isido\Zotero\storage\STV8JTHP\Pentland e.a. - 2022 - Error bound analysis of the stochastic parareal al.pdf","","ODE; parareal","","","","","","","","","","","","","","","","","","","","arXiv:2211.05496","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XALDZ5Y","preprint","2022","Carrel, Benjamin; Gander, Martin J.; Vandereycken, Bart","Low-rank Parareal: a low-rank parallel-in-time integrator","","","","","http://arxiv.org/abs/2203.08455","In this work, the Parareal algorithm is applied to evolution problems that admit good low-rank approximations and for which the dynamical low-rank approximation (DLRA) can be used as time stepper. Many discrete integrators for DLRA have recently been proposed, based on splitting the projected vector ﬁeld or by applying projected Runge–Kutta methods. The cost and accuracy of these methods are mostly governed by the rank chosen for the approximation. These properties are used in a new method, called low-rank Parareal, in order to obtain a time-parallel DLRA solver for evolution problems. The algorithm is analyzed on aﬃne linear problems and the results are illustrated numerically.","2022-09-13","2023-02-25 20:03:50","2024-02-07 20:15:03","2023-02-25 20:03:50","","","","","","","Low-rank Parareal","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2203.08455 [cs, math]","","C:\Users\isido\Zotero\storage\L5268KDM\Carrel e.a. - 2022 - Low-rank Parareal a low-rank parallel-in-time int.pdf","","ODE; parareal","","","","","","","","","","","","","","","","","","","","arXiv:2203.08455","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S8V3RHIR","journalArticle","2008","Gander, M.; Petcu, M.","Analysis of a Krylov subspace enhanced parareal algorithm for linear problems","ESAIM: Proceedings","","1270-900X","10.1051/proc:082508","http://www.esaim-proc.org/10.1051/proc:082508","The parareal algorithm is a numerical method to integrate evolution problems on parallel computers. The performance of the algorithm is well understood for diﬀusive problems, and it can have spectacular performance when applied to certain non-linear problems. Its convergence properties are however less favorable for hyperbolic problems. We present and analyze in this paper a variant of the parareal algorithm, recently proposed in the PITA framework for systems of second order ordinary diﬀerential equations.","2008","2023-02-25 20:07:34","2023-08-11 18:44:57","2023-02-25 20:07:33","114-129","","","25","","ESAIM: Proc.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\Q9UAI328\Gander en Petcu - 2008 - Analysis of a Krylov subspace enhanced parareal al.pdf","","ODE; parareal","","Cancès, E.; Faure, S.; Graille, B.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDXUPPVJ","preprint","2023","Gander, M. J.; Lunet, T.; Ruprecht, D.; Speck, R.","A unified analysis framework for iterative parallel-in-time algorithms","","","","","http://arxiv.org/abs/2203.16069","Parallel-in-time integration has been the focus of intensive research eﬀorts over the past two decades due to the advent of massively parallel computer architectures and the scaling limits of purely spatial parallelization. Various iterative parallel-in-time (PinT) algorithms have been proposed, like Parareal, PFASST, MGRIT, and Space-Time Multi-Grid (STMG). These methods have been described using diﬀerent notations, and the convergence estimates that are available are diﬃcult to compare. We describe Parareal, PFASST, MGRIT and STMG for the Dahlquist model problem using a common notation and give precise convergence estimates using generating functions. This allows us, for the ﬁrst time, to directly compare their convergence. We prove that all four methods eventually converge super-linearly, and also compare them numerically. The generating function framework provides further opportunities to explore and analyze existing and new methods.","2023-02-08","2023-02-25 20:11:45","2024-04-17 08:51:55","2023-02-25 20:11:45","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2203.16069 [cs, math]","","C:\Users\isido\Zotero\storage\ARYQ6U3W\2203.16069.pdf","","ODE; parareal","","","","","","","","","","","","","","","","","","","","arXiv:2203.16069","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VSZRFWNB","preprint","2020","Maday, Y.; Mula, O.","An Adaptive Parareal Algorithm","","","","","http://arxiv.org/abs/1909.08333","In this paper, we consider the problem of accelerating the numerical simulation of time dependent problems by time domain decomposition. The available algorithms enabling such decompositions present severe eﬃciency limitations and are an obstacle for the solution of large scale and high dimensional problems. Our main contribution is the improvement of the parallel eﬃciency of the parareal in time method. The parareal method is based on combining predictions made by a numerically inexpensive solver (with coarse physics and/or coarse resolution) with corrections coming from an expensive solver (with highﬁdelity physics and high resolution). At convergence, the algorithm provides a solution that has the ﬁne solver’s high-ﬁdelity physics and high resolution. In the classical version, the ﬁne solver has a ﬁxed high accuracy which is the major obstacle to achieve a competitive parallel eﬃciency. In this paper, we develop an adaptive variant that overcomes this obstacle by dynamically increasing the accuracy of the ﬁne solver across the parareal iterations. We theoretically show that the parallel eﬃciency becomes very competitive in the ideal case where the cost of the coarse solver is small, thus proving that the only remaining factors impeding full scalability become the cost of the coarse solver and communication time. The developed theory has also the merit of setting a general framework to understand the success of several extensions of parareal based on iteratively improving the quality of the ﬁne solver and re-using information from previous parareal steps. We illustrate the actual performance of the method in stiﬀ ODEs, which are a challenging family of problems since the only mechanism for adaptivity is time and eﬃciency is aﬀected by the cost of the coarse solver.","2020-03-26","2023-02-25 20:14:10","2024-02-07 20:14:58","2023-02-25 20:14:10","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1909.08333 [cs, math]","","C:\Users\isido\Zotero\storage\3JZWT34D\Maday en Mula - 2020 - An Adaptive Parareal Algorithm.pdf","","ODE; parareal","","","","","","","","","","","","","","","","","","","","arXiv:1909.08333","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHSY4AN9","encyclopediaArticle","2023","","Monte Carlo integration","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Monte_Carlo_integration&oldid=1135560956","In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.There are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, importance sampling, sequential Monte Carlo (also known as a particle filter), and mean-field particle methods.","2023-01-25","2023-03-01 17:30:09","2023-03-18 19:56:43","2023-03-01 17:30:09","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1135560956","","C:\Users\isido\Zotero\storage\GELHMB8Z\Monte_Carlo_integration.html","","monte carlo; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJNXRK4T","journalArticle","2021","Lepage, G. Peter","Adaptive Multidimensional Integration: VEGAS Enhanced","Journal of Computational Physics","","00219991","10.1016/j.jcp.2021.110386","http://arxiv.org/abs/2009.05112","We describe a new algorithm, VEGAS+, for adaptive multidimensional Monte Carlo integration. The new algorithm adds a second adaptive strategy, adaptive stratiﬁed sampling, to the adaptive importance sampling that is the basis for its widely used predecessor VEGAS. Both VEGAS and VEGAS+ are effective for integrands with large peaks, but VEGAS+ can be much more effective for integrands with multiple peaks or other signiﬁcant structures aligned with diagonals of the integration volume. We give examples where VEGAS+ is 2–19× more accurate than VEGAS. We also show how to combine VEGAS+ with other integrators, such as the widely available MISER algorithm, to make new hybrid integrators. For a different kind of hybrid, we show how to use integrand samples, generated using MCMC or other methods, to optimize VEGAS+ before integrating. We give an example where preconditioned VEGAS+ is more than 100× as efﬁcient as VEGAS+ without preconditioning. Finally, we give examples where VEGAS+ is more than 10× as efﬁcient as MCMC for Bayesian integrals with D = 3 and 21 parameters. We explain why VEGAS+ will often outperform MCMC for small and moderate sized problems.","2021-08","2023-03-01 17:53:35","2024-02-07 20:15:12","2023-03-01 17:53:35","110386","","","439","","Journal of Computational Physics","Adaptive Multidimensional Integration","","","","","","","en","","","","","arXiv.org","","arXiv:2009.05112 [hep-ph, physics:physics]","Comment: 23 pages, 11 figures","C:\Users\isido\Zotero\storage\HQJAQY43\Lepage - 2021 - Adaptive Multidimensional Integration VEGAS Enhan.pdf","","monte carlo; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKWA9A36","journalArticle","2021","Mousavi, Ali; Monsefi, Reza; Elvira, Víctor","Hamiltonian Adaptive Importance Sampling","IEEE Signal Processing Letters","","1070-9908, 1558-2361","10.1109/LSP.2021.3068616","http://arxiv.org/abs/2209.13716","Importance sampling (IS) is a powerful Monte Carlo (MC) methodology for approximating integrals, for instance in the context of Bayesian inference. In IS, the samples are simulated from the so-called proposal distribution, and the choice of this proposal is key for achieving a high performance. In adaptive IS (AIS) methods, a set of proposals is iteratively improved. AIS is a relevant and timely methodology although many limitations remain yet to be overcome, e.g., the curse of dimensionality in high-dimensional and multi-modal problems. Moreover, the Hamiltonian Monte Carlo (HMC) algorithm has become increasingly popular in machine learning and statistics. HMC has several appealing features such as its exploratory behavior, especially in high-dimensional targets, when other methods suffer. In this paper, we introduce the novel Hamiltonian adaptive importance sampling (HAIS) method. HAIS implements a two-step adaptive process with parallel HMC chains that cooperate at each iteration. The proposed HAIS efﬁciently adapts a population of proposals, extracting the advantages of HMC. HAIS can be understood as a particular instance of the generic layered AIS family with an additional resampling step. HAIS achieves a signiﬁcant performance improvement in high-dimensional problems w.r.t. state-of-the-art algorithms. We discuss the statistical properties of HAIS and show its high performance in two challenging examples.","2021","2023-03-01 18:11:46","2024-02-07 20:15:19","2023-03-01 18:11:46","713-717","","","28","","IEEE Signal Process. Lett.","","","","","","","","en","","","","","arXiv.org","","arXiv:2209.13716 [cs, stat]","","C:\Users\isido\Zotero\storage\CKVBTMH7\Mousavi e.a. - 2021 - Hamiltonian Adaptive Importance Sampling.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MV9IBLKD","journalArticle","2020","Carrazza, Stefano; Cruz-Martinez, Juan M.","VegasFlow: accelerating Monte Carlo simulation across multiple hardware platforms","Computer Physics Communications","","00104655","10.1016/j.cpc.2020.107376","http://arxiv.org/abs/2002.12921","We present VegasFlow , a new software for fast evaluation of high dimensional integrals based on Monte Carlo integration techniques designed for platforms with hardware accelerators. The growing complexity of calculations and simulations in many areas of science have been accompanied by advances in the computational tools which have helped their developments. VegasFlow enables developers to delegate all complicated aspects of hardware or platform implementation to the library so they can focus on the problem at hand. This software is inspired on the Vegas algorithm, ubiquitous in the particle physics community as the driver of cross section integration, and based on Google’s powerful TensorFlow library. We benchmark the performance of this library on many diﬀerent consumer and professional grade GPUs and CPUs.","2020-09","2023-03-01 18:42:17","2024-02-07 20:15:50","2023-03-01 18:42:17","107376","","","254","","Computer Physics Communications","VegasFlow","","","","","","","en","","","","","arXiv.org","","arXiv:2002.12921 [hep-ex, physics:hep-ph, physics:physics, stat]","Comment: 6 pages, 5 figures, final version published in CPC","C:\Users\isido\Zotero\storage\DVF3BGFQ\Carrazza en Cruz-Martinez - 2020 - VegasFlow accelerating Monte Carlo simulation acr.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CF2EK23Q","preprint","2017","Bendavid, Joshua","Efficient Monte Carlo Integration Using Boosted Decision Trees and Generative Deep Neural Networks","","","","","http://arxiv.org/abs/1707.00028","New machine learning based algorithms have been developed and tested for Monte Carlo integration based on generative Boosted Decision Trees and Deep Neural Networks. Both of these algorithms exhibit substantial improvements compared to existing algorithms for non-factorizable integrands in terms of the achievable integration precision for a given number of target function evaluations. Large scale Monte Carlo generation of complex collider physics processes with improved eﬃciency can be achieved by implementing these algorithms into commonly used matrix element Monte Carlo generators once their robustness is demonstrated and performance validated for the relevant classes of matrix elements.","2017-06-30","2023-03-01 19:30:24","2024-02-07 20:15:43","2023-03-01 19:30:24","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1707.00028 [hep-ph, physics:physics]","","C:\Users\isido\Zotero\storage\VAWIJ2CM\Bendavid - 2017 - Efficient Monte Carlo Integration Using Boosted De.pdf","","monte carlo; machine learning; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:1707.00028","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BBQTH2T","journalArticle","2017","Huang, Yipeng; Guo, Ning; Seok, Mingoo; Tsividis, Yannis; Sethumadhavan, Simha","Evaluation of an Analog Accelerator for Linear Algebra","","","","","","Due to the end of supply voltage scaling and the increasing percentage of dark silicon in modern integrated circuits, researchers are looking for new scalable ways to get useful computation from existing silicon technology. In this paper we present a reconﬁgurable analog accelerator for solving systems of linear equations. Commonly perceived downsides of analog computing, such as low precision and accuracy, limited problem sizes, and difﬁculty in programming are all compensated for using methods we discuss. Based on a prototyped analog accelerator chip we compare the performance and energy consumption of the analog solver against an efﬁcient digital algorithm running on a CPU, and ﬁnd that the analog accelerator approach may be an order of magnitude faster and provide one third energy savings, depending on the accelerator design. Due to the speed and efﬁciency of linear algebra algorithms running on digital computers, an analog accelerator that matches digital performance needs a large silicon footprint. Finally, we conclude that problem classes outside of systems of linear equations may hold more promise for analog acceleration.","2017-06","2023-03-01 19:33:00","2024-08-12 14:29:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\TF7SI6TR\Huang e.a. - Evaluation of an Analog Accelerator for Linear Alg.pdf","","analog","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72HKM9L7","encyclopediaArticle","2023","","Analog computer","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Analog_computer&oldid=1140592830","An analog computer or analogue computer is a type of computer that uses the continuous variation aspect of physical phenomena such as electrical, mechanical, or hydraulic quantities (analog signals) to model the problem being solved. In contrast, digital computers represent varying quantities symbolically and by discrete values of both time and amplitude (digital signals). Analog computers can have a very wide range of complexity. Slide rules and nomograms are the simplest, while naval gunfire control computers and large hybrid digital/analog computers were among the most complicated. Complex mechanisms for process control and protective relays used analog computation to perform control and protective functions. Analog computers were widely used in scientific and industrial applications even after the advent of digital computers, because at the time they were typically much faster, but they started to become obsolete as early as the 1950s and 1960s, although they remained in use in some specific applications, such as aircraft flight simulators, the flight computer in aircraft, and for teaching control systems in universities. Perhaps the most relatable example of analog computers are mechanical watches where the continuous and periodic rotation of interlinked gears drives the seconds, minutes and hours needles in the clock. More complex applications, such as aircraft flight simulators and synthetic-aperture radar, remained the domain of analog computing (and hybrid computing) well into the 1980s, since digital computers were insufficient for the task.","2023-02-20","2023-03-01 20:03:49","2023-03-18 19:48:04","2023-03-01 20:03:49","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1140592830","","C:\Users\isido\Zotero\storage\78SJPMU4\Analog_computer.html","","analog","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUF8X5CR","journalArticle","2017","Guo, Ning","Investigation of Energy-Efficient Hybrid Analog/Digital Approximate Computation in Continuous Time","","","","","","This work investigates energy-efficient approximate computation for solving differential equations. It extends the analog computing techniques to a new paradigm: continuous-time hybrid computation, where both analog and digital circuits operate in continuous time. In this approach, the time intervals in the digital signals contain important information. Unlike conventional synchronous digital circuits, continuous-time digital signals offer the benefits of adaptive power dissipation and no quantization noise. Two prototype chips have been fabricated in 65 nm CMOS technology and tested successfully. The first chip is capable of solving nonlinear differential equations up to 4th order, and the second chip scales up to 16th order based on the first chip. Nonlinear functions are generated by a programmable, clockless, continuous-time 8-bit hybrid architecture (ADC+SRAM+DAC). Digitally-assisted calibration is used in all analog/mixed-signal blocks. Compared to the prior art [1], our chips makes possible arbitrary nonlinearities and achieves 16× lower power dissipation, thanks to technology scaling and extensive use of class-AB analog blocks Typically, the unit achieves a computational accuracy of about 0.5% to 5% RMS, solution times from a fraction of 1 μs to several hundred μs, and total computational energy from a fraction of 1 nJ to hundreds of nJ, depending on equation details. Very significant advantages are observed in computational speed and energy (over two orders of magnitude and over one order of magnitude, respectively) compared to those obtained with a modern MSP430 microcontroller for the same RMS error.","2017","2023-03-01 20:15:03","2024-08-12 14:23:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\DZW3H37R\Guo - Investigation of Energy-Efficient Hybrid AnalogDi.pdf","","analog; ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4FTH329","conferencePaper","2018","Huang, Yipeng; Guo, Ning; Sethumadhavan, Simha; Seok, Mingoo; Tsividis, Yannis","A Case Study in Analog Co-Processing for Solving Stochastic Differential Equations","2018 IEEE 23rd International Conference on Digital Signal Processing (DSP)","978-1-5386-6811-5","","10.1109/ICDSP.2018.8631831","https://ieeexplore.ieee.org/document/8631831/","Stochastic differential equations (SDEs) are an important class of mathematical models for areas such as physics and ﬁnance. Usually the model outputs are in the form of statistics of the dependent variables, generated from many solutions of the SDE using different samples of the random variables. Challenges in using existing conventional digital computer architectures for solving SDEs include: rapidly generating the random input variables for the SDE solutions, and having to use numerical integration to solve the differential equations. Recent work by our group has explored using hybrid analog-digital computing to solve differential equations. In the hybrid computing model, we solve differential equations by encoding variables as continuous values, which evolve in continuous time. In this paper we review the prior work, and study using the architecture, in conjunction with analog noise, to solve a canonical SDE, the Black-Scholes SDE.","2018-11","2023-03-01 20:16:01","2023-03-18 19:31:02","2023-03-01 20:16:01","1-5","","","","","","","","","","","IEEE","Shanghai, China","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\DVZPDWCW\Huang e.a. - 2018 - A Case Study in Analog Co-Processing for Solving S.pdf","","analog","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE 23rd International Conference on Digital Signal Processing (DSP)","","","","","","","","","","","","","","",""
"EF36SFGB","conferencePaper","2017","Huang, Yipeng; Guo, Ning; Seok, Mingoo; Tsividis, Yannis; Mandli, Kyle; Sethumadhavan, Simha","Hybrid analog-digital solution of nonlinear partial differential equations","Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture","978-1-4503-4952-9","","10.1145/3123939.3124550","https://dl.acm.org/doi/10.1145/3123939.3124550","We tackle the important problem class of solving nonlinear partial di↵erential equations. While nonlinear PDEs are typically solved in high-performance supercomputers, they are increasingly used in graphics and embedded systems, where e ciency is important.","2017-10-14","2023-03-01 20:17:38","2023-08-11 18:43:33","2023-03-01 20:17:38","665-678","","","","","","","","","","","ACM","Cambridge Massachusetts","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\WZ7SMKY8\Huang e.a. - 2017 - Hybrid analog-digital solution of nonlinear partia.pdf","","PDE; analog; gradient descent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","MICRO-50: The 50th Annual IEEE/ACM International Symposium on Microarchitecture","","","","","","","","","","","","","","",""
"QM4XAKYD","journalArticle","2019","Georgiev, Iliyan; Misso, Zackary; Hachisuka, Toshiya; Nowrouzezahrai, Derek; Křivánek, Jaroslav; Jarosz, Wojciech","Integral formulations of volumetric transmittance","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3355089.3356559","https://dl.acm.org/doi/10.1145/3355089.3356559","Computing the light attenuation between two given points is an essential yet expensive task in volumetric light transport simulation. Existing unbiased transmittance estimators are all based on ""null-scattering"" random walks enabled by augmenting the media with fictitious matter. This formulation prevents the use of traditional Monte Carlo estimator variance analysis, thus the efficiency of such methods is understood from a mostly empirical perspective. In this paper, we present several novel integral formulations of volumetric transmittance in which existing estimators arise as direct Monte Carlo estimators. Breaking from physical intuition, we show that the null-scattering concept is not strictly required for unbiased transmittance estimation, but is a form of control variates for effectively reducing variance. Our formulations bring new insight into the problem and the efficiency of existing estimators. They also provide a framework for devising new types of transmittance estimators with distinct and complementary performance tradeoffs, as well as a clear recipe for applying sample stratification.","2019-12-31","2023-03-02 20:25:03","2023-03-18 16:07:44","2023-03-02 20:25:03","1-17","","6","38","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\J48ZZ86P\3355089.pdf","","monte carlo; rendering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLKQLSFS","preprint","2022","Goda, Takashi; Kazashi, Yoshihito; Suzuki, Yuya","Randomizing the trapezoidal rule gives the optimal RMSE rate in Gaussian Sobolev spaces","","","","","http://arxiv.org/abs/2212.11476","Randomized quadratures for integrating functions in Sobolev spaces of order α ≥ 1, where the integrability condition is with respect to the Gaussian measure, are considered. In this function space, the optimal rate for the worst-case root-mean-squared error (RMSE) is established. Here, optimality is for a general class of quadratures, in which adaptive non-linear algorithms with a possibly varying number of function evaluations are also allowed. The optimal rate is given by showing matching bounds. First, a lower bound on the worst-case RMSE of O(n−α−1/2) is proven, where n denotes an upper bound on the expected number of function evaluations. It turns out that a suitably randomized trapezoidal rule attains this rate, up to a logarithmic factor. A practical error estimator for this trapezoidal rule is also presented. Numerical results support our theory.","2022-12-21","2023-03-06 09:27:13","2024-04-17 08:51:45","2023-03-06 09:27:13","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2212.11476 [cs, math]","Comment: 21 pages; Comment: 21 pages","C:\Users\isido\Zotero\storage\E7GZXZDN\Goda e.a. - 2022 - Randomizing the trapezoidal rule gives the optimal.pdf","","IBC; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2212.11476","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AM7SI2XZ","preprint","2022","Mitchell, William; Natkin, Abbie; Robertson, Paige; Sullivan, Marika; Yu, Xuechen; Zhu, Chenxin","Decomposition and conformal mapping techniques for the quadrature of nearly singular integrals","","","","","http://arxiv.org/abs/2210.09954","Gauss-Legendre quadrature and the trapezoidal rule are powerful tools for numerical integration of analytic functions. For nearly singular problems, however, these standard methods become unacceptably slow. We discuss and generalize some existing methods for improving on these schemes when the location of the nearby singularity is known. We conclude with an application to some nearly singular surface integrals of viscous flow.","2022-10-18","2023-03-06 09:32:15","2024-02-07 20:16:29","2023-03-06 09:32:15","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.09954 [cs, math]","","C:\Users\isido\Zotero\storage\I4A6MUMA\2210.09954.pdf","","quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2210.09954","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWMQCLIT","preprint","2022","Izzo, Federico; Runborg, Olof; Tsai, Richard","Convergence of a class of high order corrected trapezoidal rules","","","","","http://arxiv.org/abs/2208.08216","We present convergence theory for corrected quadrature rules on uniform Cartesian grids for functions with a point singularity. We begin by deriving an error estimate for the punctured trapezoidal rule, and then derive error expansions. We deﬁne the corrected trapezoidal rules, based on the punctured trapezoidal rule, where the weights for the nodes close to the singularity are judiciously corrected based on these expansions. Then we deﬁne the composite corrected trapezoidal rules for a larger family of functions using series expansions around the point singularity and applying corrected trapezoidal rules appropriately. We prove that we can achieve high order accuracy by using a suﬃcient number of correction nodes around the point singularity and of expansion terms.","2022-08-27","2023-03-06 09:38:52","2024-02-07 20:16:31","2023-03-06 09:38:52","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2208.08216 [cs, math]","Comment: 23 pages, 1 figure","C:\Users\isido\Zotero\storage\7BMQ6MUX\2208.08216.pdf","","quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2208.08216","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVT896JW","journalArticle","2022","Song, Chongmin; Eisenträger, Sascha","High-order implicit time integration scheme based on Pad\'e expansions","Computer Methods in Applied Mechanics and Engineering","","00457825","10.1016/j.cma.2021.114436","http://arxiv.org/abs/2103.12282","A single-step high-order implicit time integration scheme for the solution of transient and wave propagation problems is presented. It is constructed from the Pad´e expansions of the matrix exponential solution of a system of ﬁrst-order ordinary diﬀerential equations formulated in the state-space. A computationally eﬃcient scheme is developed exploiting the techniques of polynomial factorization and partial fractions of rational functions, and by decoupling the solution for the displacement and velocity vectors. An important feature of the novel algorithm is that no direct inversion of the mass matrix is required. From the diagonal Pad´e expansion of order M a time-stepping scheme of order 2M is developed. Here, each elevation of the accuracy by two orders results in an additional system of real or complex sparse equations to be solved. These systems are comparable in complexity to the standard Newmark method, i.e., the eﬀective system matrix is a linear combination of the static stiﬀness, damping, and mass matrices. It is shown that the second-order scheme is equivalent to Newmark’s constant average acceleration method, often also referred to as trapezoidal rule. The proposed time integrator has been implemented in MATLAB using the built-in direct linear equation solvers. In this article, numerical examples featuring nearly one million degrees of freedom are presented. High-accuracy and eﬃciency in comparison with common second-order time integration schemes are observed. The MATLAB-implementation is available from the authors upon request or from the GitHub repository (to be added).","2022-02","2023-03-06 09:42:24","2024-02-07 20:16:40","2023-03-06 09:42:24","114436","","","390","","Computer Methods in Applied Mechanics and Engineering","","","","","","","","en","","","","","arXiv.org","","arXiv:2103.12282 [cs, math]","Comment: 43 pages, 19 figures","C:\Users\isido\Zotero\storage\NTJ24PYR\2103.12282.pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YA4LBIR","journalArticle","2017","Kruse, Raphael; Wu, Yue","Error analysis of randomized Runge-Kutta methods for differential equations with time-irregular coefficients","Computational Methods in Applied Mathematics","","1609-9389, 1609-4840","10.1515/cmam-2016-0048","http://arxiv.org/abs/1701.03444","This paper contains an error analysis of two randomized explicit Runge-Kutta schemes for ordinary diﬀerential equations (ODEs) with timeirregular coeﬃcient functions. In particular, the methods are applicable to ODEs of Carath´eodory type, whose coeﬃcient functions are only integrable with respect to the time variable but are not assumed to be continuous. A further ﬁeld of application are ODEs with coeﬃcient functions that contain weak singularities with respect to the time variable.","2017-07-01","2023-03-06 10:03:32","2024-02-07 20:16:56","2023-03-06 10:03:32","479-498","","3","17","","","","","","","","","","en","","","","","arXiv.org","","arXiv:1701.03444 [math]","Comment: 24 pages, 3 figures","C:\Users\isido\Zotero\storage\WF8T4VEL\1701.03444.pdf","","ODE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K96C2LFK","preprint","2020","Wu, Yue","A randomised trapezoidal quadrature","","","","","http://arxiv.org/abs/2011.15086","A randomised trapezoidal quadrature rule is proposed for continuous functions which enjoys less regularity than commonly required. Indeed, we consider functions in some fractional Sobolev space. Various error bounds for this randomised rule are established while an error bound for classical trapezoidal quadrature is obtained for comparison. The randomised trapezoidal quadrature rule is shown to improve the order of convergence by half.","2020-12-02","2023-03-06 10:07:59","2024-02-07 20:17:12","2023-03-06 10:07:59","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2011.15086 [cs, math]","","C:\Users\isido\Zotero\storage\ZZ4A2GLQ\Wu - 2020 - A randomised trapezoidal quadrature.pdf","","IBC; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:2011.15086","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q24CAJ5X","preprint","2019","Grant, Cameron; Talvila, Erik","Elementary numerical methods for double integrals","","","","","http://arxiv.org/abs/1905.05805","Approximations to the integral $\int_a^b\int_c^d f(x,y)\,dy\,dx$ are obtained under the assumption that the partial derivatives of the integrand are in an $L^p$ space, for some $1\leq p\leq\infty$. We assume ${\lVert f_{xy}\rVert}_p$ is bounded (integration over $[a,b]\times[c,d]$), assume ${\lVert f_x(\cdot,c)\rVert}_p$ and ${\lVert f_x(\cdot,d)\rVert}_p$ are bounded (integration over $[a,b]$), and assume ${\lVert f_y(a,\cdot)\rVert}_p$ and ${\lVert f_y(b,\cdot)\rVert}_p$ are bounded (integration over $[c,d]$). The methods are elementary, using only integration by parts and H\""older's inequality. Versions of the trapezoidal rule, composite trapezoidal rule, midpoint rule and composite midpoint rule are given, with error estimates in terms of the above norms.","2019-05-14","2023-03-06 10:13:45","2024-02-07 20:17:21","2023-03-06 10:13:45","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1905.05805 [math]","Comment: To appear in Minnesota Journal of Undergraduate Mathematics","C:\Users\isido\Zotero\storage\C2T9FPFP\Grant en Talvila - 2019 - Elementary numerical methods for double integrals.pdf","","quadrature","","","","","","","","","","","","","","","","","","","","arXiv:1905.05805","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXGACHGR","preprint","2018","Brune, Carl R.","Derivative Corrections to the Trapezoidal Rule","","","","","http://arxiv.org/abs/1808.04743","Extensions to the trapezoidal rule using derivative information are studied for periodic integrands and integrals along the entire real line. Integrands which are analytic within a half plane or within a strip containing the path of integration are considered. Derivative-free error bounds are obtained. Alternative approaches to including derivative information are discussed.","2018-08-14","2023-03-06 10:17:12","2024-02-07 20:17:32","2023-03-06 10:17:12","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1808.04743 [math]","Comment: 19 pages, 2 figures","C:\Users\isido\Zotero\storage\ADTJQB7R\Brune - 2018 - Derivative Corrections to the Trapezoidal Rule.pdf","","quadrature","","","","","","","","","","","","","","","","","","","","arXiv:1808.04743","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2KTSR8W","preprint","2012","Talvila, Erik; Wiersma, Matthew","Simple derivation of basic quadrature formulas","","","","","http://arxiv.org/abs/1202.0249","Simple proofs of the midpoint, trapezoidal and Simpson’s rules are proved for numerical integration on a compact interval. The integrand is assumed to be twice continuously diﬀerentiable for the midpoint and trapezoidal rules, and to be four times continuously diﬀerentiable for Simpson’s rule. Errors are estimated in terms of the uniform norm of second or fourth derivatives of the integrand. The proof uses only integration by parts, applied to the second or fourth derivative of the integrand, multiplied by an appropriate polynomial or piecewise polynomial function. A corrected trapezoidal rule that includes the ﬁrst derivative of the integrand at the endpoints of the integration interval is also proved in this manner, the coeﬃcient in the error estimate being smaller than for the midpoint and trapezoidal rules. The proofs are suitable for presentation in a calculus or elementary numerical analysis class. Several student projects are suggested.","2012-02-01","2023-03-06 11:42:52","2024-02-07 20:17:46","2023-03-06 11:42:52","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1202.0249 [math]","Comment: To appear in Atlantic Electronic Journal of Mathematics","C:\Users\isido\Zotero\storage\XSLY6YKE\Talvila en Wiersma - 2012 - Simple derivation of basic quadrature formulas.pdf","","quadrature","","","","","","","","","","","","","","","","","","","","arXiv:1202.0249","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AW26Z694","journalArticle","2019","Khan, Md. Mamun-Ur-Rashid","Analytical Solution of Van Der Pol’s Differential Equation Using Homotopy Perturbation Method","Journal of Applied Mathematics and Physics","","2327-4352, 2327-4379","10.4236/jamp.2019.71001","http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jamp.2019.71001","In this research work, Homotopy perturbation method (HPM) is applied to find the approximate solution of the Van der Pol Differential equation (VDPDE), which is a well-known nonlinear ODE. Firstly, the approximate solution of Van Der Pol equation is developed using Dirichlet boundary conditions. Then a comparison between the present results and previously published results is presented and a good agreement is observed. Finally, HPM method is applied to find the approximate solution of VDPDE with Robin and Neumann boundary conditions.","2019","2023-03-07 13:05:03","2023-03-18 14:59:59","2023-03-07 13:05:03","1-12","","01","07","","JAMP","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\S442NHWI\Khan - 2019 - Analytical Solution of Van Der Pol’s Differential .pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J8QGLSEV","journalArticle","2013","Soomro, Abdul Sattar; Tularam, Gurudeo Anand; Shaikh, Muhammad Mujtaba","A Comparison of Numerical Methods for Solving the Unforced Van Der Pol’s Equation","","","","","","Due to the advancements in the field of computational mathematics, numerical methods are most widely being utilized to solve the equations arising in the fields of applied medical sciences, engineering and technology. In this paper, the numerical solutions of an important equation of applied dynamics: namely, the Unforced Van der Pol’s Equation (UFVDP) are obtained by reducing it to a system of two first order differential equations. The objective of this work is to investigate the efficiency of improved Heun’s (IH) method against the classical Runge-Kutta (RK4) and Mid-point (MP) methods for UFVDP equation. For analysis of accuracy, the Poincare-Lindstedt method has been used as a comparison criterion and respective error bounds are obtained. The results show that the popular RK4 method retains its better accuracy than other methods used for comparison.","2013","2023-03-07 13:38:13","2023-03-18 14:59:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\G8ZFGQ6S\Soomro e.a. - 2013 - A Comparison of Numerical Methods for Solving the .pdf","","ODE; perturbation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G6DWZHQ3","journalArticle","2019","Kritzer, Peter; Kuo, Frances Y.; Nuyens, Dirk; Ullrich, Mario","Lattice rules with random $n$ achieve nearly the optimal $\mathcal{O}(n^{-\alpha-1/2})$ error independently of the dimension","Journal of Approximation Theory","","00219045","10.1016/j.jat.2018.09.011","http://arxiv.org/abs/1706.04502","We analyze a new random algorithm for numerical integration of $d$-variate functions over $[0,1]^d$ from a weighted Sobolev space with dominating mixed smoothness $\alpha\ge 0$ and product weights $1\ge\gamma_1\ge\gamma_2\ge\cdots>0$, where the functions are continuous and periodic when $\alpha>1/2$. The algorithm is based on rank-$1$ lattice rules with a random number of points~$n$. For the case $\alpha>1/2$, we prove that the algorithm achieves almost the optimal order of convergence of $\mathcal{O}(n^{-\alpha-1/2})$, where the implied constant is independent of the dimension~$d$ if the weights satisfy $\sum_{j=1}^\infty \gamma_j^{1/\alpha}<\infty$. The same rate of convergence holds for the more general case $\alpha>0$ by adding a random shift to the lattice rule with random $n$. This shows, in particular, that the exponent of strong tractability in the randomized setting equals $1/(\alpha+1/2)$, if the weights decay fast enough. We obtain a lower bound to indicate that our results are essentially optimal. This paper is a significant advancement over previous related works with respect to the potential for implementation and the independence of error bounds on the problem dimension. Other known algorithms which achieve the optimal error bounds, such as those based on Frolov's method, are very difficult to implement especially in high dimensions. Here we adapt a lesser-known randomization technique introduced by Bakhvalov in 1961. This algorithm is based on rank-$1$ lattice rules which are very easy to implement given the integer generating vectors. A simple probabilistic approach can be used to obtain suitable generating vectors.","2019-04","2023-03-14 19:17:22","2024-02-07 20:18:37","2023-03-14 19:17:22","96-113","","","240","","Journal of Approximation Theory","","","","","","","","en","","","","","arXiv.org","","arXiv:1706.04502 [math]","Comment: 17 pages","C:\Users\isido\Zotero\storage\85WBUZK2\Kritzer e.a. - 2019 - Lattice rules with random $n$ achieve nearly the o.pdf","","IBC; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K2YAVL8Y","preprint","2006","Kacewicz, Boleslaw","Almost Optimal Solution of Initial-Value Problems by Randomized and Quantum Algorithms","","","","","http://arxiv.org/abs/quant-ph/0510045","We establish essentially optimal bounds on the complexity of initial-value problems in the randomized and quantum settings. For this purpose we define a sequence of new algorithms whose error/cost properties improve from step to step. These algorithms yield new upper complexity bounds, which differ from known lower bounds by only an arbitrarily small positive parameter in the exponent, and a logarithmic factor. In both the randomized and quantum settings, initial-value problems turn out to be essentially as difficult as scalar integration.","2006-10-09","2023-03-18 14:51:38","2024-02-07 20:18:59","2023-03-18 14:51:38","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:quant-ph/0510045","Comment: 16 pages, minor presentation changes","C:\Users\isido\Zotero\storage\5U78U57J\Kacewicz - 2006 - Almost Optimal Solution of Initial-Value Problems .pdf","","ODE; IBC","","","","","","","","","","","","","","","","","","","","arXiv:quant-ph/0510045","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8ZRQ62Y","journalArticle","1999","Coulibaly, Ibrahim; Lécot, Christian","A quasi-randomized Runge-Kutta method","Mathematics of Computation","","0025-5718","10.1090/S0025-5718-99-01056-X","http://www.ams.org/journal-getitem?pii=S0025-5718-99-01056-X","We analyze a quasi-Monte Carlo method to solve the initial-value problem for a system of diﬀerential equations y (t) = f (t, y(t)). The function f is smooth in y and we suppose that f and Dy1f are of bounded variation in t and that Dy2f is bounded in a neighborhood of the graph of the solution. The method is akin to the second order Heun method of the Runge-Kutta family. It uses a quasi-Monte Carlo estimate of integrals. The error bound involves the square of the step size as well as the discrepancy of the point set used for quasi-Monte Carlo approximation. Numerical experiments show that the quasi-randomized method outperforms a recently proposed randomized numerical method.","1999-04-01","2023-03-06 12:54:54","2023-03-18 14:57:39","2023-03-06 12:54:54","651-660","","226","68","","Math. Comp.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\LQ4QYKR2\Coulibaly en Lécot - 1999 - A quasi-randomized Runge-Kutta method.pdf","","ODE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65EP6SI2","webpage","1967","","Stiff Differential Equations","","","","","https://www.mathworks.com/company/newsletters/articles/stiff-differential-equations.html","Stiffness is a subtle, difficult, and important - concept in the numerical solution of ordinary differential equations.","1967-10-18","2023-03-07 10:18:54","2024-08-12 14:58:25","2023-03-07 10:18:54","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\674H6FG6\stiff-differential-equations.html","","ODE; stiff ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"286TPP28","encyclopediaArticle","2023","","Perturbation theory","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Perturbation_theory&oldid=1131120602","In mathematics and applied mathematics, perturbation theory comprises methods for finding an approximate solution to a problem, by starting from the exact solution of a related, simpler problem. A critical feature of the technique is a middle step that breaks the problem into ""solvable"" and ""perturbative"" parts.  In perturbation theory, the solution is expressed as a power series in a small parameter                         ε                 {\displaystyle \varepsilon }   . The first term is the known solution to the solvable problem.  Successive terms in the series at higher powers of                         ε                 {\displaystyle \varepsilon }    usually become smaller.  An approximate 'perturbation solution' is obtained by truncating the series, usually by keeping only the first two terms, the solution to the known problem and the 'first order' perturbation correction. Perturbation theory is used in a wide range of fields, and reaches its most sophisticated and advanced forms in quantum field theory. Perturbation theory (quantum mechanics) describes the use of this method in quantum mechanics. The field in general remains actively and heavily researched across multiple disciplines.","2023-01-02","2023-03-07 12:35:07","2023-03-18 14:28:41","2023-03-07 12:35:07","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1131120602","","C:\Users\isido\Zotero\storage\Y4XGJLUT\Perturbation_theory.html","","perturbation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMBUXMUA","journalArticle","2023","Tzitzouris, James A","Notes on Perturbation Techniques for ODEs","","","","","","","2023","2023-03-08 11:41:51","2024-08-12 14:40:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\DYKBRWX6\Tzitzouris - Notes on Perturbation Techniques for ODEs.pdf","","perturbation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8Y3CMITE","book","2015","Kuehn, Christian","Multiple Time Scale Dynamics","","978-3-319-12315-8 978-3-319-12316-5","","","https://link.springer.com/10.1007/978-3-319-12316-5","This book aims to provide an introduction to dynamical systems with multiple time scales. As in any overview book, several topics are covered only quite briefly. My aim was to focus on topics that seem to be less available in introductory form. However, I try to give a global view of the subject by covering a broad spectrum of ideas and tools. The detailed bibliography aims to direct the reader to further topics. To explain it with a simple metaphor: using this book should make you more familiar with a country’s map, culture, and main attractions rather than imparting details of every street in just one city. Both things are useful at times. The term “multiple time scale dynamics” is rather modern. The subject and many of its core ideas are much older. For example, “singular perturbation theory” or “multiscale systems” encompass a larger variety of topics than what I present here. On the one hand, no serious multidimensional spatial problems are considered in this book. Furthermore, there are many singularly perturbed problems that have very little to do with dynamical systems. On the other hand, ordinary differential equations (ODEs) with multiple time scales already contain motivation, technique, and intuition for more complicated scenarios. Classical singular perturbation theory for multiple time scale systems provides many asymptotic techniques centered on series expansions, matching, and averaging. These methods are still indispensable today, and this book gives an overview of them. However, the details are not covered, since many excellent introductory texts are available. The last two decades have brought major additional progress with a particular focus on geometric ideas as well as powerful numerical algorithms. A major goal of this book was to merge several viewpoints with a wide variety of different techniques into a unified framework. Another reason for the broad choice of topics was to make it easier for students and researchers new to the field to get a much quicker overview. Again, I would like to warn the reader that this book is obviously not a mathematical monograph aiming at a complete treatment of the entire field of multiple time scales. Some readers, particularly students, may wonder how a book of over 700 pages can be only an “introduction,” but let me point out that most chapters, and even many five-page sections, in this book in fact deserve their own mathematical monograph of 300 pages or more. A few such books have vi PREFACE been written, while many exist only in a distant, happier future. I encourage my colleagues working in the field—you know who you are—to begin work on such projects and fill in the missing mathematical details that I decided to leave out in order to make the subject much more accessible to beginners. Despite the simplifications, there seem to be several advantages of the style of presentation. The great diversity of the subject, ranging from mathematical theory in dynamics, analysis, geometry, topology, stochastics, and numerics to virtually all fields in science and engineering applications, easily becomes visible. The unity and interconnections between different approaches to multiple time scale problems can be identified much more readily. Also, scientists with particular applications in mind should find it easier to spot many potential tools right away, while a “purer” mathematician can use this text as a source book of open mathematical problems. The target audience of the book is senior undergraduates, graduate students, as well as researchers interested in using the theory of multiple time scale dynamics in nonlinear science, either from a theoretical or a mathematical modeling perspective. Section 1.1provides a more detailed guide to the book. Now I have the pleasure of thanking several colleagues, collaborators, and institutions that have helped to get this book started, keep it on track, and eventually push it over the finish line. First and foremost, I would like to thank my thesis adviser, John Guckenheimer, for introducing me to the field during my time as a graduate student. Undoubtedly, he shaped my view of the field, and without his support and encouragement, I would never have attempted to undertake a book project on multiple time scale systems. Important influences on this book during my postdoctoral years have come from my colleagues Thilo Gross, Peter Szmolyan, Nils Berglund, and Barbara Gentz. Thilo helped me to form bridges from multiscale dynamics to such seemingly distant areas as ecology, networks, systems biology, and statistical physics. I would like to thank Peter for sharing his tremendous insights into all aspects of geometric multiscale dynamics. Nils and Barbara have been constant sources of inspiration on everything stochastic. Although it is clear that I am responsible for all potential errors that may remain within this version, I would like to thank several colleagues who responded with valuable feedback—alerting me to anything from tiny typos to blatant blunders—in various draft versions of this book: Nils Berglund, Alan Champneys, Hayato Chiba, Mike Cortez, Peter De Maesschalck, John Guckenheimer, Pavel Gurevich, Annalisa Iuorio, Mike Jeffrey, Hans Kaper, Daniel Karrasch, Chris Jones, Ilona Kosiuk, Steven Lade, Gabriel Lord, Anatoly Neishtadt, Clare Perryman, Sofia Piltz, Nikola Popovic, Jens Rademacher, Martin Rasmussen, Martin Riedler, Stephen Schecter, Jan Sieber, Eric Siero, Peter Szmolyan, Frits Veerman, Martin Wechselberger, and Antonios Zagaris. Furthermore, I would like to thank the production staff at Springer for the handling of my manuscript. In particular, Achi Dosanjh has been extremely important and tremendously helpful in leading the entire editorial process. Regarding the formatting of the book, I would also like to thank Yuri Kuznetsov PREFACE vii who shared his LATEX book formatting preamble with me, from which I took some inspiration for the format of this book. Several anonymous referees also provided very valuable feedback, which helped to improve the book. During the writing of this book I have also benefited from the generous hospitality and financial support of various institutions, including Cornell University, the Max Planck Institute for Physics of Complex Systems, and the Vienna University of Technology. Furthermore, I would like to thank the Austrian Academy of Sciences for support via the “Austrian Programme for Advanced Research and Technology” and the European Commission for support via a “Marie Curie International Reintegration Grant.” The final push of this project has been supported through the program “Oberwolfach Leibniz Fellows” by the Mathematisches Forschungsinstitut Oberwolfach. Although it is obvious for an overview book on a topic, let me stress that I do not make any claims to novelty of its content. I have tried to summarize and condense the extensive literature on multiple time scale dynamics into a more accessible expository format. However, I can certainly say that during the writing of this book, several very natural new ideas arose. I hope that the research-oriented reader will have a similar experience and that this book will provide a starting point for new ideas in multiscale dynamics.","2015","2023-03-08 14:04:36","2023-08-01 13:48:59","2023-03-08 14:04:36","","","","191","","","","Applied Mathematical Sciences","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-319-12316-5","","C:\Users\isido\Zotero\storage\KEKJNI5S\Kuehn - 2015 - Multiple Time Scale Dynamics.pdf","","perturbation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SED9FB9T","document","2013","","intro perturbation theory","","","","","","First, let me say hello and welcome to the subject of perturbation methods. For those who may be unfamiliar with the topic, the title can be confusing. The first time I became aware of this was during a family reunion when someone asked what I did as a mathematician. This is not an easy question to answer, but I started by describing how a certain segment of the applied mathematics community was interested in problems that arise from physical problems. Examples such as water waves, sound propagation, and the aerodynamics of airplanes were discussed. The difficulty of solving such problems was also described in exaggerated detail. Next came the part about how one generally ends up using a computer to actually find the solution. At this point I editorialized on the limitations of computer solutions and why it is important to derive, if at all possible, accurate approximations of the solution. This lead naturally to the mentioning of asymptotics and perturbation methods. These terms ended the conversation because I was unprepared for their reactions. They were not sure exactly what asymptotics meant, but they were quite perplexed about perturbation methods. I tried, unsuccessfully, to explain what it means, but it was not until sometime later that I realized the difficulty. For them, as in Webster's Collegiate Dictionary, the first two meanings for the word perturb are ""to disturb greatly in mind (disquiet); to throw into confusion (disorder)."" Although a cynic might suggest this is indeed appropriate for the subject, the intent is exactly the opposite. (For a related comment, see Exercise 3.4.1(d).) In a nutshell, this book serves as an introduction into how to systematically construct an approximation of the solution of a problem that i viii Preface otherwise intractable. The methods all rely on there being a parameter in the problem that is relatively small. Such a situation is relatively common in applications, and this is one of the reasons that perturbation methods are a cornerstone of applied mathematics. One of the other cornerstones is scientific computing, and it is interesting that the two subjects have grown up together. However, this is not unexpected given their respective capabilities. When using a computer, one is capable of solving problems that are nonlinear, inhomogeneous, and multidimensional. Moreover, it is possible to achieve very high accuracy. The drawbacks are that computer solutions do not provide much insight into the physics of the problem (particularly for those who do not have access to the appropriate software or computer), and there is always the question as to whether or not the computed solution is correct. On the other hand, perturbation methods are also capable of dealing with nonlinear, inhomogeneous, and multidimensional problems (although not to the same extent as computer-generated solutions). The principal objective when using perturbation methods, at least as far as the author is concerned, is to provide a reasonably accurate expression for the solution. By doing this one is able to derive an understanding of the physics of the problem. Also, one can use the result, in conjunction with the original problem, to obtain more efficient numerical procedures for computing the solution. The methods covered in the text vary widely in their applicability. The first chapter introduces the fundamental ideas underlying asymptotic approximations. This includes their use in constructing approximate solutions of transcendental equations as well as differential equations. In the second chapter, matched asymptotic expansions are used to analyze problems with layers. Chapter 3 describes a method for dealing with problems with more than one time scale. In Chapter 4, the WKB method for analyzing linear singular perturbation problems is developed, while in Chapter 5 a method for dealing with materials containing disparate spatial scales (e.g., microscopic versus macroscopic) is discussed. The last chapter examines the topics of multiple solutions and stability. The mathematical prerequisites for this text include a basic background in differential equations and advanced calculus. In terms of difficulty, the chapters are written so that the first sections are either elementary or intermediate, while the later sections are somewhat more advanced. Also, the ideas developed in each chapter are applied to a spectrum of problems, including ordinary differential equations, partial differential equations, and difference equations. Scattered through the exercises are applications to integral equations, integra-differential equations, differential-difference equations, and delay equations. What will not be found is an in-depth discussion of the theory underlying the methods. This aspect of the subject is important, and references to the more theoretical work in the area are given in each chapter Preface ix The exercises in each section vary in their complexity. In addition to the more standard textbook problems, an attempt has been made to include problems from the research literature. The latter are intended to provide a window into the wide range of areas that use perturbation methods. Solutions to some of the exercises are available from the author's home page located at http://www.math.rpi.edu/""'holmes. Also located there is an errata list. Those who may want to make a contribution to one of these files, or have suggestions about the text, can reach the author at holmes@rpi.edu. I would like to express my gratitude to the many students who took my course in perturbation methods at Rensselaer. They helped me immeasurably in understanding the subject and provided much needed encouragement to write this book. It is a pleasure to acknowledge the suggestions of Jon Bell, Ash Kapila, and Bob O'Malley, who read early versions of the manuscript. I would also like to thank Julian Cole, who first introduced me to perturbation methods and is still, to this day, showing me what the subject is about.","2013","2023-03-08 17:07:46","2024-08-12 14:50:50","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\A7D8KXBJ\[Texts in Applied Mathematics №20] Mark H. Holmes (auth.) - Introduction to Perturbation Methods (1995, Springer) [10.1007_978-1-4612-5347-1] - libgen.li.pdf","","perturbation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQKMS4BA","webpage","2022","Wijgerden, Jeroen van","Tail recursion for branching problems","","","","","https://jeroenvanwijgerden.me/post/recursion-1/","Recently one of my clients asked me for help in learning about graphs. As graphs are a particular favorite topic of mine I took on this challenge with zeal and alacrity. In our ensuing sessions we touched upon some interesting topics regarding recursion, which inspired me to write this article. I assume you, dear reader, are already somewhat familiar with recursion. Nevertheless, I begin this article with a brief recap of a classic recursive solution: calculating a factorial. Then I introduce the what and why of tail recursion and how to modify regular recursion to tail recursion. The meat of this article is a discussion of how to use regular recursion and tail recursion to solve branching problems. The particular branching problem I use as an example is finding paths in a tree. I discuss not one but two different approaches to finding paths in trees. The first is more flexible, the second is more performant.","2022-02-16","2023-03-15 10:54:40","2023-08-01 13:46:26","2023-03-15 10:54:40","","","","","","","","","","","","","","en","","","","","","","Section: post","","C:\Users\isido\Zotero\storage\4BTFZQHH\recursion-1.html","","tail recursion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CB6RZ9HA","preprint","2013","Christopoulos, Demetris T.","Polynomial regression using trapezoidal rule for computing Legendre coefficients","","","","","http://arxiv.org/abs/1311.7525","We are presenting a method for computing the Fourier coeﬃcients of a given polynomial regression by using the trapezoidal rule for numerical integration. As function basis we use the orthogonal Legendre polynomials. The results are accurate and stable compared to Forsythe’s method.","2013-11-29","2023-03-06 11:42:06","2024-02-07 20:17:38","2023-03-06 11:42:06","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1311.7525 [math, stat]","Comment: 13 pages, 2 figures, 4 tables","C:\Users\isido\Zotero\storage\MNCK7RST\Christopoulos - 2013 - Polynomial regression using trapezoidal rule for c.pdf","","chebychev","","","","","","","","","","","","","","","","","","","","arXiv:1311.7525","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NGIU4CSX","encyclopediaArticle","2022","","Hardware acceleration","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Hardware_acceleration&oldid=1098931508","Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both. To perform computing tasks more quickly (or better in some other way), generally one can invest time and money in improving the software, improving the hardware, or both. There are various approaches with advantages and disadvantages in terms of decreased latency, increased throughput and reduced energy consumption. Typical advantages of focusing on software may include more rapid development, lower non-recurring engineering costs, heightened portability, and ease of updating features or patching bugs, at the cost of overhead to compute general operations. Advantages of focusing on hardware may include speedup, reduced power consumption, lower latency, increased parallelism and bandwidth, and better utilization of area and functional components available on an integrated circuit; at the cost of lower ability to update designs once etched onto silicon and higher costs of functional verification, and times to market. In the hierarchy of digital computing systems ranging from general-purpose processors to fully customized hardware, there is a tradeoff between flexibility and efficiency, with efficiency increasing by orders of magnitude when any given application is implemented higher up that hierarchy. This hierarchy includes general-purpose processors such as CPUs, more specialized processors such as GPUs, fixed-function implemented on field-programmable gate arrays (FPGAs), and fixed-function implemented on application-specific integrated circuits (ASICs).Hardware acceleration is advantageous for performance, and practical when the functions are fixed so updates are not as needed as in software solutions. With the advent of reprogrammable logic devices such as FPGAs, the restriction of hardware acceleration to fully fixed algorithms has eased since 2010, allowing hardware acceleration to be applied to problem domains requiring modification to algorithms and processing control flow. The disadvantage however, is that in many open source projects, it requires proprietary libraries that not all vendors are keen to distribute or expose, making it difficult to integrate in such projects.","2022-07-18","2023-03-01 19:25:27","2023-03-01 19:25:27","2023-03-01 19:25:27","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1098931508","","C:\Users\isido\Zotero\storage\DYFFG8BP\Hardware_acceleration.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SBW63VKY","webpage","2023","","parallelODEs.pdf","","","","","https://www.cs.usask.ca/~spiteri/M314/notes/parallelODEs.pdf","","2023-01-25","2023-03-18 20:28:55","2024-08-12 14:55:04","2023-01-25 10:18:45","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\NW56ESQ7\parallelODEs.pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7JCGRS9","journalArticle","2017","Ullrich, Mario","A Monte Carlo method for integration of multivariate smooth functions","SIAM Journal on Numerical Analysis","","0036-1429, 1095-7170","10.1137/16M1075557","http://arxiv.org/abs/1604.06008","We study a Monte Carlo algorithm that is based on a speciﬁc (randomly shifted and dilated) lattice point set. The main result of this paper is that the mean squared error for a given compactly supported, square-integrable function is bounded by n−1/2 times the L2-norm of the Fourier transform outside a region around the origin, where n is the expected number of function evaluations. As corollaries we obtain the optimal order of convergence for functions from the Sobolev spaces Hps with isotropic, anisotropic or mixed smoothness with given compact support for all values of the parameters. If the region of integration is the unit cube, we obtain the same optimal orders for functions without boundary conditions. This proves, in particular, that the optimal order of convergence in the latter case is n−s−1/2 for p ≥ 2, which is, in contrast to the case of deterministic algorithms, independent of the dimension. This shows that Monte Carlo algorithms can improve the order by more than n−1/2 for a whole class of natural function spaces. Note that a similar result (for a diﬀerent class) was obtained by Heinrich et al. [13].","2017-01","2023-05-25 16:15:41","2024-02-08 15:52:55","2023-05-25 16:15:41","1188-1200","","3","55","","SIAM J. Numer. Anal.","","","","","","","","en","","","","","arXiv.org","","arXiv:1604.06008 [math]","Comment: The numbering of the theorems differs from the published version","C:\Users\isido\Zotero\storage\WBZMRYM7\Ullrich - 2017 - A Monte Carlo method for integration of multivaria.pdf","","monte carlo; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9YCFEND","journalArticle","2018","Daluiso, Roberto","Second Order Sensitivities in Linear or Constant Time","SSRN Electronic Journal","","1556-5068","10.2139/ssrn.3201559","https://www.ssrn.com/abstract=3201559","We analyse and compare methods to compute the full set of second order sensitivities of a Monte Carlo price in a time which is at most O(N · T ) where N is the number of inputs and T is the time required to compute the price. The new ones include the ﬁrst algorithm which achieves a complexity O(T ) and has acceptable (in fact very low) statistical uncertainties at least in one relevant test case. Keywords: Greeks; Gamma; Vanna; Volga; Vomma; algorithmic differentiation; adjoints; pathwise differentiation; derivatives pricing; discontinuous payoffs; digital options.","2018","2023-05-24 16:49:50","2023-08-11 18:34:18","2023-05-24 16:49:50","","","","","","SSRN Journal","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\T5E6FRLG\Daluiso - 2018 - Second Order Sensitivities in Linear or Constant T.pdf","","option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRW7G2PA","preprint","2018","Elliott, Conal","The simple essence of automatic differentiation","","","","","http://arxiv.org/abs/1804.00746","Automatic diﬀerentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural speciﬁcation. The general algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms deﬁned here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.","2018-10-02","2023-05-23 13:26:37","2024-02-08 15:52:43","2023-05-23 13:26:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1804.00746 [cs]","Comment: 37 pages with proof appendices and 15 figures. Extended version of a paper appearing at ICFP 2018. More info at http://conal.net/papers/essence-of-ad/","C:\Users\isido\Zotero\storage\XCWM9PGQ\1804.00746.pdf","","optimization","","","","","","","","","","","","","","","","","","","","arXiv:1804.00746","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCQC24AA","webpage","2023","Ph.D, Jacob Marks","How I Turned My Company’s Docs into a Searchable Database with OpenAI","Medium","","","","https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736","For the past six months, I’ve been working at series A startup Voxel51, a and creator of the open source computer vision toolkit FiftyOne. As a machine learning engineer and developer evangelist, my job is to listen to our open source community and bring them what they need — new features, integrations, tutorials, workshops, you name it. A few weeks ago, we added native support for vector search engines and text similarity queries to FiftyOne, so that users can find the most relevant images in their (often massive — containing millions or tens of millions of samples) datasets, via simple natural language queries. This put us in a curious position: it was now possible for people using open source FiftyOne to readily search datasets with natural language queries, but using our documentation still required traditional keyword search. We have a lot of documentation, which has its pros and cons. As a user myself, I sometimes find that given the sheer quantity of documentation, finding precisely what I’m looking for requires more time than I’d like.","2023-04-27","2023-05-23 10:20:07","2024-08-08 13:02:54","2023-05-23 10:20:07","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\CPBYHIF6\how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736.html","","machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U82ZZXN2","preprint","2014","Mirza, Mehdi; Osindero, Simon","Conditional Generative Adversarial Nets","","","","","http://arxiv.org/abs/1411.1784","Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.","2014-11-06","2023-05-23 09:56:33","2024-02-08 15:52:37","2023-05-23 09:56:33","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1411.1784 [cs, stat]","","C:\Users\isido\Zotero\storage\C933QUW2\Mirza en Osindero - 2014 - Conditional Generative Adversarial Nets.pdf","","machine learning; GANS; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:1411.1784","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGY9QT3V","preprint","2016","Radford, Alec; Metz, Luke; Chintala, Soumith","Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","","","","","http://arxiv.org/abs/1511.06434","In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","2016-01-07","2023-05-23 09:39:01","2024-02-08 15:52:29","2023-05-23 09:39:01","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1511.06434 [cs]","Comment: Under review as a conference paper at ICLR 2016","C:\Users\isido\Zotero\storage\8NCTCL8G\Radford e.a. - 2016 - Unsupervised Representation Learning with Deep Con.pdf","","machine learning; GANS; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:1511.06434","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3TXQFB2","preprint","2023","Yang, Ling; Zhang, Zhilong; Song, Yang; Hong, Shenda; Xu, Runsheng; Zhao, Yue; Zhang, Wentao; Cui, Bin; Yang, Ming-Hsuan","Diffusion Models: A Comprehensive Survey of Methods and Applications","","","","","http://arxiv.org/abs/2209.00796","Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.","2023-03-23","2023-05-23 09:29:13","2024-02-08 15:52:25","2023-05-23 09:29:13","","","","","","","Diffusion Models","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.00796 [cs]","Comment: 49 pages, 17 figures, citing 337 (up-to-date) papers, project: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy","C:\Users\isido\Zotero\storage\8VW927IA\Yang e.a. - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf","","machine learning; diffusion model; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2209.00796","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTMPSBW8","preprint","2017","Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia","Attention Is All You Need","","","","","http://arxiv.org/abs/1706.03762","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","2017-12-05","2023-05-23 09:20:21","2023-08-03 19:20:08","2023-05-23 09:20:21","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1706.03762 [cs]","Comment: 15 pages, 5 figures","C:\Users\isido\Zotero\storage\I5AYFTK6\1706.03762.pdf","","machine learning; deep learning","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:1706.03762","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6V5DD3D","preprint","2021","Yang, Keyu; Chen, Lu; Zeng, Zhihao; Gao, Yunjun","FastSGD: A Fast Compressed SGD Framework for Distributed Machine Learning","","","","","http://arxiv.org/abs/2112.04291","With the rapid increase of big data, distributed Machine Learning (ML) has been widely applied in training large-scale models. Stochastic Gradient Descent (SGD) is arguably the workhorse algorithm of ML. Distributed ML models trained by SGD involve large amounts of gradient communication, which limits the scalability of distributed ML. Thus, it is important to compress the gradients for reducing communication. In this paper, we propose FastSGD, a Fast compressed SGD framework for distributed ML. To achieve a high compression ratio at a low cost, FastSGD represents the gradients as key-value pairs, and compresses both the gradient keys and values in linear time complexity. For the gradient value compression, FastSGD ﬁrst uses a reciprocal mapper to transform original values into reciprocal values, and then, it utilizes a logarithm quantization to further reduce reciprocal values to small integers. Finally, FastSGD ﬁlters reduced gradient integers by a given threshold. For the gradient key compression, FastSGD provides an adaptive ﬁne-grained delta encoding method to store gradient keys with fewer bits. Extensive experiments on practical ML models and datasets demonstrate that FastSGD achieves the compression ratio up to 4 orders of magnitude, and accelerates the convergence time up to 8×, compared with state-of-the-art methods.","2021-12-08","2023-05-22 19:05:26","2024-02-09 18:20:03","2023-05-22 19:05:26","","","","","","","FastSGD","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2112.04291 [cs]","","C:\Users\isido\Zotero\storage\S8Q4SDYG\2112.04291.pdf","","optimization; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","arXiv:2112.04291","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFGUWP52","preprint","2023","Ghaffari, Alireza; Tahaei, Marzieh S.; Tayaranian, Mohammadreza; Asgharian, Masoud; Nia, Vahid Partovi","Is Integer Arithmetic Enough for Deep Learning Training?","","","","","http://arxiv.org/abs/2207.08822","The ever-increasing computational complexity of deep learning models makes their training and deployment difﬁcult on various cloud and edge platforms. Replacing ﬂoating-point arithmetic with low-bit integer arithmetic is a promising approach to save energy, memory footprint, and latency of deep learning models. As such, quantization has attracted the attention of researchers in recent years. However, using integer numbers to form a fully functional integer training pipeline including forward pass, back-propagation, and stochastic gradient descent is not studied in detail. Our empirical and mathematical results reveal that integer arithmetic seems to be enough to train deep learning models. Unlike recent proposals, instead of quantization, we directly switch the number representation of computations. Our novel training method forms a fully integer training pipeline that does not change the trajectory of the loss and accuracy compared to ﬂoating-point, nor does it need any special hyper-parameter tuning, distribution adjustment, or gradient clipping. Our experimental results show that our proposed method is effective in a wide variety of tasks such as classiﬁcation (including vision transformers), object detection, and semantic segmentation.","2023-01-04","2023-05-22 19:01:46","2024-02-09 18:20:06","2023-05-22 19:01:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2207.08822 [cs]","Comment: final camera ready submitted to NeurIPS","C:\Users\isido\Zotero\storage\3P85UYGA\2207.08822.pdf","","optimization; machine learning; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","arXiv:2207.08822","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRJ7W6YJ","preprint","2023","Yan, Guangfeng; Li, Tan; Wu, Kui; Song, Linqi","Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning","","","","","http://arxiv.org/abs/2304.13545","Communication efﬁciency and privacy protection are two critical issues in distributed machine learning. Existing methods tackle these two issues separately and may have a high implementation complexity that constrains their application in a resource-limited environment. We propose a comprehensive quantization-based solution that could simultaneously achieve communication efﬁciency and privacy protection, providing new insights into the correlated nature of communication and privacy. Speciﬁcally, we demonstrate the effectiveness of our proposed solutions in the distributed stochastic gradient descent (SGD) framework by adding binomial noise to the uniformly quantized gradients to reach the desired differential privacy level but with a minor sacriﬁce in communication efﬁciency. We theoretically capture the new trade-offs between communication, privacy, and learning performance.","2023-04-26","2023-05-22 18:53:39","2024-02-09 18:20:09","2023-05-22 18:53:39","","","","","","","Killing Two Birds with One Stone","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2304.13545 [cs]","","C:\Users\isido\Zotero\storage\AXSWITAR\Yan e.a. - 2023 - Killing Two Birds with One Stone Quantization Ach.pdf","","optimization; machine learning; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","arXiv:2304.13545","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CT7WBFB2","preprint","2018","Lin, Hongzhou; Mairal, Julien; Harchaoui, Zaid","Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice","","","","","http://arxiv.org/abs/1712.05654","We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems.","2018-06-19","2023-05-22 10:43:39","2024-02-08 15:51:52","2023-05-22 10:43:39","","","","","","","Catalyst Acceleration for First-order Convex Optimization","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1712.05654 [math, stat]","Comment: link to publisher website: http://jmlr.org/papers/volume18/17-748/17-748.pdf","C:\Users\isido\Zotero\storage\PPKCAMMC\1712.05654.pdf","","optimization; machine learning","","","","","","","","","","","","","","","","","","","","arXiv:1712.05654","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NE73XC8U","journalArticle","2002","Stampfer, Erwin; Stadlober, Ernst","METHODS FOR ESTIMATING PRINCIPAL POINTS","Communications in Statistics - Simulation and Computation","","0361-0918, 1532-4141","10.1081/SAC-120003338","http://www.tandfonline.com/doi/abs/10.1081/SAC-120003338","Principal points of a distribution have been introduced by Flury (1) who tackled the problem of optimal grouping in multivariate data. In essence, principal points are the theoretical counterparts of cluster means obtained by a $k$-means clustering algorithm. There has been considerable effort to find efficient estimation procedures for principal points. It is well known that under certain conditions the $k$-means estimator is a consistent and asymptotically normal estimator of the population principal points. In this paper some material on principal points is reviewed and new algorithms for the estimation of principal points in univariate distributions (univariate principal points) are proposed. Additionally, the Bootstrap approach is applied to assess the variability of the suggested estimators.","2002-05-23","2023-05-17 16:28:25","2023-08-11 18:35:53","2023-05-17 16:28:25","261-277","","2","31","","Communications in Statistics - Simulation and Computation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\7XXCGFY2\Stampfer en Stadlober - 2002 - METHODS FOR ESTIMATING PRINCIPAL POINTS.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PY59DVS","preprint","2018","Jankowiak, Martin; Obermeyer, Fritz","Pathwise Derivatives Beyond the Reparameterization Trick","","","","","http://arxiv.org/abs/1806.01851","We observe that gradients computed via the reparameterization trick are in direct correspondence with solutions of the transport equation in the formalism of optimal transport. We use this perspective to compute (approximate) pathwise gradients for probability distributions not directly amenable to the reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that when the reparameterization trick is applied to the Choleskyfactorized multivariate Normal distribution, the resulting gradients are suboptimal in the sense of optimal transport. We derive the optimal gradients and show that they have reduced variance in a Gaussian Process regression task. We demonstrate with a variety of synthetic experiments and stochastic variational inference tasks that our pathwise gradients are competitive with other methods.","2018-07-05","2023-05-11 09:15:59","2024-02-08 15:51:25","2023-05-11 09:15:58","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1806.01851 [cs, stat]","Comment: ICML 2018","C:\Users\isido\Zotero\storage\E9PELC7L\Jankowiak en Obermeyer - 2018 - Pathwise Derivatives Beyond the Reparameterization.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:1806.01851","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3KNYP4T","journalArticle","2021","Xie, Yantong; Zhou, Zhennan","Frozen Gaussian Sampling: A Mesh-free Monte Carlo Method For Approximating Semiclassical Schr¨odinger Equations","","","","","","In this paper, we develop a Monte Carlo algorithm named the Frozen Gaussian Sampling (FGS) to solve the semiclassical Schr¨odinger equation based on the frozen Gaussian approximation. Due to the highly oscillatory structure of the wave function, traditional mesh-based algorithms suﬀer from ”the curse of dimensionality”, which gives rise to more severe computational burden when the semiclassical parameter ε is small. The Frozen Gaussian sampling outperforms the existing algorithms in that it is mesh-free in computing the physical observables and is suitable for high dimensional problems. In this work, we provide detailed procedures to implement the FGS for both Gaussian and WKB initial data cases, where the sampling strategies on the phase space balance the need of variance reduction and sampling convenience. Moreover, we rigorously prove that, to reach a certain accuracy, the number of samples needed for the FGS is independent of the scaling parameter ε. Furthermore, the complexity of the FGS algorithm is of a sublinear scaling with respect to the microscopic degrees of freedom and, in particular, is insensitive to the dimension number. The performance of the FGS is validated through several typical numerical experiments, including simulating scattering by the barrier potential, formation of the caustics and computing the high-dimensional physical observables without mesh.","2021-12-13","2023-05-10 16:37:26","2024-08-12 14:41:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\DR2NE79L\Xie en Zhou - Frozen Gaussian Sampling A Mesh-free Monte Carlo .pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UDCHQVJK","book","2003","Øksendal, Bernt","Stochastic Differential Equations","","978-3-540-04758-2 978-3-642-14394-6","","","http://link.springer.com/10.1007/978-3-642-14394-6","These notes are based on a postgraduate course I gave on stochastic differential equations at Edinburgh University in the spring 1982. No previous knowledge about the subject was assumed, but the presentation is based on some background in measure theory. There are several reasons why one should learn more about stochastic differential equations: They have a wide range of applications outside mathematics, there are many fruitful connections to other mathematical disciplines and the subject has a rapidly developing life of its own as a fascinating research field with many interesting unanswered questions. Unfortunately most of the literature about stochastic differential equations seems to place so much emphasis on rigor and completenessthatitscares many nonexperts away. These notes are an attempt to approach the subject from the nonexpert point of view: Not knowing anything (except rumours, maybe) about a subject to start with, what would I like to know first of all? My answer would be: 1) In what situations does the subject arise? 2) What are its essential features? 3) What are the applications and the connections to other fields? I would not be so interested in the proof of the most general case, but rather in an easier proof of a special case, which may give just as much of the basic idea in the argument. And I would be willing to believe some basic results without proof (at first stage, anyway) in order to have time for some more basic applications. These notes reflect this point of view. Such an approach enables us to reach the highlights of the theory quicker and easier. Thus it is hoped that these notes may contribute to fill a gap in the existing literature. The course is meant to be an appetizer. If it succeeds in awaking further interest, the reader will have a large selection of excellent literature available for the study of the whole story. Some of this literature is listed at the back. xxvi xxviii In the introduction we state 6 problems where stochastic differential equations play an essential role in the solution. In Chapter II we introduce the basic mathematical notions needed for the mathematical model of some of these problems, leading to the concept of Ito integrals in Chapter III. In Chapter IV we develop the stochastic calculus (the Ito formula) and in Chapter V we use this to solve some stochastic differential equations, including the first two problems in the introduction. In Chapter VI we present a solution of the linear filtering problem (of which problem 3 is an example), using the stochastic calculus. Problem 4 is the Dirichlet problem. Although this is purely deterministic we outline in Chapters VII and VIII how the introduction of an associated Ito diffusion (i.e. solution of a stochastic differential equation) leads to a simple, intuitive and useful stochastic solution, which is the cornerstone of stochastic potential theory. Problem 5 is an optimal stopping problem. In Chapter IX we represent the state of a game at time t by an Ito diffusion and solve the corresponding optimal stopping problem. The solution involves potential theoretic notions, such as the generalized harmonic extension provided by the solution of the Dirichlet problem in Chapter VIII. Problem 6 is a stochastic version of F.P. Ramsey’s classical control problem from 1928. In Chapter X we formulate the general stochastic control problem in terms of stochastic differential equations, and we apply the results of Chapters VII and VIII to show that the problem can be reduced to solving the (deterministic) Hamilton-Jacobi-Bellman equation. As an illustration we solve a problem about optimal portfolio selection. After the course was first given in Edinburgh in 1982, revised and expanded versions were presented at Agder College, Kristiansand and University of Oslo. Every time about half of the audience have come from the applied section, the others being so-called “pure” mathematicians. This fruitful combination has created a broad variety of valuable comments, for which I am very grateful. I particularly wish to express my gratitude to K.K. Aase, L. Csink and A.M. Davie for many useful discussions. I wish to thank the Science and Engineering Research Council, U.K. and Norges Almenvitenskapelige Forskningsra  ̊d (NAVF), Norway for their financial support. And I am greatly indebted to Ingrid Skram, Agder College and Inger Prestbakken, University of Oslo for their excellent typing – and their patience with the innumerable changes in the manuscript during these two years.","2003","2023-05-08 11:23:45","2024-02-07 20:21:21","2023-05-08 11:23:45","","","","","","","","Universitext","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-14394-6","","C:\Users\isido\Zotero\storage\GCS3EXST\Øksendal - 2003 - Stochastic Differential Equations.pdf","","SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RHMY7FI","preprint","2023","Løvbak, Emil; Blondeel, Frédéric; Lee, Adam; Vanroye, Lander; Van Barel, Andreas; Samaey, Giovanni","Reversible random number generation for adjoint Monte Carlo simulation of the heat equation","","","","","http://arxiv.org/abs/2302.02778","In PDE-constrained optimization, one aims to ﬁnd design parameters that minimize some objective, subject to the satisfaction of a partial diﬀerential equation. A major challenges is computing gradients of the objective to the design parameters, as applying the chain rule requires computing the Jacobian of the design parameters to the PDE’s state. The adjoint method avoids this Jacobian by computing partial derivatives of a Lagrangian. Evaluating these derivatives requires the solution of a second PDE with the adjoint diﬀerential operator to the constraint, resulting in a backwards-in-time simulation.","2023-02-06","2023-05-08 06:51:05","2024-02-07 20:21:13","2023-05-08 06:51:05","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.02778 [cs, math]","Comment: 17 pages, 5 figures, submitted to the proceedings of MCQMC22","C:\Users\isido\Zotero\storage\I2IIHESY\Løvbak e.a. - 2023 - Reversible random number generation for adjoint Mo.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","arXiv:2302.02778","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TX6AHPUR","forumPost","2017","Fauskanger, Thomas","What is the advantages of Wasserstein metric compared to Kullback-Leibler divergence?","Cross Validated","","","","https://stats.stackexchange.com/q/295617","","2017-08-02","2023-05-07 07:22:55","2023-05-07 07:23:07","2023-05-07 07:22:54","","","","","","","","","","","","","","","","Forum post","","","","","","","C:\Users\isido\Zotero\storage\NW2RJDMV\what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3P8WBQIS","journalArticle","2013","Johnson, Rie; Zhang, Tong","Accelerating Stochastic Gradient Descent using Predictive Variance Reduction","","","","","","Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is signiﬁcantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.","2013-12-05","2023-05-03 10:20:33","2024-08-12 14:30:00","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\NTPY2NIB\Johnson en Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf","","gradient descent; optimization; SGD","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2U9QZ5U9","journalArticle","2009","Jentzen, A.; Neuenkirch, A.","A random Euler scheme for Carathéodory differential equations","Journal of Computational and Applied Mathematics","","03770427","10.1016/j.cam.2008.05.060","https://linkinghub.elsevier.com/retrieve/pii/S0377042708002136","We study a random Euler scheme for the approximation of Carathéodory differential equations and give a precise error analysis. In particular, we show that under weak assumptions, this approximation scheme obtains the same rate of convergence as the classical Monte–Carlo method for integration problems.","2009-02","2023-03-06 12:43:19","2023-05-02 13:32:58","2023-03-06 12:43:19","346-359","","1","224","","Journal of Computational and Applied Mathematics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\LFZQS9Q7\Jentzen en Neuenkirch - 2009 - A random Euler scheme for Carathéodory differentia.pdf","","monte carlo; ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPMCLENJ","preprint","2023","Miller, Bailey; Sawhney, Rohan; Crane, Keenan; Gkioulekas, Ioannis","Boundary Value Caching for Walk on Spheres","","","","","http://arxiv.org/abs/2302.11825","Grid-free Monte Carlo methods such as \emph{walk on spheres} can be used to solve elliptic partial differential equations without mesh generation or global solves. However, such methods independently estimate the solution at every point, and hence do not take advantage of the high spatial regularity of solutions to elliptic problems. We propose a fast caching strategy which first estimates solution values and derivatives at randomly sampled points along the boundary of the domain (or a local region of interest). These cached values then provide cheap, output-sensitive evaluation of the solution (or its gradient) at interior points, via a boundary integral formulation. Unlike classic boundary integral methods, our caching scheme introduces zero statistical bias and does not require a dense global solve. Moreover we can handle imperfect geometry (e.g., with self-intersections) and detailed boundary/source terms without repairing or resampling the boundary representation. Overall, our scheme is similar in spirit to \emph{virtual point light} methods from photorealistic rendering: it suppresses the typical salt-and-pepper noise characteristic of independent Monte Carlo estimates, while still retaining the many advantages of Monte Carlo solvers: progressive evaluation, trivial parallelization, geometric robustness, \etc{}\ We validate our approach using test problems from visual and geometric computing.","2023-02-23","2023-05-02 08:09:18","2024-02-09 14:47:49","2023-05-02 08:09:18","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.11825 [cs]","Comment: Added citation to concurrent submission","C:\Users\isido\Zotero\storage\LX3BMITM\Miller e.a. - 2023 - Boundary Value Caching for Walk on Spheres.pdf","","monte carlo; PDE; walk on spheres; boundary value problems","","","","","","","","","","","","","","","","","","","","arXiv:2302.11825","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3LPGHXU","preprint","2023","Sawhney, Rohan; Miller, Bailey; Gkioulekas, Ioannis; Crane, Keenan","Walk on Stars: A Grid-Free Monte Carlo Method for PDEs with Neumann Boundary Conditions","","","","","http://arxiv.org/abs/2302.11815","Grid-free Monte Carlo methods based on the \emph{walk on spheres (WoS)} algorithm solve fundamental partial differential equations (PDEs) like the Poisson equation without discretizing the problem domain, nor approximating functions in a finite basis. Such methods hence avoid aliasing in the solution, and evade the many challenges of mesh generation. Yet for problems with complex geometry, practical grid-free methods have been largely limited to basic Dirichlet boundary conditions. This paper introduces the \emph{walk on stars (WoSt)} method, which solves linear elliptic PDEs with arbitrary mixed Neumann and Dirichlet boundary conditions. The key insight is that one can efficiently simulate reflecting Brownian motion (which models Neumann conditions) by replacing the balls used by WoS with \emph{star-shaped} domains; we identify such domains by locating the closest visible point on the geometric silhouette. Overall, WoSt retains many attractive features of other grid-free Monte Carlo methods, such as progressive evaluation, trivial parallel implementation, and logarithmic scaling relative to geometric complexity.","2023-02-23","2023-05-01 17:04:56","2024-02-09 14:48:04","2023-05-01 17:04:56","","","","","","","Walk on Stars","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.11815 [cs]","Comment: Added citation to concurrent submission","C:\Users\isido\Zotero\storage\36DKDXFD\2302.11815.pdf","","monte carlo; PDE; walk on spheres; rendering","","","","","","","","","","","","","","","","","","","","arXiv:2302.11815","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGS69Z62","journalArticle","2021","El Filali Ech-Chafiq, Zineb; Lelong, Jérôme; Reghai, Adil","Automatic control variates for option pricing using neural networks","Monte Carlo Methods and Applications","","1569-3961, 0929-9629","10.1515/mcma-2020-2081","https://www.degruyter.com/document/doi/10.1515/mcma-2020-2081/html","Many pricing problems boil down to the computation of a high dimensional integral, which is usually estimated using Monte Carlo. In fact, the accuracy of a Monte Carlo estimator with M simulations is given by √σ . Meaning that its convergence is immune to the dimension of the problem. However, M this convergence can be relatively slow depending on the variance σ of the function to be integrated. To resolve such a problem, one would perform some variance reduction techniques such as importance sampling, stratiﬁcation, or control variates. In this paper, we will study two approaches for improving the convergence of Monte Carlo using Neural Networks. The ﬁrst approach relies on the fact that many high dimensional ﬁnancial problems are of low effective dimensions[15]. We expose a method to reduce the dimension of such problems in order to keep only the necessary variables. The integration can then be done using fast numerical integration techniques such as Gaussian quadrature. The second approach consists in building an automatic control variate using neural networks. We learn the function to be integrated (which incorporates the diffusion model plus the payoff function) in order to build a network that is highly correlated to it. As the network that we use can be integrated exactly, we can use it as a control variate.","2021-06-01","2023-04-10 14:45:27","2023-08-03 19:23:13","2023-04-10 14:45:27","91-104","","2","27","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\XVFWLNCP\El Filali Ech-Chafiq e.a. - 2021 - Automatic control variates for option pricing usin.pdf","","control variates; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SYGAZWE4","preprint","2017","Geeraert, Sébastien; Lehalle, Charles-Albert; Pearlmutter, Barak; Pironneau, Olivier; Reghai, Adil","Mini-symposium on automatic differentiation and its applications in the financial industry","","","","","http://arxiv.org/abs/1703.02311","Automatic diﬀerentiation has been involved for long in applied mathematics as an alternative to ﬁnite diﬀerence to improve the accuracy of numerical computation of derivatives. Each time a numerical minimization is involved, automatic diﬀerentiation can be used. In between formal derivation and standard numerical schemes, this approach is based on software solutions applying mechanically the chain rule formula to obtain an exact value for the desired derivative. It has a cost in memory and cpu consumption.","2017-06-07","2023-04-10 08:05:34","2024-02-07 20:20:28","2023-04-10 08:05:34","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1703.02311 [cs, q-fin]","","C:\Users\isido\Zotero\storage\2CWLCKI6\Geeraert e.a. - 2017 - Mini-symposium on automatic differentiation and it.pdf","","MC forex course","","","","","","","","","","","","","","","","","","","","arXiv:1703.02311","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJ2Z7KIN","preprint","2019","Guo, Gaoyue; Obloj, Jan","Computational Methods for Martingale Optimal Transport problems","","","","","http://arxiv.org/abs/1710.07911","We establish numerical methods for solving the martingale optimal transport problem (MOT) - a version of the classical optimal transport with an additional martingale constraint on transport's dynamics. We prove that the MOT value can be approximated using linear programming (LP) problems which result from a discretisation of the marginal distributions combined with a suitable relaxation of the martingale constraint. Specialising to dimension one, we provide bounds on the convergence rate of the above scheme. We also show a stability result under only partial specification of the marginal distributions. Finally, we specialise to a particular discretisation scheme which preserves the convex ordering and does not require the martingale relaxation. We introduce an entropic regularisation for the corresponding LP problem and detail the corresponding iterative Bregman projection. We also rewrite its dual problem as a minimisation problem without constraint and solve it by computing the concave envelope of scattered data.","2019-04-05","2023-04-10 07:46:39","2024-02-07 20:20:24","2023-04-10 07:46:39","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1710.07911 [math, q-fin]","","C:\Users\isido\Zotero\storage\WT6MVWSJ\Guo en Obloj - 2019 - Computational Methods for Martingale Optimal Trans.pdf","","MC forex course","","","","","","","","","","","","","","","","","","","","arXiv:1710.07911","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BUYS2YEE","journalArticle","2023","Belkotain, M; Reghai, A","Iterative Bregman Projection for Monte Carlo Enhancement","","","","","","","2023-03-11","2023-04-09 13:32:53","2024-08-12 13:23:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\MMDYLN8T\Belkotain en Reghai - Iterative Bregman Projection for Monte Carlo Enhan.pdf","","MC forex course","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZVGV9S4","journalArticle","2022","Belkotain, Mehdi; Reghai, Adil","Iterative Bregman Projection for Monte Carlo Enhancement","","","","","","Given a set of $N$ trajectories, $S_{t_1}, \ldots S_{t_n}$, $$ \left[\begin{array}{lll} S_{T_1}\left(\omega_1\right) & \ldots & S_{T_n}\left(\omega_1\right) \\ S_{T_1}\left(\omega_2\right) & \ldots & S_{T_n}\left(\omega_2\right) \\ \cdot & \ldots & \\ S_{T_1}\left(\omega_N\right) & \ldots & S_{T_n}\left(\omega_N\right) \end{array}\right] $$ sampled at $t_1, \ldots, t_n$, the empirical density is, $\mu=\frac{1}{N} \sum_{i=1}^N \delta_{S_{T_1}, \ldots, S_{T_n}}$. This distribution is used to price and hedge derivative products. This set of scenariosaccumulates many types of discretisation errors. These can be linked to data quality input, to calibration algorithms, discretisation in time, interpolation in space or even dependent on the sample’s size N. All these errors contribute in errors that are detrimental to the quality of the final result. The classical approach to mitigate these errors is to work separately on each step of the sequence before the generation of the trajectories. Most of the time this increases significantly the computation time and notably the size sample in order to improve convergence and the final result. In the context of dynamic markets, where real time management is essential for trading desks, Also, there are important needs for financial engineers to perform back tests and forward tests to design the appropriate financial products. For the risk department and with regard to important set of risk metrics such those requested by the FRTB (Fundamental review of the trading Desk) it becomes essential to obtain good convergence with a minimum computation budget, in this case very little set of trajectories. It is also essential to dispose of a methodology that is agnostic to the designed model which can be different from one bank to the other and even sometimes from one department to another. The contribution of this paper is to introduce a feedback loop on the Monte Carlo paths. We do not revise all steps of the pricing algorithms, we rather concentrate on the resulting set of trajectories and smoothly transport them into a new version of trajectories which is more appropriate to fit the target.The proposed method acts directly on the generated sample and proposes a way to modify them in a smooth way in order to fit the market tradeables and importantly meet the non arbitrage condition i.e.  the preservation of the martingale property. It is a re-sampling approach which ensures that the finite set of scenarios fits perfectly the pricing constraints. We achieve this through an application of the iterative Bregman Projection algorithm.","2022-10","2023-04-09 13:32:55","2024-08-12 13:24:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\WF3R3AZN\Belkotain en Reghai - Iterative Bregman Projection for Monte Carlo Enhan.pdf","","MC forex course","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JP6VW4GB","journalArticle","2012","ZhiMin, Hong; ZaiZai, Yan; JianRui, Chen","Monte Carlo Method for Solving the Fredholm Integral Equations of the Second Kind","Transport Theory and Statistical Physics","","0041-1450, 1532-2424","10.1080/00411450.2012.695317","http://www.tandfonline.com/doi/abs/10.1080/00411450.2012.695317","This article is concerned with a numerical algorithm for solving approximate solutions of Fredholm integral equations of the second kind with random sampling. We use Simpson’s rule for solving integral equations, which yields a linear system. The Monte Carlo method, based on the simulation of a finite discrete Markov chain, is employed to solve this linear system. To show the efficiency of the method, we use numerical examples. Results obtained by the present method indicate that the method is an effective alternate method. Keywords Monte Carlo algorithms; Markov chain; Simpson’s formula; Fredholm integral equation","2012-12","2023-03-26 12:40:33","2023-08-03 19:23:48","2023-03-26 12:40:33","513-528","","7","41","","Transport Theory and Statistical Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\HJ9CJZP4\ZhiMin e.a. - 2012 - Monte Carlo Method for Solving the Fredholm Integr.pdf","","monte carlo; integral equations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PXBWXNGU","book","2012","Hackbusch, Wolfgang","Integral equations: theory and numerical treatment","","978-3-0348-9947-5","","","","Book on Integral equations: theory and numerical treatment","2012","2023-03-26 12:28:24","2023-08-01 13:42:38","","","380","","","","","Integral equations","International series of numerical mathematics","120","","","Birkhäuser","Basel","eng","","","","","K10plus ISBN","","","","C:\Users\isido\Zotero\storage\UBYLDU35\[International Series of Numerical Mathematics №120] Wolfgang Hackbusch (auth.) - Integral Equations_ Theory and Numerical Treatment (1995, Birkhäuser) [10.1007_978-3-0348-9215-5] - libgen.li.pdf","","integral equations","","","","","","","","","","","","","","","","","","","","","1. softcover ed.]; [Reprint of the orig. 1st ed. 1995","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9XFVFQK","book","2008","Poli︠a︡nin, A. D.; Manzhirov, A. V.","Handbook of integral equations","","978-1-58488-507-8","","","","Integral equations are encountered in various fields of science and numerous applications (in elasticity, plasticity, heat and mass transfer, oscillation theory, fluid dynamics, filtration theory, electrostatics, electrodynamics, biomechanics, game theory, control, queuing theory, electrical engineering, economics, medicine, etc.). Exact (closed-form) solutions of integral equations play an important role in the proper understanding of qualitative features of many phenomena and processes in various areas of natural science. Lots of equations of physics, chemistry, and biology contain functions or parameters which are obtained from experiments and hence are not strictly fixed. Therefore, it is expedient to choose the structure of these functions so that it would be easier to analyze and solve the equation. As a possible selection criterion, one may adopt the requirement that the model integral equation admits a solution in a closed form. Exact solutions can be used to verify the consistency and estimate errors of various numerical, asymptotic, and approximate methods. More than 2,100 integral equations and their solutions are given in the first part of the book (Chapters 1–6). A lot of new exact solutions to linear and nonlinear equations are included. Special attention is paid to equations of general form, which depend on arbitrary functions. The other equations contain one or more free parameters (the book actually deals with families of integral xxx xxxii PREFACE equations); it is the reader’s option to fix these parameters. In total, the number of equations described in this handbook is an order of magnitude greater than in any other book currently available. The second part of the book (Chapters 7–14) presents exact, approximate analytical, and numerical methods for solving linear and nonlinear integral equations. Apart from the classical methods, some new methods are also described. When selecting the material, the authors have given a pronounced preference to practical aspects of the matter; that is, to methods that allow effectively “constructing” the solution. For the reader’s better understanding of the methods, each section is supplied with examples of specific equations. Some sections may be used by lecturers of colleges and universities as a basis for courses on integral equations and mathematical physics equations for graduate and postgraduate students. For the convenience of a wide audience with different mathematical backgrounds, the authors tried to do their best, wherever possible, to avoid special terminology. Therefore, some of the methods are outlined in a schematic and somewhat simplified manner, with necessary references made to books where these methods are considered in more detail. For some nonlinear equations, only solutions of the simplest form are given. The book does not cover two-, three-, and multidimensional integral equations. The handbook consists of chapters, sections, and subsections. Equations and formulas are numbered separately in each section. The equations within a section are arranged in increasing order of complexity. The extensive table of contents provides rapid access to the desired equations. For the reader’s convenience, the main material is followed by a number of supplements, where some properties of elementary and special functions are described, tables of indefinite and definite integrals are given, as well as tables of Laplace, Mellin, and other transforms, which are used in the book. The first and second parts of the book, just as many sections, were written so that they could be read independently from each other. This allows the reader to quickly get to the heart of the matter. We would like to express our deep gratitude to Rolf Sulanke and Alexei Zhurov for fruitful discussions and valuable remarks. We also appreciate the help of Vladimir Nazaikinskii and Alexander Shtern in translating the second part of this book, and are thankful to Inna Shingareva for her assistance in preparing the camera-ready copy of the book. The authors hope that the handbook will prove helpful for a wide audience of researchers, college and university teachers, engineers, and students in various fields of mathematics, mechanics, physics, chemistry, biology, economics, and engineering sciences.","2008","2023-03-26 12:07:56","2024-02-07 20:19:44","","","1108","","","","","","Handbooks of mathematical equations","","","","Chapman & Hall/CRC","Boca Raton","en","","","","","Library of Congress ISBN","QA431 .P65 2008","OCLC: ocn167516078","","C:\Users\isido\Zotero\storage\83JNUZ6J\Poli︠a︡nin en Manzhirov - 2008 - Handbook of integral equations.pdf","","integral equations","","","","","","","","","","","","","","","","","","","","","2nd ed","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZ7PU6PM","webpage","2023","","Fredholm Integral Equations colorado","","","","","https://www.colorado.edu/amath/sites/default/files/attached-files/fredholm.pdf","","2023-03-26","2023-03-26 11:18:49","2024-08-12 14:47:45","2023-03-26 11:18:13","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\G9NULA4I\fredholm.pdf","","integral equations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D823325H","preprint","2001","Heinrich, S.; Novak, E.","Optimal Summation and Integration by Deterministic, Randomized, and Quantum Algorithms","","","","","http://arxiv.org/abs/quant-ph/0105114","We survey old and new results about optimal algorithms for summation of finite sequences and for integration of functions from Hoelder or Sobolev spaces. First we discuss optimal deterministic and randomized algorithms. Then we add a new aspect, which has not been covered before on conferences about (quasi-) Monte Carlo methods: quantum computation. We give a short introduction into this setting and present recent results of the authors on optimal quantum algorithms for summation and integration. We discuss comparisons between the three settings. The most interesting case for Monte Carlo and quantum integration is that of moderate smoothness k and large dimension d which, in fact, occurs in a number of important applied problems. In that case the deterministic exponent is negligible, so the n^{-1/2} Monte Carlo and the n^{-1} quantum speedup essentially constitute the entire convergence rate. We observe that -- there is an exponential speed-up of quantum algorithms over deterministic (classical) algorithms, if k/d tends to zero; -- there is a (roughly) quadratic speed-up of quantum algorithms over randomized classical algorithms, if k/d is small.","2001-05-23","2023-03-19 16:36:30","2024-02-07 20:19:39","2023-03-19 16:36:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:quant-ph/0105114","Comment: 13 pages, contribution to the 4th International Conference on Monte Carlo and Quasi-Monte Carlo Methods, Hong Kong 2000","C:\Users\isido\Zotero\storage\LK6RKYIX\Heinrich en Novak - 2001 - Optimal Summation and Integration by Deterministic.pdf","","IBC; quadrature","","","","","","","","","","","","","","","","","","","","arXiv:quant-ph/0105114","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TSKZBGG6","journalArticle","2011","Daun, Thomas","On the randomized solution of initial value problems","Journal of Complexity","","0885064X","10.1016/j.jco.2010.07.002","https://linkinghub.elsevier.com/retrieve/pii/S0885064X1000066X","We study the randomized solution of initial value problems for systems of ordinary differential equations $$ y^{\prime}(x)=f(x, y(x)), x \in[a, b], y(a)=y_0 \in \mathbb{R}^d . $$ Recently S. Heinrich and B. Milla presented an order optimal randomized algorithm solving this problem for $\gamma$-smooth input data (i.e. $\gamma=r+\rho$ : the $r$-th derivatives of $f$ satisfy a $\rho$-Hölder condition). This algorithm uses function values and values of derivatives of $f$. In this paper we present an order optimal randomized algorithm for the class of $\gamma$-smooth functions that uses only values of $f$. For this purpose we show how to obtain an order optimal randomized algorithm from an order (sub)optimal deterministic one.","2011-06","2023-03-06 14:05:17","2023-08-01 13:50:15","2023-03-06 14:05:17","300-311","","3-4","27","","Journal of Complexity","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\NCVQFBK5\Daun - 2011 - On the randomized solution of initial value proble.pdf","","monte carlo; ODE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Q7VSW3M","journalArticle","2014","Goćwin, Maciej","Randomized and quantum complexity of nonlinear two-point BVPs","Applied Mathematics and Computation","","00963003","10.1016/j.amc.2014.07.106","https://linkinghub.elsevier.com/retrieve/pii/S009630031401073X","We deal with the complexity of nonlinear BVPs with nonlinear two-point boundary conditions. We consider the randomized and quantum models of computation. We assume that the right-hand side function is r times differentiable with all derivatives bounded by a constant. We show that the e-complexity is roughly of order eÀ1=ðrþ1=2Þ in the randomized setting, and eÀ1=ðrþ1Þ in the quantum setting. We compare our results with known results in the deterministic setting. The speed-up of the randomized computations with respect to the deterministic computations is by 1=ðrð2r þ 1ÞÞ in the exponent of 1=e, and the speed-up of the quantum computations by 1=ðrðr þ 1ÞÞ in the exponent.","2014-10","2023-03-19 15:36:30","2024-02-07 20:19:36","2023-03-19 15:36:30","357-371","","","245","","Applied Mathematics and Computation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\AVIKYTNP\Goćwin - 2014 - Randomized and quantum complexity of nonlinear two.pdf; C:\Users\isido\Zotero\storage\CRRV662Q\S009630031401073X.html","","ODE; boundary value problems; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K25543D6","videoRecording","2022","Marcin Anforowicz","The ""Just One More"" Paradox","","","","","https://www.youtube.com/watch?v=_FuuYSM7yOo","","2022-09-25","2023-05-29 06:46:11","2024-02-09 19:48:45","2023-05-29 06:46:11","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","9:12","","","","","","","","","","","","","","","","","","","","","","","","",""
"GBRHHG8Z","preprint","2013","Cuturi, Marco","Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances","","","","","http://arxiv.org/abs/1306.0895","Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms’ dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp’s matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.","2013-06-04","2023-05-29 07:51:25","2024-02-08 15:53:16","2023-05-29 07:51:25","","","","","","","Sinkhorn Distances","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1306.0895 [stat]","","C:\Users\isido\Zotero\storage\772CFQJN\Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1306.0895","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QE259D7Q","journalArticle","2020","Hu, Zhicheng; Liu, Zhihui","Heat Conduction Simulation of 2D Moving Heat Source Problems Using a Moving Mesh Method","Advances in Mathematical Physics","","1687-9120, 1687-9139","10.1155/2020/6067854","https://www.hindawi.com/journals/amp/2020/6067854/","This paper focuses on efficiently numerical investigation of two-dimensional heat conduction problems of material subjected to multiple moving Gaussian point heat sources. All heat sources are imposed on the inside of material and assumed to move along some specified straight lines or curves with time-dependent velocities. A simple but efficient moving mesh method, which continuously adjusts the two-dimensional mesh dimension by dimension upon the one-dimensional moving mesh partial differential equation with an appropriate monitor function of the temperature field, has been developed. The physical model problem is then solved on this adaptive moving mesh. Numerical experiments are presented to exhibit the capability of the proposed moving mesh algorithm to efficiently and accurately simulate the moving heat source problems. The transient heat conduction phenomena due to various parameters of the moving heat sources, including the number of heat sources and the types of motion, are well simulated and investigated.","2020-02-11","2023-05-29 11:58:37","2023-08-03 19:19:06","2023-05-29 11:58:37","1-16","","","2020","","Advances in Mathematical Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\G7ZGFDCD\Hu en Liu - 2020 - Heat Conduction Simulation of 2D Moving Heat Sourc.pdf","","PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B97ZGWZI","videoRecording","2022","Mathemaniac","The weirdest paradox in statistics (and machine learning)","","","","","https://www.youtube.com/watch?v=cUqoHQDinCM","Stein's paradox is of fundamental importance in modern statistics, introducing concepts of shrinkage to further reduce the mean squared error, especially in higher dimensional statistics that is particularly relevant nowadays, in the world of machine learning, for example. However, this is usually ignored, because it is mostly seen as a toy problem. Precisely because it is such a simple problem that illustrates the problem of maximum likelihood estimation! This paradox is the subject of many blogposts (linked below), but not really here on YouTube, except in some lecture recordings, so I have to bring this up to YouTube. This is not to say that maximum likelihood estimator is not useful - in most situations, especially in lower dimensional statistics, it is still good, but to hold it to such a high place, as statisticians did before 1961? That is not a healthy attitude to this theory. One thing I did not say, but perhaps a lot of people will want me to, is that this is an emprical Bayes estimator, but again, more links below. Video chapters: 00:00 Introduction 04:38 Chapter 1: The ""best"" estimator 09:48 Chapter 2: Why shrinkage works 15:51 Chapter 3: Bias-variance tradeoff 18:45 Chapter 4: Applications","2022-08-30","2023-06-05 09:26:37","2024-02-09 19:48:42","2023-06-05 09:26:37","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","21:43","","","","","","","","","","","","","","","","","","","","","","","","",""
"6H6BV477","preprint","2023","Sugimoto, Ryusuke; Chen, Terry; Jiang, Yiti; Batty, Christopher; Hachisuka, Toshiya","A Practical Walk-on-Boundary Method for Boundary Value Problems","","","","10.1145/3592109","http://arxiv.org/abs/2305.04403","We introduce the walk-on-boundary (WoB) method for solving boundary value problems to computer graphics. WoB is a grid-free Monte Carlo solver for certain classes of second order partial differential equations. A similar Monte Carlo solver, the walk-on-spheres (WoS) method, has been recently popularized in computer graphics due to its advantages over traditional spatial discretization-based alternatives. We show that WoB’s intrinsic properties yield further advantages beyond those of WoS. Unlike WoS, WoB naturally supports various boundary conditions (Dirichlet, Neumann, Robin, and mixed) for both interior and exterior domains. WoB builds upon boundary integral formulations, and it is mathematically more similar to light transport simulation in rendering than the random walk formulation of WoS. This similarity between WoB and rendering allows us to implement WoB on top of Monte Carlo ray tracing, and to incorporate advanced rendering techniques (e.g., bidirectional estimators with multiple importance sampling, the virtual point lights method, and Markov chain Monte Carlo) into WoB. WoB does not suffer from the intrinsic bias of WoS near the boundary and can estimate solutions precisely on the boundary. Our numerical results highlight the advantages of WoB over WoS as an attractive alternative to solve boundary value problems based on Monte Carlo. CCS Concepts: • Mathematics of computing → Integral equations; Partial differential equations; • Computing methodologies → Ray tracing.","2023-05-19","2023-06-07 12:04:51","2024-02-09 14:47:30","2023-06-07 12:04:51","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv:2305.04403 [cs, math]","Comment: Accepted to ACM SIGGRAPH North America 2023 / Transactions on Graphics. See https://rsugimoto.net/WoBforBVPsProject/ for updates, including the reference implementation","C:\Users\isido\Zotero\storage\7AGTX5CS\Sugimoto e.a. - 2023 - A Practical Walk-on-Boundary Method for Boundary V.pdf","","monte carlo; PDE; walk on spheres; boundary value problems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PHG2FICZ","journalArticle","2021","Mossberg, Linus","GPU-Accelerated Monte Carlo Geometry Processing for Gradient-Domain Methods","","","","","","This thesis extends the utility of the Monte Carlo approach to PDE-based methods presented in the paper Monte Carlo Geometry Processing. In particular, we implement this method on the GPU using CUDA, and investigate more viable methods of estimating the source integral when solving Poisson’s equation with intricate source terms. This is the case for a large group of gradient-domain methods in computer graphics, where source terms are represented by discrete volumetric data on regular grids. We develop unbiased source integral estimators like image-based importance sampling (IBIS) and biased estimators like source integral caching (SIC), and evaluate these against existing GPU-accelerated finite difference solvers for gradient-domain applications. By decoupling the source integration step from the WoS-algorithm, we find that the SIC method can improve performance by several orders of magnitude, making it competitive with existing finite difference solvers in many cases. We further investigate the viability of distance fields for accelerated distance queries, and find that these can provide significant performance improvements compared to BVHs without meaningfully affecting bias.","2021-12-07","2023-06-08 07:22:07","2024-08-12 14:34:10","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\M3CIZFYI\Mossberg - GPU-Accelerated Monte Carlo Geometry Processing fo.pdf","","boundary value problems; monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATUUSCKM","journalArticle","2017","Tibshirani, Ryan","Nonparametric Regression (and Classiﬁcation)","","","","","","","2017","2023-06-09 12:59:43","2024-08-12 14:38:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\GSH6DL6E\Tibshirani - Nonparametric Regression (and Classiﬁcation).pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GNQ2D6T5","videoRecording","2015","Ryan T","Lecture 11: Nonparametric Bayes","","","","","https://www.youtube.com/watch?v=l0HQXtqnBZY","Lecture Date: 02/18/15","2015-02-26","2023-06-09 13:00:23","2024-02-09 19:48:38","2023-06-09 13:00:23","","","","","","","Lecture 11","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","1:18:18","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KSSJRL8","journalArticle","2017","Tibshirani, Ryan","Sparsity, the Lasso, and Friends","","","","","","","2017","2023-06-09 13:22:06","2024-08-12 14:38:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\FMEX9G6K\Tibshirani - Sparsity, the Lasso, and Friends.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMC2MPGC","bookSection","2012","Bottou, Léon","Stochastic Gradient Descent Tricks","Neural Networks: Tricks of the Trade","978-3-642-35288-1 978-3-642-35289-8","","","http://link.springer.com/10.1007/978-3-642-35289-8_25","Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.","2012","2023-06-09 14:11:09","2023-08-03 19:18:02","2023-06-09 14:11:09","421-436","","","7700","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35289-8_25","","C:\Users\isido\Zotero\storage\38JMSAZM\Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf","","optimization; gradient descent","","Montavon, Grégoire; Orr, Geneviève B.; Müller, Klaus-Robert","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZW7642E","journalArticle","2017","Tibshirani, Ryan","Numerical Linear Algebra Primer","","","","","","","2017","2023-06-09 14:20:48","2024-08-12 14:38:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\VZ7YFNWJ\Tibshirani - Numerical Linear Algebra Primer.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KPG7863A","journalArticle","2009","Blanes, S.; Casas, F.; Oteo, J. A.; Ros, J.","The Magnus expansion and some of its applications","Physics Reports","","03701573","10.1016/j.physrep.2008.11.001","http://arxiv.org/abs/0810.5488","Approximate resolution of linear systems of diﬀerential equations with varying coeﬃcients is a recurrent problem shared by a number of scientiﬁc and engineering areas, ranging from Quantum Mechanics to Control Theory. When formulated in operator or matrix form, the Magnus expansion furnishes an elegant setting to built up approximate exponential representations of the solution of the system. It provides a power series expansion for the corresponding exponent and is sometimes referred to as Time-Dependent Exponential Perturbation Theory. Every Magnus approximant corresponds in Perturbation Theory to a partial re-summation of inﬁnite terms with the important additional property of preserving at any order certain symmetries of the exact solution.","2009-01","2023-06-12 11:19:37","2024-02-08 15:54:34","2023-06-12 11:19:37","151-238","","5-6","470","","Physics Reports","","","","","","","","en","","","","","arXiv.org","","arXiv:0810.5488 [math-ph]","Comment: Report on the Magnus expansion for differential equations and its applications to several physical problems","C:\Users\isido\Zotero\storage\352XINMQ\0810.5488.pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"28TH8SZ5","book","1988","Novak, Erich","Deterministic and stochastic error bounds in numerical analysis","","978-3-540-50368-2 978-0-387-50368-4","","","","In these notes we want to investigate different deterministic and stochastic error bounds of numerical analysis. For many computational problems (such as approximation, optimization, and quadrature) we have only partial information and consequently such problems can only be solved with uncertainty in the answer. The information-centered approach asks for optimal methods and optimal error bounds if only the type of information available is indicated. We begin with worst case error bounds for deterministic methods and consider relations between these error bounds and the n-widths of the class of problem elements (1.2). In 1.3 we give worst case error bounds for some special problems. We are mainly interested in the problems of approximation (App), optimization (Opt and Opt*), and quadrature or integration (Int). We consider different function classes, for both adaptive and nonadaptive methods. First of all, I explain the information-based approach by means of an example.","1988","2023-06-13 10:47:04","2023-08-03 19:17:43","","","113","","","","","","Lecture notes in mathematics","1349","","","Springer","Berlin Heidelberg","en","","","","","K10plus ISBN","","","","C:\Users\isido\Zotero\storage\CT75RT9T\Novak - 1988 - Deterministic and stochastic error bounds in numer.pdf; C:\Users\isido\Zotero\storage\3YULLQV7\Novak - Deterministic and Stochastic Error Bounds In Numer.pdf","","IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3MAD9IY","encyclopediaArticle","2023","","No free lunch theorem","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=No_free_lunch_theorem&oldid=1154448734","In mathematical folklore, the ""no free lunch"" (NFL) theorem (sometimes pluralized) of David Wolpert and William Macready appears in the 1997 ""No Free Lunch Theorems for Optimization"". Wolpert had previously derived no free lunch theorems for machine learning (statistical inference). The name alludes to the saying ""there ain't no such thing as a free lunch"", that is, there are no easy shortcuts to success. In 2005, Wolpert and Macready themselves indicated that the first theorem in their paper ""state[s] that any two optimization algorithms are equivalent when their performance is averaged across all possible problems"".The ""no free lunch"" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them. Various investigators have extended the work of Wolpert and Macready substantively. In terms of how the NFL theorem is used in the context of the research area, the no free lunch in search and optimization is a field that is dedicated for purposes of mathematically analyzing data for statistical identity, particularly search and optimization.While some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.","2023-05-12","2023-06-13 11:01:16","2023-08-03 19:17:37","2023-06-13 11:01:16","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1154448734","","C:\Users\isido\Zotero\storage\S7SXIJCH\No_free_lunch_theorem.html","","optimization; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7G2K4SLJ","encyclopediaArticle","2022","","Information-based complexity","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Information-based_complexity&oldid=1116392336","Information-based complexity (IBC) studies optimal algorithms and computational complexity for the continuous problems that arise in physical science, economics, engineering, and mathematical finance. IBC has studied such continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration. All these problems involve functions (typically multivariate) of a real or complex variable. Since one can never obtain a closed-form solution to the problems of interest one has to settle for a numerical solution. Since a function of a real or complex variable cannot be entered into a digital computer, the solution of continuous problems involves partial information. To give a simple illustration, in the numerical approximation of an integral, only samples of the integrand at a finite number of points are available. In the numerical solution of partial differential equations the functions specifying the boundary conditions and the coefficients of the differential operator can only be sampled. Furthermore, this partial information can be expensive to obtain. Finally the information is often contaminated by noise. The goal of information-based complexity is to create a theory of computational complexity and optimal algorithms for problems with partial, contaminated and priced information, and to apply the results to answering questions in various disciplines. Examples of such disciplines include physics, economics, mathematical finance, computer vision, control theory, geophysics, medical imaging, weather forecasting and climate prediction, and statistics. The theory is developed over abstract spaces, typically Hilbert or Banach spaces, while the applications are usually for multivariate problems. Since the information is partial and contaminated, only approximate solutions can be obtained. IBC studies computational complexity and optimal algorithms for approximate solutions in various settings. Since the worst case setting often leads to negative results such as unsolvability and intractability, settings with weaker assurances such as average, probabilistic and randomized are also studied. A fairly new area of IBC research is continuous quantum computing.","2022-10-16","2023-06-14 07:37:41","2023-08-03 19:17:20","2023-06-14 07:37:41","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1116392336","","C:\Users\isido\Zotero\storage\K3D6TU8D\Information-based_complexity.html","","IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8AP6T8IT","preprint","2018","Robbe, Pieterjan; Nuyens, Dirk; Vandewalle, Stefan","Recycling Samples in the Multigrid Multilevel (Quasi-)Monte Carlo Method","","","","","http://arxiv.org/abs/1806.05619","The Multilevel Monte Carlo method is an eﬃcient variance reduction technique. It uses a sequence of coarse approximations to reduce the computational cost in uncertainty quantiﬁcation applications. The method is nowadays often considered to be the method of choice for solving PDEs with random coeﬃcients when many uncertainties are involved. When using Full Multigrid to solve the deterministic problem, coarse solutions obtained by the solver can be recycled as samples in the Multilevel Monte Carlo method, as was pointed out by Kumar, Oosterlee and Dwight [Int. J. Uncertain. Quantif., 7 (2017), pp. 57–81]. In this article, an alternative approach is considered, using Quasi-Monte Carlo points, to speed up convergence. Additionally, our method comes with an improved variance estimate which is also valid in case of the Monte Carlo based approach. The new method is illustrated on the example of an elliptic PDE with lognormal diﬀusion coeﬃcient. Numerical results for a variety of random ﬁelds with diﬀerent smoothness parameters in the Matérn covariance function show that sample recycling is more eﬃcient when the input random ﬁeld is nonsmooth.","2018-06-14","2023-06-14 08:26:10","2024-02-08 15:54:46","2023-06-14 08:26:10","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1806.05619 [math]","","C:\Users\isido\Zotero\storage\PWK6DY7M\Robbe e.a. - 2018 - Recycling Samples in the Multigrid Multilevel (Qua.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","arXiv:1806.05619","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3435ZYKY","preprint","2017","Kuo, Frances Y.; Nuyens, Dirk","Application of quasi-Monte Carlo methods to PDEs with random coefficients -- an overview and tutorial","","","","","http://arxiv.org/abs/1710.10984","This article provides a high-level overview of some recent works on the application of quasi-Monte Carlo (QMC) methods to PDEs with random coeﬃcients. It is based on an indepth survey of a similar title by the same authors, with an accompanying software package which is also brieﬂy discussed here. Embedded in this article is a step-by-step tutorial of the required analysis for the setting known as the uniform case with ﬁrst order QMC rules. The aim of this article is to provide an easy entry point for QMC experts wanting to start research in this direction and for PDE analysts and practitioners wanting to tap into contemporary QMC theory and methods.","2017-10-26","2023-06-14 08:34:06","2024-02-08 15:55:29","2023-06-14 08:34:06","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1710.10984 [math]","Comment: arXiv admin note: text overlap with arXiv:1606.06613","C:\Users\isido\Zotero\storage\CN5A6NQ6\Kuo en Nuyens - 2017 - Application of quasi-Monte Carlo methods to PDEs w.pdf","","monte carlo; PDE; random PDE","","","","","","","","","","","","","","","","","","","","arXiv:1710.10984","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUJDV556","preprint","2001","Heinrich, Stefan","From Monte Carlo to Quantum Computation","","","","","http://arxiv.org/abs/quant-ph/0112152","Quantum computing was so far mainly concerned with discrete problems. Recently, E. Novak and the author studied quantum algorithms for high dimensional integration and dealt with the question, which advantages quantum computing can bring over classical deterministic or randomized methods for this type of problem.","2001-12-23","2023-06-15 10:27:55","2024-02-08 15:55:27","2023-06-15 10:27:55","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:quant-ph/0112152","Comment: Paper submitted to the Proceedings of the 3rd IMACS Seminar on Monte Carlo Methods MCM2001, Salzburg. 15 pages","C:\Users\isido\Zotero\storage\2BSDMTVF\Heinrich - 2001 - From Monte Carlo to Quantum Computation.pdf","","monte carlo; quantum computing","","","","","","","","","","","","","","","","","","","","arXiv:quant-ph/0112152","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3UTMNWQF","preprint","2015","Lan, Guanghui; Zhou, Yi","An optimal randomized incremental gradient method","","","","","http://arxiv.org/abs/1507.02000","In this paper, we consider a class of finite-sum convex optimization problems whose objective function is given by the summation of $m$ ($\ge 1$) smooth components together with some other relatively simple terms. We first introduce a deterministic primal-dual gradient (PDG) method that can achieve the optimal black-box iteration complexity for solving these composite optimization problems using a primal-dual termination criterion. Our major contribution is to develop a randomized primal-dual gradient (RPDG) method, which needs to compute the gradient of only one randomly selected smooth component at each iteration, but can possibly achieve better complexity than PDG in terms of the total number of gradient evaluations. More specifically, we show that the total number of gradient evaluations performed by RPDG can be ${\cal O} (\sqrt{m})$ times smaller, both in expectation and with high probability, than those performed by deterministic optimal first-order methods under favorable situations. We also show that the complexity of the RPDG method is not improvable by developing a new lower complexity bound for a general class of randomized methods for solving large-scale finite-sum convex optimization problems. Moreover, through the development of PDG and RPDG, we introduce a novel game-theoretic interpretation for these optimal methods for convex optimization.","2015-10-18","2023-06-16 12:18:37","2024-02-08 15:55:25","2023-06-16 12:18:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1507.02000 [cs, math, stat]","","C:\Users\isido\Zotero\storage\YKXRE23K\Lan en Zhou - 2015 - An optimal randomized incremental gradient method.pdf","","optimization; gradient descent","","","","","","","","","","","","","","","","","","","","arXiv:1507.02000","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSC4MMNZ","journalArticle","2022","Driggs, Derek; Ehrhardt, Matthias J.; Schönlieb, Carola-Bibiane","Accelerating variance-reduced stochastic gradient methods","Mathematical Programming","","0025-5610, 1436-4646","10.1007/s10107-020-01566-2","https://link.springer.com/10.1007/s10107-020-01566-2","Variance reduction is a crucial tool for improving the slow convergence of stochastic gradient descent. Only a few variance-reduced methods, however, have yet been shown to directly beneﬁt from Nesterov’s acceleration techniques to match the convergence rates of accelerated gradient methods. Such approaches rely on “negative momentum”, a technique for further variance reduction that is generally speciﬁc to the SVRG gradient estimator. In this work, we show for the ﬁrst time that negative momentum is unnecessary for acceleration and develop a universal acceleration framework that allows all popular variance-reduced methods to achieve accelerated convergence rates. The constants appearing in these rates, including their dependence on the number of functions n, scale with the mean-squared-error and bias of the gradient estimator. In a series of numerical experiments, we demonstrate that versions of SAGA, SVRG, SARAH, and SARGE using our framework signiﬁcantly outperform non-accelerated versions and compare favourably with algorithms using negative momentum.","2022-02","2023-06-16 12:28:36","2024-02-09 18:20:01","2023-06-16 12:28:36","671-715","","2","191","","Math. Program.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\HBSFVERS\Driggs e.a. - 2022 - Accelerating variance-reduced stochastic gradient .pdf","","optimization; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78DJTG8P","journalArticle","2021","Kettunen, Markus; d'Eon, Eugene; Pantaleoni, Jacopo; Novak, Jan","An unbiased ray-marching transmittance estimator","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3450626.3459937","http://arxiv.org/abs/2102.10294","We present an in-depth analysis of the sources of variance in state-of-the-art unbiased volumetric transmittance estimators, and propose several new methods for improving their efficiency. These combine to produce a single estimator that is universally optimal relative to prior work, with up to several orders of magnitude lower variance at the same cost, and has zero variance for any ray with non-varying extinction. We first reduce the variance of truncated power-series estimators using a novel efficient application of U-statistics. We then greatly reduce the average expansion order of the power series and redistribute density evaluations to filter the optical depth estimates with an equidistant sampling comb. Combined with the use of an online control variate built from a sampled mean density estimate, the resulting estimator effectively performs ray marching most of the time while using rarely-sampled higher order terms to correct the bias.","2021-08-31","2023-06-18 08:56:46","2024-02-08 15:55:21","2023-06-18 08:56:46","1-20","","4","40","","ACM Trans. Graph.","","","","","","","","en","","","","","arXiv.org","","arXiv:2102.10294 [cs]","Comment: 20 pages","C:\Users\isido\Zotero\storage\6276WE78\Kettunen e.a. - 2021 - An unbiased ray-marching transmittance estimator.pdf","","rendering; exponential integrators","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDME6GF7","journalArticle","2022","Nimier-David, Merlin; Müller, Thomas; Keller, Alexander; Jakob, Wenzel","Unbiased inverse volume rendering with differential trackers","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3528223.3530073","https://dl.acm.org/doi/10.1145/3528223.3530073","Volumetric representations are popular in inverse rendering because they have a simple parameterization, are smoothly varying, and transparently handle topology changes. However, incorporating the full volumetric transport of light is costly and challenging, often leading practitioners to implement simplified models, such as purely emissive and absorbing volumes with ""baked"" lighting. One such challenge is the efficient estimation of the gradients of the volume's appearance with respect to its scattering and absorption parameters. We show that the straightforward approach---differentiating a volumetric free-flight sampler---can lead to biased and high-variance gradients, hindering optimization. Instead, we propose using a new sampling strategy:               differential ratio tracking               , which is unbiased, yields low-variance gradients, and runs in linear time. Differential ratio tracking combines ratio tracking and reservoir sampling to estimate gradients by sampling distances proportional to the unweighted transmittance rather than the usual extinction-weighted transmittance. In addition, we observe local minima when optimizing scattering parameters to reproduce dense volumes or surfaces. We show that these local minima can be overcome by bootstrapping the optimization from nonphysical emissive volumes that are easily optimized.","2022-07","2023-06-18 11:02:46","2023-08-03 19:15:21","2023-06-18 11:02:46","1-20","","4","41","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\4PYPVM7M\Nimier-David e.a. - 2022 - Unbiased inverse volume rendering with differentia.pdf","","rendering; inverse problem","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P66IEAQN","preprint","2017","Kingma, Diederik P.; Ba, Jimmy","Adam: A Method for Stochastic Optimization","","","","","http://arxiv.org/abs/1412.6980","We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.","2017-01-29","2023-06-18 11:09:30","2024-02-09 18:19:58","2023-06-18 11:09:30","","","","","","","Adam","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1412.6980 [cs]","Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015","C:\Users\isido\Zotero\storage\PZYRRCM2\Kingma en Ba - 2017 - Adam A Method for Stochastic Optimization.pdf","","optimization; machine learning; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","arXiv:1412.6980","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D86IEHBA","preprint","2021","Buvoli, Tommaso; Minion, Michael L.","IMEX Runge-Kutta Parareal for Non-Diffusive Equations","","","","","http://arxiv.org/abs/2011.01604","Parareal is a widely studied parallel-in-time method that can achieve meaningful speedup on certain problems. However, it is well known that the method typically performs poorly on non-diﬀusive equations. This paper analyzes linear stability and convergence for IMEX Runge-Kutta Parareal methods on non-diﬀusive equations. By combining standard linear stability analysis with a simple convergence analysis, we ﬁnd that certain Parareal conﬁgurations can achieve parallel speedup on non-diﬀusive equations. These stable conﬁgurations all posses low iteration counts, large block sizes, and a large number of processors. Numerical examples using the nonlinear Schro¨dinger equation demonstrate the analytical conclusions.","2021-07-31","2023-06-25 11:38:40","2024-02-08 15:55:53","2023-06-25 11:38:40","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2011.01604 [cs, math]","","C:\Users\isido\Zotero\storage\JJGX8M7T\Buvoli en Minion - 2021 - IMEX Runge-Kutta Parareal for Non-Diffusive Equati.pdf","","ODE; parareal","","","","","","","","","","","","","","","","","","","","arXiv:2011.01604","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SBHYGDMQ","preprint","2021","Buvoli, Tommaso; Minion, Michael L.","On the Stability of Exponential Integrators for Non-Diffusive Equations","","","","","http://arxiv.org/abs/2108.00185","Exponential integrators are a well-known class of time integration methods that have been the subject of many studies and developments in the past two decades. Surprisingly, there have been limited eﬀorts to analyze their stability and eﬃciency on non-diﬀusive equations to date. In this paper we apply linear stability analysis to showcase the poor stability properties of exponential integrators on non-diﬀusive problems. We then propose a simple repartitioning approach that stabilizes the integrators and enables the eﬃcient solution of stiﬀ, non-diﬀusive equations. To validate the eﬀectiveness of our approach, we perform several numerical experiments that compare partitioned exponential integrators to unmodiﬁed ones. We also compare repartitioning to the well-known approach of adding hyperviscosity to the equation right-hand-side. Overall, we ﬁnd that the repartitioning restores convergence at large timesteps and, unlike hyperviscosity, it does not require the use of high-order spatial derivatives.","2021-07-31","2023-06-25 11:39:56","2024-02-08 16:03:59","2023-06-25 11:39:56","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2108.00185 [cs, math]","","C:\Users\isido\Zotero\storage\ZTRRZAV8\Buvoli en Minion - 2021 - On the Stability of Exponential Integrators for No.pdf","","ODE; exponential integrators","","","","","","","","","","","","","","","","","","","","arXiv:2108.00185","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PL5J5TAN","preprint","2023","Buvoli, Tommaso; Minion, Michael L.","Exponential Runge-Kutta Parareal for Non-Diffusive Equations","","","","","http://arxiv.org/abs/2301.03764","Parareal is a well-known parallel-in-time algorithm that combines a coarse and ﬁne propagator within a parallel iteration. It allows for large-scale parallelism that leads to signiﬁcantly reduced computational time compared to serial time-stepping methods. However, like many parallel-intime methods it can fail to achieve parallel speedup when applied to non-diﬀusive equations such as hyperbolic systems or dispersive nonlinear wave equations. This paper explores the use of exponential integrators within the Parareal iteration. Exponential integrators are particularly interesting candidates for Parareal because of their ability to resolve fast-moving waves, even at the large stepsizes used by coarse propagators. This work begins with an introduction to exponential Parareal integrators followed by several motivating numerical experiments involving the nonlinear Schrödinger equation. These experiments are then analyzed using linear analysis that approximates the stability and convergence properties of the exponential Parareal iteration on nonlinear problems. The paper concludes with two additional numerical experiments involving the dispersive Kadomtsev-Petviashvili equation and the hyperbolic Vlasov-Poisson equation. These experiments demonstrate that exponential Parareal methods can achieve signiﬁcant parallel speedup on diﬀerent types of non-diﬀusive equations.","2023-01-09","2023-06-25 11:42:20","2024-02-08 16:04:06","2023-06-25 11:42:20","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.03764 [cs, math]","","C:\Users\isido\Zotero\storage\EJTQSHKJ\Buvoli en Minion - 2023 - Exponential Runge-Kutta Parareal for Non-Diffusive.pdf","","ODE; exponential integrators; parareal","","","","","","","","","","","","","","","","","","","","arXiv:2301.03764","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3P73L5V","journalArticle","2021","Zanger, Benjamin; Mendl, Christian B.; Schulz, Martin; Schreiber, Martin","Quantum Algorithms for Solving Ordinary Differential Equations via Classical Integration Methods","Quantum","","2521-327X","10.22331/q-2021-07-13-502","http://arxiv.org/abs/2012.09469","Identifying computational tasks suitable for (future) quantum computers is an active field of research. Here we explore utilizing quantum computers for the purpose of solving differential equations. We consider two approaches: (i) basis encoding and fixed-point arithmetic on a digital quantum computer, and (ii) representing and solving high-order Runge-Kutta methods as optimization problems on quantum annealers. As realizations applied to two-dimensional linear ordinary differential equations, we devise and simulate corresponding digital quantum circuits, and implement and run a 6$^{\mathrm{th}}$ order Gauss-Legendre collocation method on a D-Wave 2000Q system, showing good agreement with the reference solution. We find that the quantum annealing approach exhibits the largest potential for high-order implicit integration methods. As promising future scenario, the digital arithmetic method could be employed as an ""oracle"" within quantum search algorithms for inverse problems.","2021-07-13","2023-06-25 11:45:32","2024-02-08 16:04:18","2023-06-25 11:45:32","502","","","5","","Quantum","","","","","","","","en","","","","","arXiv.org","","arXiv:2012.09469 [quant-ph]","","C:\Users\isido\Zotero\storage\6LH65XLT\Zanger e.a. - 2021 - Quantum Algorithms for Solving Ordinary Differenti.pdf","","ODE; quantum computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TF8J32A5","preprint","2022","Huang, Zhongzhan; Liang, Senwei; Zhang, Hong; Yang, Haizhao; Lin, Liang","Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec","","","","","http://arxiv.org/abs/2208.03680","Ensemble-based large-scale simulation of dynamical systems is essential to a wide range of science and engineering problems. Conventional numerical solvers used in the simulation are significantly limited by the step size for time integration, which hampers efficiency and feasibility especially when high accuracy is desired. To overcome this limitation, we propose a data-driven corrector method that allows using large step sizes while compensating for the integration error for high accuracy. This corrector is represented in the form of a vector-valued function and is modeled by a neural network to regress the error in the phase space. Hence we name the corrector neural vector (NeurVec). We show that NeurVec can achieve the same accuracy as traditional solvers with much larger step sizes. We empirically demonstrate that NeurVec can accelerate a variety of numerical solvers significantly and overcome the stability restriction of these solvers. Our results on benchmark problems, ranging from high-dimensional problems to chaotic systems, suggest that NeurVec is capable of capturing the leading error term and maintaining the statistics of ensemble forecasts.","2022-08-07","2023-07-06 12:34:51","2024-02-08 16:05:30","2023-07-06 12:34:51","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2208.03680 [cs, math]","Comment: Technical report","C:\Users\isido\Zotero\storage\V8G6BSNX\Huang e.a. - 2022 - Accelerating Numerical Solvers for Large-Scale Sim.pdf","","ODE; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2208.03680","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VSTZWV6","preprint","2023","Huang, Zhongzhan; Liang, Mingfu; Lin, Liang","On Robust Numerical Solver for ODE via Self-Attention Mechanism","","","","","http://arxiv.org/abs/2302.10184","With the development of deep learning techniques, AI-enhanced numerical solvers are expected to become a new paradigm for solving differential equations due to their versatility and effectiveness in alleviating the accuracy-speed trade-off in traditional numerical solvers. However, this paradigm still inevitably requires a large amount of high-quality data, whose acquisition is often very expensive in natural science and engineering problems. Therefore, in this paper, we explore training efﬁcient and robust AI-enhanced numerical solvers with a small data size by mitigating intrinsic noise disturbances. We ﬁrst analyze the ability of the self-attention mechanism to regulate noise in supervised learning and then propose a simple-yet-effective numerical solver, AttSolver, which introduces an additive self-attention mechanism to the numerical solution of differential equations based on the dynamical system perspective of the residual neural network. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, demonstrate the effectiveness of AttSolver in generally improving the performance of existing traditional numerical solvers without any elaborated model crafting. Finally, we analyze the convergence, generalization, and robustness of the proposed method experimentally and theoretically.","2023-02-04","2023-07-06 12:29:43","2024-02-08 16:05:33","2023-07-06 12:29:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.10184 [cs, math]","Comment: Work in progress. Technical report","C:\Users\isido\Zotero\storage\B6JQY5GS\Huang e.a. - 2023 - On Robust Numerical Solver for ODE via Self-Attent.pdf","","ODE; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2302.10184","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHB8H5KI","preprint","2023","Finzi, Marc; Potapczynski, Andres; Choptuik, Matthew; Wilson, Andrew Gordon","A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks","","","","","http://arxiv.org/abs/2304.14994","Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difﬁcult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of model parameters, they are restricted to small neural networks, signiﬁcantly limiting their ability to represent intricate PDE initial conditions and solutions. Building on these insights, we develop Neural IVP, an ODE based IVP solver which prevents the network from getting ill-conditioned and runs in time linear in the number of parameters, enabling us to evolve the dynamics of challenging PDEs with neural networks.","2023-04-28","2023-07-06 12:20:45","2024-02-08 16:05:39","2023-07-06 12:20:45","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2304.14994 [cs, math, stat]","Comment: ICLR 2023. Code available at https://github.com/mfinzi/neural-ivp","C:\Users\isido\Zotero\storage\G6V5GEWN\2304.14994.pdf","","PDE; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2304.14994","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7BXR7I33","journalArticle","2020","Mukhopadhyay, Samrat","Stochastic Gradient Descent For Linear Systems With Sequential Matrix Entry Accumulation","Signal Processing","","","10.1016/j.sigpro.2020.107494","","Conventional stochastic iterative methods are often employed for solving linear systems of equations involving large matrix sizes using low memory footprint. However, their performances are often limited by the unavailability of all the matrix entries, which is often termed as the problem of missing data. Although Ma and Needell [1] have recently proposed a method, termed as mSGD, assuming a model for data missing that results in improved convergence, their result is also affected by constant large variance of the stochastic gradient. In this paper we propose a SGD type method termed as cumulative information SGD (CISGD) for solving a linear system with missing data with an additional provision to accumulate a very small number of matrix entries sequentially per iteration, termed as the sequential matrix entry accumulation (SEMEA) mechanism. CISGD uses the data collected by SEMEA mechanism along with the prior model for data missing mechanism of [1] to gradually reduce variance of the stochastic gradient. The convergence of the proposed CISGD is theoretically analyzed and some interesting implications of the result are investigated under a specific SEMEA mechanism. Finally, numerical experiments are performed along with simulations that corroborate the theoretical findings regarding the efficacy of the proposed CISGD method.","2020-06-01","2023-07-05 06:47:09","2024-02-09 18:19:52","","107494","","","171","","Signal Processing","","","","","","","","","","","","","ResearchGate","","","","","https://www.researchgate.net/publication/338760625_Stochastic_Gradient_Descent_For_Linear_Systems_With_Sequential_Matrix_Entry_Accumulation","linear systems; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JETBS48M","preprint","2019","Ma, Anna; Needell, Deanna","Stochastic Gradient Descent for Linear Systems with Missing Data","","","","","http://arxiv.org/abs/1702.07098","Traditional methods for solving linear systems have quickly become impractical due to an increase in the size of available data. Utilizing massive amounts of data is further complicated when the data is incomplete or has missing entries. In this work, we address the obstacles presented when working with large data and incomplete data simultaneously. In particular, we propose to adapt the Stochastic Gradient Descent method to address missing data in linear systems. Our proposed algorithm, the Stochastic Gradient Descent for Missing Data method (mSGD), is introduced and theoretical convergence guarantees are provided. In addition, we include numerical experiments on simulated and real world data that demonstrate the usefulness of our method.","2019-01-07","2023-07-05 06:19:40","2024-02-09 18:19:55","2023-07-05 06:19:40","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1702.07098 [math]","","C:\Users\isido\Zotero\storage\BE762FG6\1702.07098.pdf","","linear systems; gradient descent; SGD","","","","","","","","","","","","","","","","","","","","arXiv:1702.07098","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKY8LKAD","videoRecording","2023","Shlomi Steinberg","Towards Practical Physical-Optics Rendering — Presentation","","","","","https://www.youtube.com/watch?v=4Z3ohq0ZszI","Physical light transport (PLT) algorithms can represent the wave nature of light globally in a scene, and are consistent with Maxwell’s theory of electromagnetism. As such, they are able to reproduce the wave-interference and diffraction effects of real physical optics. However, the recent works that have proposed PLT are too expensive to apply to real-world scenes with complex geometry and materials. To address this problem, we propose a novel framework for physical light transport based on several key ideas that actually makes PLT practical for complex scenes. First, we restrict the spatial coherence shape of light to an anisotropic Gaussian and justify this restriction with general arguments based on entropy. This restriction serves to simplify the rest of the derivations, without practical loss of generality. To describe partially-coherent light, we present new rendering primitives that generalize the radiometric radiance and irradiance, and are based on the well-known Stokes parameters. We are able to represent light of arbitrary spectral content and states of polarization, and with any coherence volume and anisotropy. We also present the wave BSDF to accurately render diffractions and wave-interference effects. Furthermore, we present an approach to importance sample this wave BSDF to facilitate bi-directional path tracing, which has been previously impossible. We show good agreement with state-of-the-art methods, but unlike them we are able to render complex scenes where all the materials are new, coherence-aware physical optics materials, and with performance approaching that of “classical” rendering methods.","2023-07-15","2023-07-15 09:52:53","2024-02-09 19:48:33","2023-07-15 09:52:53","","","","","","","","","","","","","","","","","","","YouTube","","","","","","rendering","","","","","","","","","","","","","","","","","","","","","","19:00","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5HVVJ63","journalArticle","2024","Steinberg, Shlomi; Ramamoorthi, Ravi; Bitterli, Benedikt; D’Eon, Eugene; Yan, Ling-Qi; Pharr, Matt","A Generalized Ray Formulation For Wave-Optics Rendering","","","","","","In this paper we present the generalized ray: an extension of the classical ray to wave optics. The generalized ray retains the defining characteristics of the ray-optical ray: locality and linearity. These properties allow the generalized ray to serve as a “point query” of light’s behaviour—the same purpose that the classical ray fulfils in rendering. By using such generalized rays, we enable the rendering of complex scenes, like the one shown, under rigorous wave-optical light transport. Materials admitting diffractive optical phenomena are visible: (a) a Bornite ore with a layer of copper oxide causing interference; (b) a Brazilian Rainbow Boa, whose scales are biological diffraction grated surfaces; and (c) a Chrysomelidae beetle, whose colour arises due to naturally-occurring multilayered interference reflectors in its elytron. Our formalism serves as a link between path tracing techniques and wave optics, and admits a highly general validity domain. Therefore, we are able to apply sophisticated sampling techniques, and achieve performance that surpasses the state-of-the-art by orders-of-magnitude. We indicate resolution and samples-per-pixel (spp) count in all figures rendered using our method. While these figures showcase converged (high spp) results, our implementation also allows interactive rendering of all these scenes at 1 spp. Frame times (at 1 spp) for interactive rendering are indicated. Implementation, as well as additional renderings and videos are available in our supplemental material.","2024-01-07","2023-07-15 09:33:11","2024-08-12 14:37:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\E9BU6M43\Steinberg e.a. - A Generalized Ray Formulation For Wave-Optics Rend_compressed.pdf","","rendering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N46GEQUR","journalArticle","2017","Daun, Thomas; Heinrich, Stefan","Complexity of parametric initial value problems for systems of ODEs","Mathematics and Computers in Simulation","","03784754","10.1016/j.matcom.2015.04.008","https://linkinghub.elsevier.com/retrieve/pii/S0378475415000713","We study the approximate solution of initial value problems for parameter dependent ﬁnite or inﬁnite systems of scalar ordinary diﬀerential equations (ODEs). Both the deterministic and the randomized setting is considered, with input data from various smoothness classes. We study deterministic and Monte Carlo multilevel algorithms and derive convergence rates. Moreover, we prove their optimality by showing matching (in some limit cases up to logarithmic factors) lower bounds and settle this way the complexity. Comparisons between the deterministic and randomized setting are given, as well.","2017-05","2023-07-17 07:32:11","2023-08-03 19:08:03","2023-07-17 07:32:11","72-85","","","135","","Mathematics and Computers in Simulation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BJIUZ98W\Daun en Heinrich - 2017 - Complexity of parametric initial value problems fo.pdf","","ODE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPPIFCN4","journalArticle","2010","Goćwin, Maciej; Szczęsny, Marek","On the complexity of a two-point boundary value problem in different settings","International Journal of Computer Mathematics","","0020-7160, 1029-0265","10.1080/00207160903401852","http://www.tandfonline.com/doi/abs/10.1080/00207160903401852","We study the complexity of a two-point boundary value problem. We concentrate on the linear problem of order k with separated boundary conditions. Right-hand side functions are assumed to be r times differentiable with all derivatives bounded by a constant. We consider three models of computation: deterministic with standard and linear information, randomized and quantum. In each setting, we construct an algorithm for solving the problem, which allows us to establish upper complexity bounds. In the deterministic setting, we show that the use of linear information gives us a speed-up of at least one order of magnitude compared with the standard information. For randomized algorithms, we show that the speed-up over standard deterministic algorithms is by 1/2 in the exponent. For quantum algorithms, we can achieve a speed-up by one order of magnitude. We also provide lower complexity bounds. They match upper bounds in the deterministic setting with the standard information, and almost match upper bounds in the randomized and quantum settings. In the deterministic setting with the linear information, a gap still remains between the upper and lower complexity bounds.","2010-12","2023-07-17 07:50:28","2023-08-11 18:28:29","2023-07-17 07:50:28","3370-3386","","15","87","","International Journal of Computer Mathematics","","","","","","","","en","","","","","Semantic Scholar","","","[TLDR] This work studies the complexity of a two-point boundary value problem, and shows that the use of linear information gives us a speed-up of at least one order of magnitude compared with the standard information.","","https://www.semanticscholar.org/paper/On-the-complexity-of-a-two-point-boundary-value-in-Go%C4%87win-Szczesny/383b09d521353ea4639246999ec63b788114f2c8","boundary value problems; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNQXJK34","preprint","2022","Herman, Dylan; Googin, Cody; Liu, Xiaoyuan; Galda, Alexey; Safro, Ilya; Sun, Yue; Pistoia, Marco; Alexeev, Yuri","A Survey of Quantum Computing for Finance","","","","","http://arxiv.org/abs/2201.02773","Quantum computers are expected to surpass the computational capabilities of classical computers during this decade and have transformative impact on numerous industry sectors, particularly ﬁnance. In fact, ﬁnance is estimated to be the ﬁrst industry sector to beneﬁt from quantum computing, not only in the medium and long terms, but even in the short term. This survey paper presents a comprehensive summary of the state of the art of quantum computing for ﬁnancial applications, with particular emphasis on stochastic modeling, optimization, and machine learning, describing how these solutions, adapted to work on a quantum computer, can potentially help to solve ﬁnancial problems, such as derivative pricing, risk modeling, portfolio optimization, natural language processing, and fraud detection, more eﬃciently and accurately. We also discuss the feasibility of these algorithms on nearterm quantum computers with various hardware implementations and demonstrate how they relate to a wide range of use cases in ﬁnance. We hope this article will not only serve as a reference for academic researchers and industry practitioners but also inspire new ideas for future research.","2022-06-27","2023-07-20 14:49:17","2024-02-08 16:05:23","2023-07-20 14:49:17","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2201.02773 [quant-ph, q-fin]","Comment: 60 pages, 5 figures","C:\Users\isido\Zotero\storage\JM9ZHVD5\2201.02773.pdf","","quantum computing","","","","","","","","","","","","","","","","","","","","arXiv:2201.02773","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTU6RLC9","preprint","2021","Pistoia, Marco; Ahmad, Syed Farhan; Ajagekar, Akshay; Buts, Alexander; Chakrabarti, Shouvanik; Herman, Dylan; Hu, Shaohan; Jena, Andrew; Minssen, Pierre; Niroula, Pradeep; Rattew, Arthur; Sun, Yue; Yalovetzky, Romina","Quantum Machine Learning for Finance","","","","","http://arxiv.org/abs/2109.04298","Quantum computers are expected to surpass the computational capabilities of classical computers during this decade, and achieve disruptive impact on numerous industry sectors, particularly ﬁnance. In fact, ﬁnance is estimated to be the ﬁrst industry sector to beneﬁt from Quantum Computing not only in the medium and long terms, but even in the short term. This review paper presents the state of the art of quantum algorithms for ﬁnancial applications, with particular focus to those use cases that can be solved via Machine Learning.","2021-09-09","2023-07-20 14:49:23","2024-02-08 16:05:20","2023-07-20 14:49:23","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2109.04298 [quant-ph]","<div data-schema-version=""8""><p>not detailed, dont like how they present the ideas</p> </div>","C:\Users\isido\Zotero\storage\RUN9KCYU\Pistoia e.a. - 2021 - Quantum Machine Learning for Finance.pdf","","machine learning; quantum computing","","","","","","","","","","","","","","","","","","","","arXiv:2109.04298","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64CMKIKW","journalArticle","2019","Orús, Román; Mugel, Samuel; Lizaso, Enrique","Quantum computing for finance: Overview and prospects","Reviews in Physics","","24054283","10.1016/j.revip.2019.100028","https://linkinghub.elsevier.com/retrieve/pii/S2405428318300571","We discuss how quantum computation can be applied to financial problems, providing an overview of current approaches and potential prospects. We review quantum optimization algorithms, and expose how quantum annealers can be used to optimize portfolios, find arbitrage opportunities, and perform credit scoring. We also discuss deep-learning in finance, and suggestions to improve these methods through quantum machine learning. Finally, we consider quantum amplitude estimation, and how it can result in a quantum speed-up for Monte Carlo sampling. This has direct applications to many current financial methods, including pricing of derivatives and risk analysis. Perspectives are also discussed.","2019-11","2023-07-20 14:53:45","2023-08-11 18:28:12","2023-07-20 14:53:45","100028","","","4","","Reviews in Physics","Quantum computing for finance","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\7E3B7Z3C\Orús e.a. - 2019 - Quantum computing for finance Overview and prospe.pdf","","quantum computing; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KXMV99M","videoRecording","2017","Ryan T","Lecture 01: Linear regression","","","","","https://www.youtube.com/watch?v=Z1cSby8ZzhA","","2017-01-20","2023-07-22 13:46:17","2024-02-09 19:48:30","2023-07-22 13:46:17","","","","","","","Lecture 01","","","","","","","","","","","","YouTube","","","<div data-schema-version=""8""><p>3:40 </p> <p>combining computational complexity and statistical complexity is an open problem</p> <p>how hard is something statistically constraint to computational complexity</p> </div>","","","","","","","","","","","","","","","","","","","","","","","","","1:10:06","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHW6LQLK","videoRecording","2021","Cynthia Rudin","PaCMAP: An algorithm for dimension reduction","","","","","https://www.youtube.com/watch?v=sD-uDZ8zXkc","This video describes the PaCMAP technique for dimension reduction, which is an alternative to t-SNE and UMAP. It was derived based on an understanding of what makes dimension algorithms work. Paper: Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization","2021-08-30","2023-07-23 10:26:44","2024-02-09 19:48:26","2023-07-23 10:26:44","","","","","","","PaCMAP","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","14:49","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IDMEA3N","preprint","2021","Wang, Yingfan; Huang, Haiyang; Rudin, Cynthia; Shaposhnik, Yaron","Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization","","","","","http://arxiv.org/abs/2012.04456","Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMAP have demonstrated impressive visualization performance on many real world datasets. One tension that has always faced these methods is the trade-off between preservation of global structure and preservation of local structure: these methods can either handle one or the other, but not both. In this work, our main goal is to understand what aspects of DR methods are important for preserving both local and global structure: it is difficult to design a better method without a true understanding of the choices we make in our algorithms and their empirical impact on the lower-dimensional embeddings they produce. Towards the goal of local structure preservation, we provide several useful design principles for DR loss functions based on our new understanding of the mechanisms behind successful DR methods. Towards the goal of global structure preservation, our analysis illuminates that the choice of which components to preserve is important. We leverage these insights to design a new algorithm for DR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP), which preserves both local and global structure. Our work provides several unexpected insights into what design choices both to make and avoid when constructing DR algorithms.","2021-08-24","2023-07-23 10:26:53","2024-02-08 16:05:51","2023-07-23 10:26:53","","","","","","","Understanding How Dimension Reduction Tools Work","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2012.04456 [cs, stat]","","C:\Users\isido\Zotero\storage\TW5ZFTXU\2012.04456.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2012.04456","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9KSZG368","videoRecording","2023","PyData","Leland McInnes - Data Mapping for Data Exploration | PyData Seattle 2023","","","","","https://www.youtube.com/watch?v=r8dWZX8IGw8","As embeddings and and vector databases become ever more popular we need to develop new tools for exploratory data analysis. One such approach is interactive data maps -- using 2D map style representations of the data, combined with rich interactivity that can link back to the source data. We'll look at the open source tools available for building interactive data maps, and work through an example use case.","2023-06-20","2023-07-23 11:34:45","2024-02-09 19:48:23","2023-07-23 11:34:45","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","38:15","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6UB3DBA","webpage","2023","","Tutte Institute for Mathematics and Computing","GitHub","","","","https://github.com/TutteInstitute","Tutte Institute for Mathematics and Computing. Tutte Institute for Mathematics and Computing has 8 repositories available. Follow their code on GitHub.","2023-07-23","2023-07-23 11:48:17","2024-08-12 14:59:46","2023-07-23 11:48:17","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\XQFYKYGV\tutteinstitute.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8P3AXZTK","preprint","2022","Xin, Rui; Zhong, Chudi; Chen, Zhi; Takagi, Takuya; Seltzer, Margo; Rudin, Cynthia","Exploring the Whole Rashomon Set of Sparse Decision Trees","","","","","http://arxiv.org/abs/2209.08040","In any given machine learning problem, there might be many models that explain the data almost equally well. However, most learning algorithms return only one of these models, leaving practitioners with no practical way to explore alternative models that might have desirable properties beyond what could be expressed by a loss function. The Rashomon set is the set of these all almost-optimal models. Rashomon sets can be large in size and complicated in structure, particularly for highly nonlinear function classes that allow complex interaction terms, such as decision trees. We provide the ﬁrst technique for completely enumerating the Rashomon set for sparse decision trees; in fact, our work provides the ﬁrst complete enumeration of any Rashomon set for a non-trivial problem with a highly nonlinear discrete function class. This allows the user an unprecedented level of control over model choice among all models that are approximately equally good. We represent the Rashomon set in a specialized data structure that supports efﬁcient querying and sampling. We show three applications of the Rashomon set: 1) it can be used to study variable importance for the set of almost-optimal trees (as opposed to a single tree), 2) the Rashomon set for accuracy enables enumeration of the Rashomon sets for balanced accuracy and F1-score, and 3) the Rashomon set for a full dataset can be used to produce Rashomon sets constructed with only subsets of the data set. Thus, we are able to examine Rashomon sets across problems with a new lens, enabling users to choose models rather than be at the mercy of an algorithm that produces only a single model.","2022-10-25","2023-07-23 11:56:34","2023-08-11 18:27:29","2023-07-23 11:56:34","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.08040 [cs]","Comment: NeurIPS 2022 (Oral)","C:\Users\isido\Zotero\storage\D6ZQM9GH\Xin e.a. - 2022 - Exploring the Whole Rashomon Set of Sparse Decisio.pdf","","machine learning; interpretable machine learning","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","arXiv:2209.08040","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5N5M8MW","conferencePaper","2022","Wang, Zijie J.; Zhong, Chudi; Xin, Rui; Takagi, Takuya; Chen, Zhi; Chau, Duen Horng; Rudin, Cynthia; Seltzer, Margo","TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization","2022 IEEE Visualization and Visual Analytics (VIS)","","","10.1109/VIS54862.2022.00021","http://arxiv.org/abs/2209.09227","Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees--a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.","2022-10","2023-07-23 11:57:56","2023-08-11 18:27:20","2023-07-23 11:57:56","60-64","","","","","","TimberTrek","","","","","","","en","","","","","arXiv.org","","arXiv:2209.09227 [cs]","Comment: Accepted at IEEE VIS 2022. 5 pages, 6 figures. For a demo video, see https://youtu.be/3eGqTmsStJM. For a live demo, visit https://poloclub.github.io/timbertrek","C:\Users\isido\Zotero\storage\8A6FQSQ2\Wang e.a. - 2022 - TimberTrek Exploring and Curating Sparse Decision.pdf","","machine learning; interpretable machine learning","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NE3GSXAN","videoRecording","2021","Cynthia Rudin","Nobel Conference - Rudin","","","","","https://www.youtube.com/watch?v=Eokq35hm3mM","Cynthia Rudin's lecture for the 2021 Nobel Conference, on the topic of interpretable machine learning.","2021-10-04","2023-07-23 12:38:27","2024-02-09 19:48:18","2023-07-23 12:38:27","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; interpretable machine learning","","","","","","","","","","","","","","","","","","","","","","37:24","","","","","","","","","","","","","","","","","","","","","","","","",""
"NP6BXKFA","preprint","2023","Bonev, Boris; Kurth, Thorsten; Hundt, Christian; Pathak, Jaideep; Baust, Maximilian; Kashinath, Karthik; Anandkumar, Anima","Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere","","","","","http://arxiv.org/abs/2306.03838","Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolutionindependent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates since they incorrectly assume a flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmospheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.","2023-06-06","2023-07-25 18:14:05","2023-08-03 19:06:25","2023-07-25 18:14:05","","","","","","","Spherical Fourier Neural Operators","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2306.03838 [physics]","","C:\Users\isido\Zotero\storage\4GD8MR5N\Bonev e.a. - 2023 - Spherical Fourier Neural Operators Learning Stabl.pdf","","machine learning","Computer Science - Machine Learning; Mathematics - Numerical Analysis; Physics - Computational Physics; Physics - Atmospheric and Oceanic Physics","","","","","","","","","","","","","","","","","","","arXiv:2306.03838","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9U582DK7","preprint","2023","Kalinin, Kirill P.; Mourgias-Alexandris, George; Ballani, Hitesh; Berloff, Natalia G.; Clegg, James H.; Cletheroe, Daniel; Gkantsidis, Christos; Haller, Istvan; Lyutsarev, Vassily; Parmigiani, Francesca; Pickup, Lucinda; Rowstron, Antony","Analog Iterative Machine (AIM): using light to solve quadratic optimization problems with mixed variables","","","","","http://arxiv.org/abs/2304.12594","Solving optimization problems is challenging for existing digital computers and even for future quantum hardware. The practical importance of diverse problems, from healthcare to financial optimization, has driven the emergence of specialised hardware over the past decade. However, their support for problems with only binary variables severely restricts the scope of practical problems that can be efficiently embedded. We build analog iterative machine (AIM), the first instance of an opto-electronic solver that natively implements a wider class of quadratic unconstrained mixed optimization (QUMO) problems and supports all-to-all connectivity of both continuous and binary variables.Beyond synthetic 7-bit problems at small-scale, AIM solves the financial transaction settlement problem entirely in analog domain with higher accuracy than quantum hardware and at room temperature. With compute-in-memory operation and spatial-division multiplexed representation of variables, the design of AIM paves the path to chip-scale architecture with 100 times speed-up per unit-power over the latest GPUs for solving problems with 10,000 variables. The robustness of the AIM algorithm at such scale is further demonstrated by comparing it with commercial production solvers across multiple benchmarks, where for several problems we report new best solutions. By combining the superior QUMO abstraction, sophisticated gradient descent methods inspired by machine learning, and commodity hardware, AIM introduces a novel platform with a step change in expressiveness, performance, and scalability, for optimization in the post-Moores law era.","2023-06-20","2023-07-27 18:27:48","2023-08-11 18:25:13","2023-07-27 18:27:48","","","","","","","Analog Iterative Machine (AIM)","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2304.12594 [physics]","Comment: Main sections plus supplementa material for a total of 41 pages. 7 figures","C:\Users\isido\Zotero\storage\LIAP6PZ5\Kalinin e.a. - 2023 - Analog Iterative Machine (AIM) using light to sol.pdf","","optimization; analog","Mathematics - Optimization and Control; Computer Science - Emerging Technologies; Physics - Applied Physics","","","","","","","","","","","","","","","","","","","arXiv:2304.12594","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYG5E5Y7","book","2022","Foucart, Simon","Mathematical Pictures at a Data Science Exhibition","","978-1-00-900393-3 978-1-316-51888-5 978-1-00-900185-4","","","https://www.cambridge.org/core/product/identifier/9781009003933/type/book","This text provides deep and comprehensive coverage of the mathematical background for data science, including machine learning, optimal recovery, compressed sensing, optimization, and neural networks. In the past few decades, heuristic methods adopted by big tech companies have complemented existing scientific disciplines to form the new field of Data Science. This text embarks the readers on an engaging itinerary through the theory supporting the field. Altogether, twenty-seven lecture-length chapters with exercises provide all the details necessary for a solid understanding of key topics in data science. While the book covers standard material on machine learning and optimization, it also includes distinctive presentations of topics such as reproducing kernel Hilbert spaces, spectral clustering, optimal recovery, compressed sensing, group testing, and applications of semidefinite programming. Students and data scientists with less mathematical background will appreciate the appendices that provide more background on some of the more abstract concepts.","2022-05-31","2023-07-29 11:52:05","2023-08-11 18:23:53","2023-07-29 11:52:05","","","","","","","","","","","","Cambridge University Press","","en","","","","","DOI.org (Crossref)","","DOI: 10.1017/9781009003933","","C:\Users\isido\Zotero\storage\IV37PZ8W\Foucart - 2022 - Mathematical Pictures at a Data Science Exhibition.pdf","","optimization; machine learning; IBC","","","","","","","","","","","","","","","","","","","","","1","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36T4NZIV","journalArticle","2023","Bakbouk, Ghada; Peers, Pieter","Mean Value Caching for Walk on Spheres","","","","10.2312/SR.20231120","https://diglib.eg.org/handle/10.2312/sr20231120","Walk on Spheres (WoS) is a grid-free Monte Carlo method for numerically estimating solutions for elliptical partial differential equations (PDE) such as the Laplace and Poisson PDEs. While WoS is efficient for computing a solution value at a single evaluation point, it becomes less efficient when the solution is required over a whole domain or a region of interest. WoS computes a solution for each evaluation point separately, possibly recomputing similar sub-walks multiple times over multiple evaluation points. In this paper, we introduce a novel filtering and caching strategy that leverages the volume mean value property (in contrast to the boundary mean value property that forms the core of WoS). In addition, to improve quality under sparse cache regimes, we describe a weighted mean as well as a non-uniform sampling method. Finally, we show that we can reduce the variance within the cache by recursively applying the volume mean value property on the cached elements.","2023","2023-08-01 08:39:05","2024-02-09 14:46:59","2023-08-01 08:39:05","10 pages","","","","","","","","","","","","","en","Creative Commons Attribution 4.0 International","","","","DOI.org (Datacite)","","Artwork Size: 10 pages Publisher: The Eurographics Association","<h2>Other</h2> Ray Tracing<h2>Other</h2> Ghada Bakbouk and Pieter Peers","C:\Users\isido\Zotero\storage\VT2WT79A\Bakbouk en Peers - 2023 - Mean Value Caching for Walk on Spheres.pdf","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E3E7WWC9","videoRecording","2023","Computerphile","Quantum Computing in Reality (Pt3: Beyond the Hype) - Computerphile","","","","","https://www.youtube.com/watch?v=gH_rF9LLzFA","What's actually possible vs what's theoretically possible vs what's actually useful with quantum computing? Victor V. Albert of University of Maryland and NIST simplifies!","2023-07-06","2023-08-01 09:48:47","2024-02-09 19:48:14","2023-08-01 09:48:47","","","","","","","Quantum Computing in Reality (Pt3","","","","","","","","","","","","YouTube","","","","","","quantum computing","","","","","","","","","","","","","","","","","","","","","","7:39","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CAH26QQ","webpage","2017","","BSc_Mathematics_2017_Hollander_RM.pdf","","","","","https://fse.studenttheses.ub.rug.nl/14905/1/BSc_Mathematics_2017_Hollander_RM.pdf","For a homogeneous system of linear differential equations with a constant coefficient matrix, the fundamental matrix can be computed for example using the Jordan Canonical Form. However, when the coefficient matrix depends on a single variable t, this method does not always provide a correct solution. The fundamental matrix can be computed using a numerical method, for example the Picard iterative method, but using such method, one can lose important qualitative properties. Wilhelm Magnus provided a method to approximate the fundamental matrix, such that these qualitative properties are preserved. In this thesis, we will state Magnus’ theorem and it’s proof. We will compute the Magnus Expansion for some simple examples, and compare the solutions with the fundamental matrices obtained by applying Picard iteration.","2017","2023-08-01 13:24:32","2024-08-12 14:44:46","2023-06-12 11:16:15","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\Y5W4DSKX\BSc_Mathematics_2017_Hollander_RM.pdf","","exponential integrators; ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVAQMSES","journalArticle","2021","Bochacik, Tomasz; Goćwin, Maciej; Morkisz, Paweł M.; Przybyłowicz, Paweł","Randomized Runge-Kutta method -- stability and convergence under inexact information","Journal of Complexity","","0885064X","10.1016/j.jco.2021.101554","http://arxiv.org/abs/2006.12131","We deal with optimal approximation of solutions of ODEs under local Lipschitz condition and inexact discrete information about the right-hand side functions. We show that the randomized two-stage Runge-Kutta scheme is the optimal method among all randomized algorithms based on standard noisy information. We perform numerical experiments that conﬁrm our theoretical ﬁndings. Moreover, for the optimal algorithm we rigorously investigate properties of regions of absolute stability.","2021-08","2023-08-02 13:46:47","2024-02-08 16:06:48","2023-08-02 13:46:47","101554","","","65","","Journal of Complexity","","","","","","","","en","","","","","arXiv.org","","arXiv:2006.12131 [cs, math]","","C:\Users\isido\Zotero\storage\NYK5DYWB\Bochacik e.a. - 2021 - Randomized Runge-Kutta method -- stability and con.pdf","","ODE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EIIDDXXM","journalArticle","2019","Eisenmann, Monika; Kovács, Mihály; Kruse, Raphael; Larsson, Stig","On a Randomized Backward Euler Method for Nonlinear Evolution Equations with Time-Irregular Coefficients","Foundations of Computational Mathematics","","1615-3375, 1615-3383","10.1007/s10208-018-09412-w","http://link.springer.com/10.1007/s10208-018-09412-w","In this paper, we introduce a randomized version of the backward Euler method that is applicable to stiff ordinary differential equations and nonlinear evolution equations with time-irregular coefﬁcients. In the ﬁnite-dimensional case, we consider Carathéodory-type functions satisfying a one-sided Lipschitz condition. After investigating the well-posedness and the stability properties of the randomized scheme, we prove the convergence to the exact solution with a rate of 0.5 in the root-mean-square norm assuming only that the coefﬁcient function is square integrable with respect to the temporal parameter. These results are then extended to the approximation of inﬁnitedimensional evolution equations under monotonicity and Lipschitz conditions. Here, we consider a combination of the randomized backward Euler scheme with a Galerkin ﬁnite element method. We obtain error estimates that correspond to the regularity of the exact solution. The practicability of the randomized scheme is also illustrated through several numerical experiments.","2019-12","2023-08-02 14:19:04","2023-08-03 19:04:59","2023-08-02 14:19:04","1387-1430","","6","19","","Found Comput Math","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\NBPU49QC\Eisenmann e.a. - 2019 - On a Randomized Backward Euler Method for Nonlinea.pdf","","ODE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4NEYBB7","bookSection","2007","Dimov, Ivan; Georgieva, Rayna","Complexity of Monte Carlo Algorithms for a Class of Integral Equations","Computational Science – ICCS 2007","978-3-540-72583-1 978-3-540-72584-8","","","http://link.springer.com/10.1007/978-3-540-72584-8_97","In this work we study the computational complexity of a class of grid Monte Carlo algorithms for integral equations. The idea of the algorithms consists in an approximation of the integral equation by a system of algebraic equations. Then the Markov chain iterative Monte Carlo is used to solve the system. The assumption here is that the corresponding Neumann series for the iterative matrix does not necessarily converge or converges slowly. We use a special technique to accelerate the convergence. An estimate of the computational complexity of Monte Carlo algorithm using the considered approach is obtained. The estimate of the complexity is compared with the corresponding quantity for the complexity of the grid-free Monte Carlo algorithm. The conditions under which the class of grid Monte Carlo algorithms is more efﬁcient are given.","2007","2023-08-06 10:55:18","2023-08-11 18:23:03","2023-08-06 10:55:18","731-738","","","4487","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-72584-8_97","","C:\Users\isido\Zotero\storage\IQMEVCF4\Dimov en Georgieva - 2007 - Complexity of Monte Carlo Algorithms for a Class o.pdf","","monte carlo; walk on spheres; integral equations","","Shi, Yong; Van Albada, Geert Dick; Dongarra, Jack; Sloot, Peter M. A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Rangan, C. Pandu; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44PWSP8Y","videoRecording","2023","Qiskit","Understanding Quantum Information and Computation | Lesson 05 | Quantum Query Algorithms","","","","","https://www.youtube.com/watch?v=2wticzHE1vs","Lesson 5 is the first lesson of the second unit of the series, which is on the fundamentals of quantum algorithms. This lesson is on the query model of computation, and describes a progression of quantum algorithms that offer advantages over classical algorithms within this model. The quantum algorithms include Deutsch’s algorithm, the Deutsch-Josza algorithm, and Simon’s algorithm. 0:00 — Introduction 2:13 — Overview 3:45 — A standard picture of computation 5:19 — The query model of computation 8:19 — Examples of query problems 12:49 — Query gates 22:08 — Deutsch’s algorithm 22:50 — Deutsch’s problem 24:46 — Deutsch’s algorithm 31:08 — Phase kickback 34:06 — The Deutsch-Jozsa circuit 35:40 — The Deutsch-Jozsa problem 37:48 — Deutsch-Jozsa analysis 47:30 — The Bernstein-Vazirani problem 51:53 — Simon’s algorithm 52:46 — Simon’s problem 57:46 — Simon’s algorithm 59:25 — Simon’s algorithm analysis 1:09:28 — Classical post-processing 1:15:31 — Classical difficulty 1:18:14 — Conclusion Link to textbook lesson: https://learn.qiskit.org/course/algor... Series Overview:    • Understanding Quantum Information and...   #ibmquantum #learnquantum #qiskit","2023-06-08","2023-08-06 15:51:48","2024-02-09 19:48:12","2023-08-06 15:51:48","","","","","","","","","","","","","","","","","","","YouTube","","","","","","IBC; quantum computing","","","","","","","","","","","","","","","","","","","","","","1:19:17","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJPCZ7WN","preprint","2023","Stoudenmire, E. M.; Waintal, Xavier","Grover's Algorithm Offers No Quantum Advantage","","","","","http://arxiv.org/abs/2303.11317","Grover's algorithm is one of the primary algorithms offered as evidence that quantum computers can provide an advantage over classical computers. It involves an ""oracle"" (external quantum subroutine) which must be specified for a given application and whose internal structure is not part of the formal scaling of the quantum speedup guaranteed by the algorithm. Grover's algorithm also requires exponentially many steps to succeed, raising the question of its implementation on near-term, non-error-corrected hardware and indeed even on error-corrected quantum computers. In this work, we construct a quantum inspired algorithm, executable on a classical computer, that performs Grover's task in a linear number of call to the oracle - an exponentially smaller number than Grover's algorithm - and demonstrate this algorithm explicitly for boolean satisfiability problems (3-SAT). Our finding implies that there is no a priori theoretical quantum speedup associated with Grover's algorithm. We critically examine the possibility of a practical speedup, a possibility that depends on the nature of the quantum circuit associated with the oracle. We argue that the unfavorable scaling of the success probability of Grover's algorithm, which in the presence of noise decays as the exponential of the exponential of the number of qubits, makes a practical speedup unrealistic even under extremely optimistic assumptions on both hardware quality and availability.","2023-03-20","2023-08-06 17:23:31","2024-02-08 16:07:37","2023-08-06 17:23:31","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2303.11317 [cond-mat, physics:quant-ph]","Comment: 16 pages, 4 figures","C:\Users\isido\Zotero\storage\JVZATAKX\Stoudenmire en Waintal - 2023 - Grover's Algorithm Offers No Quantum Advantage.pdf","","quantum computing","","","","","","","","","","","","","","","","","","","","arXiv:2303.11317","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CST6I8ZZ","videoRecording","2023","","Sparse Training of Neural Networks Using AC/DC - YouTube","","","","","https://www.youtube.com/watch?v=npcT5VO6YOA","This video summarizes deep learning research on the ""Alternating Compressed/DeCompressed (AC/DC) training of DNNs.""  AC/DC, a novel sparse training algorithm, allows you to prune your DNNs while still being trained, leading to simpler and quicker training workflows. AC/DC outperforms existing sparse training methods in accuracy, even at high sparsity levels. And in some cases, it creates even more accurate sparse networks than their dense counterparts.  This video covers the background on training-aware sparsification and what benefits it offers; a deep dive into the AC/DC algorithm, how it works, and the intuition behind it; and a walk-through of how to use AC/DC on your own models and deploy it for better performance and accuracy. To get the best deep learning CPU performance, try our DeepSparse Runtime: https://neuralmagic.com/deepsparse/ If you have any questions, engage with the Neural Magic community: https://join.slack.com/t/discuss-neur...","2023-02-26","2023-08-08 13:00:58","2024-08-12 14:57:09","2023-08-08 13:00:58","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\PGJHNNXE\watch.html","","compressing neural networks; deep learning; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLWMC92L","preprint","2021","Nagel, Markus; Fournarakis, Marios; Amjad, Rana Ali; Bondarenko, Yelysei; van Baalen, Mart; Blankevoort, Tijmen","A White Paper on Neural Network Quantization","","","","","http://arxiv.org/abs/2106.08295","While neural networks have advanced the frontiers in many applications, they often come at a high computational cost. Reducing the power and latency of neural network inference is key if we want to integrate modern networks into edge devices with strict power and compute requirements. Neural network quantization is one of the most effective ways of achieving these savings but the additional noise it induces can lead to accuracy degradation.","2021-06-15","2023-08-08 15:38:03","2024-02-08 16:07:42","2023-08-08 15:38:03","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.08295 [cs]","","C:\Users\isido\Zotero\storage\LUBPHW5D\Nagel e.a. - 2021 - A White Paper on Neural Network Quantization.pdf","","machine learning; compressing neural networks; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2106.08295","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4Z96WLWC","journalArticle","1997","Hetzler, Steven M.","A Continuous Version of Newton's Method","The College Mathematics Journal","","0746-8342, 1931-1346","10.1080/07468342.1997.11973888","https://www.tandfonline.com/doi/full/10.1080/07468342.1997.11973888","Newton's method for finding a root of f(x) = 0, starting with an initial estimate xo, is equivalent to applying Euler's method with step size ft = 1 to approximate the solution of the initial value problem ^ = ?f(x)/f'(x), with initial condition x(0) = xo. We will examine this connection and prove that, under quite general conditions, the solution of this initial value problem converges to a root of / as t ?>? oo. Then we consider some examples to show how this continuous version of Newton's method [6], [7], which deserves to be more widely known, can be used to find roots in cases where Newton's method fails. The discussion is accessible to calculus students and provides a nice motivation for studying differential equations. It would complement a discussion, as in [4] or [8], of the chaotic behavior of Newton's method.","1997-11","2023-08-10 15:53:57","2023-08-11 19:00:08","2023-08-10 15:53:57","348-351","","5","28","","The College Mathematics Journal","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\2QQAE4CJ\Hetzler - 1997 - A Continuous Version of Newton's Method.pdf","","gradient descent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9C2DC33Z","preprint","2017","Kunsch, Robert J.","High-Dimensional Function Approximation: Breaking the Curse with Monte Carlo Methods","","","","","http://arxiv.org/abs/1704.08213","In this dissertation we study the tractability of the information-based complexity $n(\varepsilon,d)$ for $d$-variate function approximation problems. In the deterministic setting for many unweighted problems the curse of dimensionality holds, that means, for some fixed error tolerance $\varepsilon>0$ the complexity $n(\varepsilon,d)$ grows exponentially in $d$. For integration problems one can usually break the curse with the standard Monte Carlo method. For function approximation problems, however, similar effects of randomization have been unknown so far. The thesis contains results on three more or less stand-alone topics. For an extended five page abstract, see the section ""Introduction and Results"". Chapter 2 is concerned with lower bounds for the Monte Carlo error for general linear problems via Bernstein numbers. This technique is applied to the $L_{\infty}$-approximation of certain classes of $C^{\infty}$-functions, where it turns out that randomization does not affect the tractability classification of the problem. Chapter 3 studies the $L_{\infty}$-approximation of functions from Hilbert spaces with methods that may use arbitrary linear functionals as information. For certain classes of periodic functions from unweighted periodic tensor product spaces, in particular Korobov spaces, we observe the curse of dimensionality in the deterministic setting, while with randomized methods we achieve polynomial tractability. Chapter 4 deals with the $L_1$-approximation of monotone functions via function values. It is known that this problem suffers from the curse in the deterministic setting. An improved lower bound shows that the problem is still intractable in the randomized setting. However, Monte Carlo breaks the curse, in detail, for any fixed error tolerance $\varepsilon>0$ the complexity $n(\varepsilon,d)$ grows exponentially in $\sqrt{d}$ only.","2017-04-26","2023-08-11 18:06:31","2024-02-08 16:07:46","2023-08-11 18:06:31","","","","","","","High-Dimensional Function Approximation","","","","","arXiv","","de","","","","","arXiv.org","","arXiv:1704.08213 [math]","Comment: This is the author's submitted PhD thesis, still in the referee process; <div data-schema-version=""8""><p>good intro to information complexity</p> </div>","C:\Users\isido\Zotero\storage\WLUIMUHC\Kunsch - 2017 - High-Dimensional Function Approximation Breaking .pdf","","IBC","","","","","","","","","","","","","","","","","","","","arXiv:1704.08213","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L3R47SN8","videoRecording","2023","Gabriel Mongaras","RetNet: A Successor to Transformer for Large Language Models Explained","","","","","https://www.youtube.com/watch?v=B_iGSeG04qo","Paper found here: https://arxiv.org/abs/2307.08621 Code will be found here soon: https://github.com/microsoft/unilm/tr...","2023-07-20","2023-08-14 13:24:50","2024-02-09 19:48:09","2023-08-14 13:24:50","","","","","","","RetNet","","","","","","","","","","","","YouTube","","","","","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","","","1:09:56","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFRCK33U","preprint","2023","Sun, Yutao; Dong, Li; Huang, Shaohan; Ma, Shuming; Xia, Yuqing; Xue, Jilong; Wang, Jianyong; Wei, Furu","Retentive Network: A Successor to Transformer for Large Language Models","","","","","http://arxiv.org/abs/2307.08621","In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.","2023-08-09","2023-08-14 13:25:24","2024-02-08 16:07:50","2023-08-14 13:25:24","","","","","","","Retentive Network","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2307.08621 [cs]","","C:\Users\isido\Zotero\storage\VBIIYVIW\Sun e.a. - 2023 - Retentive Network A Successor to Transformer for .pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2307.08621","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AII9VHE","webpage","2023","","unilm/retnet at master · microsoft/unilm","GitHub","","","","https://github.com/microsoft/unilm/tree/master/retnet","Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities - microsoft/unilm","2023-08-14","2023-08-14 15:22:45","2024-08-12 14:59:59","2023-08-14 15:22:45","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\857SUU34\unilm.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYC5HM2S","preprint","2023","Ding, Jiayu; Ma, Shuming; Dong, Li; Zhang, Xingxing; Huang, Shaohan; Wang, Wenhui; Zheng, Nanning; Wei, Furu","LongNet: Scaling Transformers to 1,000,000,000 Tokens","","","","","http://arxiv.org/abs/2307.02486","Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LONGNET, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LONGNET has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LONGNET yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence. Code is available at https://aka.ms/LongNet.","2023-07-19","2023-08-14 15:28:23","2024-02-08 16:07:53","2023-08-14 15:28:23","","","","","","","LongNet","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2307.02486 [cs]","Comment: Work in progress","C:\Users\isido\Zotero\storage\V4J9KZAD\Ding e.a. - 2023 - LongNet Scaling Transformers to 1,000,000,000 Tok.pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2307.02486","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XKLNVIX","journalArticle","2006","Heinrich, Stefan","The randomized information complexity of elliptic PDE","Journal of Complexity","","0885064X","10.1016/j.jco.2005.11.003","https://linkinghub.elsevier.com/retrieve/pii/S0885064X05001135","Westudy the information complexityinthe randomized setting ofsolving ageneral elliptic PDE oforder 2m inasmo oth, bounded domain Q   Rd with smooth coe cientsand homogeneous boundary conditions. The solution issough ton asmo oth submanifold M   Q of dimension 0   d1   d,the righthand side issupp osed tobein Cr(Q), the error ismeasured inthe L1(M )norm. Wesho wthat the n-th minimal error is(up tologarithmic factors) oforder n min((r+2m)=d1;r=d +1=2): Forcomparison, inthe deterministic setting the n-th minimal error is oforder n r=d; for all d1.","2006-04","2023-08-15 08:10:49","2024-02-09 19:43:40","2023-08-15 08:10:49","220-249","","2","22","","Journal of Complexity","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\WJB2Z5H5\Heinrich - 2006 - The randomized information complexity of elliptic .pdf","","monte carlo; PDE; integral equations; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XQFPYPN","bookSection","2018","Heinrich, Stefan","On the Complexity of Parametric ODEs and Related Problems","Contemporary Computational Mathematics - A Celebration of the 80th Birthday of Ian Sloan","978-3-319-72455-3 978-3-319-72456-0","","","http://link.springer.com/10.1007/978-3-319-72456-0_26","We present an iterative Monte Carlo procedure to solve initial value problems for systems of ordinary diﬀerential equations depending on a parameter. It is based on a multilevel Monte Carlo algorithm for parametric indeﬁnite integration. As an application, we also obtain a respective method for solving almost linear ﬁrst order partial diﬀerential equations. We also consider deterministic algorithms.","2018","2023-08-15 09:13:20","2023-08-18 16:01:47","2023-08-15 09:13:20","567-595","","","","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-319-72456-0_26","","C:\Users\isido\Zotero\storage\UXR58HGT\Heinrich - 2018 - On the Complexity of Parametric ODEs and Related P.pdf","","monte carlo; ODE; parametric; IBC","","Dick, Josef; Kuo, Frances Y.; Woźniakowski, Henryk","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PAIT7BY","journalArticle","2014","Vavalis, Manolis","IMPLEMENTING HYBRID PDE SOLVERS","","","","10.6084/M9.FIGSHARE.1134520","https://figshare.com/articles/journal_contribution/IMPLEMENTING_HYBRID_PDE_SOLVERS/1134520","We investigate the possibility that we effectively combine both conventional deterministic PDE solving methods and traditional probabilistic Monte Carlo approaches for solving linear Elliptic Partial Differential Equations. Our objective is to provide a robust and easy to use implementation that allows further experimentation on this new type of PDE solvers in order to elucidate their capabilities and computational characteristics. We ﬁrst present the general formulation of the algorithm, then describe its implementation in C++ for a class of model problems in two and three space dimensions, we analyze its performance and we ﬁnally discuss possible extensions.","2014","2023-08-15 09:14:56","2024-02-09 14:47:21","2023-08-15 09:14:56","204576 Bytes","","","","","","","","","","","","","en","Creative Commons Attribution 4.0 International","","","","DOI.org (Datacite)","","Artwork Size: 204576 Bytes Publisher: figshare","","C:\Users\isido\Zotero\storage\2ISLXPWQ\Vavalis - 2014 - IMPLEMENTING HYBRID PDE SOLVERS.pdf","","monte carlo; PDE; walk on spheres; boundary value problems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMGG9X2U","journalArticle","2006","Heinrich, Stefan","Monte Carlo approximation of weakly singular integral operators","Journal of Complexity","","0885064X","10.1016/j.jco.2005.11.002","https://linkinghub.elsevier.com/retrieve/pii/S0885064X05001123","We study the randomized approximation of weakly singular integral operators. For a suitable class of kernels having a standard type of singularity and being otherwise of ﬁnite smoothness, we develop a Monte Carlo multilevel method, give convergence estimates and prove lower bounds which show the optimality of this method and establish the complexity. As an application we obtain optimal methods for and the complexity of randomized solution of the Poisson equation in simple domains, when the solution is sought on subdomains of arbitrary dimension.","2006-04","2023-08-15 09:24:07","2023-08-18 15:51:51","2023-08-15 09:24:07","192-219","","2","22","","Journal of Complexity","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\W79W9FBH\Heinrich - 2006 - Monte Carlo approximation of weakly singular integ.pdf","","monte carlo; integral equations; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTBDQAW4","journalArticle","2023","Kim, Bo-Kyeong; Song, Hyoung-Kyu; Castells, Thibault; Choi, Shinkook","BK-SDM: Architecturally Compressed Stable Diffusionfor Efficient Text-to-Image Generation","","","","","","Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized enabling fewer sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BKSDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models on the zero-shot MS-COCO benchmark. Moreover, we show the applicability of our lightweight pretrained models in personalized generation with DreamBooth finetuning.","2023-05-25","2023-08-17 10:23:38","2024-08-12 14:30:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\WSZLKADA\Kim e.a. - BK-SDM Architecturally Compressed Stable Diffusio.pdf","","deep learning; diffusion model; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EFA7KMX","preprint","2023","Tang, Zineng; Yang, Ziyi; Zhu, Chenguang; Zeng, Michael; Bansal, Mohit","Any-to-Any Generation via Composable Diffusion","","","","","http://arxiv.org/abs/2305.11846","We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io","2023-05-19","2023-08-17 12:51:18","2024-02-08 16:08:44","2023-08-17 12:51:18","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2305.11846 [cs, eess]","Comment: Project Page: https://codi-gen.github.io","C:\Users\isido\Zotero\storage\HSBGQDVT\2305.11846.pdf","","machine learning; diffusion model; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2305.11846","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QN483E47","videoRecording","2023","Simons Institute","What Can I Do With a Noisy Quantum Computer?","","","","","https://www.youtube.com/watch?v=OrSuP1oaMhE","Abhinav Kandala (IBM) https://simons.berkeley.edu/talks/abh... Provable NISQ Quantum Advantage Workshop","2023-08-11","2023-08-18 11:18:18","2024-02-09 19:48:06","2023-08-18 11:18:18","","","","","","","","","","","","","","","","","","","YouTube","","","","","","quantum computing","","","","","","","","","","","","","","","","","","","","","","47:09","","","","","","","","","","","","","","","","","","","","","","","","",""
"YB2G6G53","bookSection","2000","Heinrich, Stefan","Wavelet Monte Carlo Methods for the Global Solution of Integral Equations","Monte-Carlo and Quasi-Monte Carlo Methods 1998","978-3-540-66176-4 978-3-642-59657-5","","","http://link.springer.com/10.1007/978-3-642-59657-5_15","We study the global solution of Fredholm integral equations of the second kind by the help of Monte Carlo methods. Global solution means that we seek to approximate the full solution function. This is opposed to the usual applications of Monte Carlo, were one only wants to approximate a functional of the solution. In recent years several researchers developed Monte Carlo methods also for the global problem.","2000","2023-08-18 13:18:15","2023-08-18 15:49:50","2023-08-18 13:18:15","227-237","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-59657-5_15","","C:\Users\isido\Zotero\storage\MR89NC5G\Heinrich - 2000 - Wavelet Monte Carlo Methods for the Global Solutio.pdf","","monte carlo; integral equations; IBC","","Niederreiter, Harald; Spanier, Jerome","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W63E4D7Y","journalArticle","1999","Heinrich, Stefan; Sindambiwe, Eugène","Monte Carlo Complexity of Parametric Integration","Journal of Complexity","","0885064X","10.1006/jcom.1999.0508","https://linkinghub.elsevier.com/retrieve/pii/S0885064X99905083","The Monte Carlo complexity of computing integrals depending on a parameter is analyzed for smooth integrands. An optimal algorithm is developed on the basis of a multigrid variance reduction technique. The complexity analysis implies that our algorithm attains a higher convergence rate than any deterministic algorithm. Moreover, because of savings due to computation on multiple grids, this rate is also higher than that of previously developed Monte Carlo algorithms for parametric integration.","1999-09","2023-08-18 13:55:32","2024-02-09 19:43:22","2023-08-18 13:55:32","317-341","","3","15","","Journal of Complexity","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\IU5ZFZ6K\Heinrich en Sindambiwe - 1999 - Monte Carlo Complexity of Parametric Integration.pdf","","monte carlo; parametric; IBC; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YEA9GPBH","journalArticle","1977","Granovskii, L","The Monte Carlo method","","","","","","Today, the Monte Carlo method has great applied value. On the other hand, it is possible to assert that it is the source of interesting and difficult mathematical problems. The first successes in using electronic computers engendered, outside and within the Soviet Union, unjustified (as subsequently became clear) confidence that the increased power of the computer permits, with help of the Monte Carlo method, the solution of practically any problem, with no heed given to the development of the method. When it turned out that this was not the case, interest in the Monte Carlo method began to slacken, but there then began to appear theoretical works from which it followed that an unthinking utilization of the method as a universal panacea could only wreak harm on its results. In addition, the use of means for taking a priori information into account, thus enriching the method, and the careful processing of algorithms and programs, usually make it possible to solve, in acceptable quantities of machine time, the unique multidimensional problems for which it would be unsafe to utilize other computational methods. Today, the Monte Carlo method is vigorously developed, and one can say that the foundations of its theory have all been laid. It should immediately be said that the Monte Carlo method has been defined as a method for simulating random variables and processes, with the goal of estimating parameters, representing solutions of problems of interest to us. The subject matter of the present survey is comprised of the results in the areas of theory and applications of the Monte Carlo method, all obtained over the period from the end of 1968 through 1974, inclusive, although, in individual cases, we mention even earlier work. An exception is the section ""Monte Carlo method and computational mathematics~ which dontains a survey of works over the last 10 years since, in this area, it is difficult to isolate the results of the last five years only. The volume of this survey does not permit, unfortunately, the consideration of certain fundamental works which preceded 1965. The reader can familiarize himself with such works by using various monographs [79 184, and others].","1977-02","2023-08-19 09:17:29","2024-08-12 14:23:28","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\MZDRJ4TW\Granovskii - The Monte Carlo method.pdf","","integral equations; monte carlo; nonlinear systems; PDE; quadrature; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKRZJHEI","preprint","2021","Bassey, Joshua; Qian, Lijun; Li, Xianfang","A Survey of Complex-Valued Neural Networks","","","","","http://arxiv.org/abs/2101.12249","Artiﬁcial neural networks (ANNs) based machine learning models and especially deep learning models have been widely applied in computer vision, signal processing, wireless communications, and many other domains, where complex numbers occur either naturally or by design. However, most of the current implementations of ANNs and machine learning frameworks are using real numbers rather than complex numbers. There are growing interests in building ANNs using complex numbers, and exploring the potential advantages of the so called complex-valued neural networks (CVNNs) over their real-valued counterparts. In this paper, we discuss the recent development of CVNNs by performing a survey of the works on CVNNs in the literature. Speciﬁcally, detailed review of various CVNNs in terms of activation function, learning and optimization, input and output representations, and their applications in tasks such as signal processing and computer vision are provided, followed by a discussion on some pertinent challenges and future research directions.","2021-01-28","2023-08-20 07:16:00","2024-02-08 16:09:22","2023-08-20 07:16:00","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2101.12249 [cs, stat]","Comment: 15 pages","C:\Users\isido\Zotero\storage\PLZR5XNK\2101.12249.pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2101.12249","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FUCHB59","preprint","2023","Hendrycks, Dan; Gimpel, Kevin","Gaussian Error Linear Units (GELUs)","","","","","http://arxiv.org/abs/1606.08415","We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is xΦ(x), where Φ(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1x>0). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.","2023-06-05","2023-08-20 07:24:02","2024-02-08 16:09:29","2023-08-20 07:24:02","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1606.08415 [cs]","Comment: Trimmed version of 2016 draft","C:\Users\isido\Zotero\storage\IIFN5LK7\Hendrycks en Gimpel - 2023 - Gaussian Error Linear Units (GELUs).pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:1606.08415","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CATF2ZHD","conferencePaper","2021","Derrow-Pinion, Austin; She, Jennifer; Wong, David; Lange, Oliver; Hester, Todd; Perez, Luis; Nunkesser, Marc; Lee, Seongjae; Guo, Xueying; Wiltshire, Brett; Battaglia, Peter W.; Gupta, Vishal; Li, Ang; Xu, Zhongwen; Sanchez-Gonzalez, Alvaro; Li, Yujia; Veličković, Petar","ETA Prediction with Graph Neural Networks in Google Maps","Proceedings of the 30th ACM International Conference on Information & Knowledge Management","","","10.1145/3459637.3481916","http://arxiv.org/abs/2108.11482","Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events—such as rush hours—that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).","2021-10-26","2023-08-20 08:23:21","2024-02-08 16:09:32","2023-08-20 08:23:21","3767-3776","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv:2108.11482 [cs]","Comment: To appear at CIKM 2021 (Applied Research Track). 10 pages, 4 figures","C:\Users\isido\Zotero\storage\ESBWTZHD\Derrow-Pinion e.a. - 2021 - ETA Prediction with Graph Neural Networks in Googl.pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZDIEDKI","preprint","2023","Schaeffer, Rylan; Khona, Mikail; Robertson, Zachary; Boopathy, Akhilan; Pistunova, Kateryna; Rocks, Jason W.; Fiete, Ila Rani; Koyejo, Oluwasanmi","Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle","","","","","http://arxiv.org/abs/2303.14151","Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error ﬂies against classical learning theory on overﬁtting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we brieﬂy describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent. We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated. We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent. Code is publicly available.","2023-03-24","2023-08-20 09:03:28","2024-02-08 16:09:38","2023-08-20 09:03:28","","","","","","","Double Descent Demystified","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2303.14151 [cs, stat]","","C:\Users\isido\Zotero\storage\6U342PNH\Schaeffer e.a. - 2023 - Double Descent Demystified Identifying, Interpret.pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2303.14151","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILH3YHY6","journalArticle","2022","Power, Alethea; Burda, Yuri; Edwards, Harri; Babuschkin, Igor; Misra, Vedant","GROKKING: GENERALIZATION BEYOND OVERFIT- TING ON SMALL ALGORITHMIC DATASETS","","","","","","In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efﬁciency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of “grokking” a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overﬁtting. We also study generalization as a function of dataset size and ﬁnd that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the ﬁnite training dataset.","2022-01-06","2023-08-20 09:33:49","2024-08-12 14:35:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\4N6BI7X4\Power e.a. - GROKKING GENERALIZATION BEYOND OVERFIT- TING ON S.pdf","","deep learning; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YDWWYSZ","preprint","2021","Ye, Weirui; Liu, Shaohuai; Kurutach, Thanard; Abbeel, Pieter; Gao, Yang","Mastering Atari Games with Limited Data","","","","","http://arxiv.org/abs/2111.00210","Reinforcement learning has achieved great success in many applications. However, sample efﬁciency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been signiﬁcant progress in sample efﬁcient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efﬁcient model-based visual RL algorithm built on MuZero, which we name EfﬁcientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the ﬁrst time an algorithm achieves super-human performance on Atari games with such little data. EfﬁcientZero’s performance is also close to DQN’s performance at 200 million frames while we consume 500 times less data. EfﬁcientZero’s low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.","2021-12-11","2023-08-20 11:04:10","2024-02-08 16:09:43","2023-08-20 11:04:10","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2111.00210 [cs]","Comment: Published at NeurIPS 2021; Homepage: https://yewr.github.io/projects/efficientzero/; <div data-schema-version=""8""><p>particular to atari?</p> </div>","C:\Users\isido\Zotero\storage\IAVVVP6D\Ye e.a. - 2021 - Mastering Atari Games with Limited Data.pdf","","machine learning; reinforcement learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2111.00210","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R647LS59","videoRecording","2023","Simons Institute","An observation on Generalization","","","","","https://www.youtube.com/watch?v=AKMuA_TVz3A","I came up with something I'll tell you about some very old results that we had it open AI many years ago back in 2016 even which really affected the way I think about unsupervised learning and I thought I'd share them with you it is possible that at this point you'll find them obvious but maybe not all of them so there is at least a small chance that you'll find it interesting so I want to set the expectations modestly and hopefully exceed them so a theory of unsupervised learning how can such a thing exist so okay before we talk about unsupervised learning we want to talk about learning in general what is learning and why does learning work at why should learning work at all and why should computers be able to learn and now we are just used to the fact we take it for granted that neural networks learn but but why do they like mathematically why should they why would data have regularity that our machine learning models can capture so that's not an obvious question and one important conceptual advance that has taken place in machine learning many years ago by multiple people with the discovery and the formalization of supervised learning so it goes under the name of back learning or statistical learning theory and the nice thing about supervised learning is that it gives you a precise mathematical condition under which learning must succeed you're told that if you have some data from some data distribution that if you manage to achieve a low training loss and the number of your degrees of freedom is not is smaller than your training set then you will achieve low tester and you are guaranteed to do so you have a mathematical condition where you can say well if if I did find a function out of my function class which achieve low training error then learning will succeed and you can say yeah this is a very sensible mathematical thing that we can reason about and that's why supervised learning is easy and then you had all these theorems which I thought were simple you know I found the Mulligan you've seen maybe theorems like this if you have your basically it's like this is the sort of thing where if you know it you're going to say oh yeah that thing and if you don't know it it will not be possible to explain it in 30 seconds though it is possible to explain it in five minutes just not 30 seconds none of this stuff is complicated and you've got your little proof which says well if you have some number of functions in your function class the probability that your train error will be far from your test error for at least one function there is a mass and the math is simple this is all the math three lines of math so three lines of math can prove all of supervisor well that's nice that's very nice so supervised learning is something that is well understood comparatively speaking well understood we know why it must it must succeed so we can go forth and collect large supervised learning data sets and be completely certain that models will keep getting better so that's that's the story there and yeah but forgot to mention a very important piece of these results the test distribution and training distribution need to be the same if they're the same then your theory of supervised learning kicks in and works and will be successful so conceptually it is trivial we have an answer for why supervised learning works why speech recognition should work by image categorization should work because they all reduce to supervised learning which works which has this mathematical guarantee so this is very nice here I want to make a small side comment on VC Dimension to those of you who care about such things there may be a small subset so if you want to zone out for the next 30 seconds feel free to do so so a lot of writings about statistical learning theory emphasize the VC Dimension as a key component but the main reason the VC dimension in fact the only reason the VC Dimension was invented was to allow us to handle","2023-08-15","2023-08-20 11:35:02","2024-02-09 19:48:03","2023-08-20 11:35:02","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning","","","","","","","","","","","","","","","","","","","","","","57:20","","","","","","","","","","","","","","","","","","","","","","","","",""
"I7EFR4BR","preprint","2022","Press, Ofir; Smith, Noah A.; Lewis, Mike","Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","","","","","http://arxiv.org/abs/2108.12409","Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.","2022-04-22","2023-08-20 15:49:24","2024-02-08 16:09:48","2023-08-20 15:49:24","","","","","","","Train Short, Test Long","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2108.12409 [cs]","","C:\Users\isido\Zotero\storage\2ZNXT4VH\2108.12409.pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2108.12409","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXKL3YYT","videoRecording","2022","Mutual Information","Reinforcement Learning, by the Book","","","","","https://www.youtube.com/watch?v=NFo9v_yKQXA","Part one of a six part series on Reinforcement Learning. If you want to understand the fundamentals in a short amount of time, you're in the right place. SOCIAL MEDIA Enjoy learning this way? Want me to make more videos? Consider supporting me on Patreon: https://www.patreon.com/MutualInforma... Twitter : https://twitter.com/DuaneJRich Github: https://github.com/Duane321 SOURCES [1] R. Sutton and A. Barto. Reinforcement learning: An Introduction (2nd Ed). MIT Press, 2018. [2] H. Hasselt, et al. RL Lecture Series, Deepmind and UCL, 2021,    • DeepMind x UCL | Deep Learning Lectur...   [3] D. Silver, Lecture 1: Introduction to Reinforcement Learning, Deepmind, 2015,    • RL Course by David Silver - Lecture 1...   [4] Y. Wang, Pricing at Lyft, Lyft, 2022, https://eng.lyft.com/pricing-at-lyft-... [5] A. Irpan, Deep Reinforcement Learning Doesn't Work Yet, 2018, https://www.alexirpan.com/2018/02/14/... [6] D. Silver, et al., Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm, Deepmind, 2017. [7] J. Schrittwieser, et al. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, Deepmind, 2020. [8] R. Roy, et al. PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning, Nvidia, 2022 [9] J. Degrave, et al. Magnetic control of tokamak plasmas through deep reinforcement learning, Nature, 2021. SOURCE NOTES [1] is my primary source for this series. It largely determined the notation and the set of topics and motivated much of the commentary. This series could not be what it is without the comprehensive and consistent presentation of this vast subject provided by this text. If you're interested in learning more, I highly recommend this text. In preparation, I also took Deepmind's course [2], with the intend to understand a different perspective. The material is similar, though not entirely. Notably, Deepmind's problem statement involves agents receiving observations, which are summarized into an agent-specific state. This is a more application-ready problem statement. Early drafts of the series included this version of the problem, but it was cut, since it complicated following topics. Overall, I learned about RL substantially from this course. The Maze demonstration of the value function comes from David Silver's 2015 lecture [3]. [4] - [9] are sources for all papers and blogs referenced at the start of the video. It includes a blog [4] where Lyft describes how RL is used in their pricing algorithms. NOTES With regards to the statement ""RL won't be the same revolution that Neural Networks were. That's OK - NNs are quite a high bar"". I should elaborate. I anticipate a response of, ""You can't separate RL and NN, since much of the most impactful applications of RL involve NNS"". Yes, comparing these technologies is fraught. In effect, I'm presuming that if a widespread subscription to RL's problem statement is enabled by more performant NNs, that is part of the RL migration. That's not fair if we're to attribute successes to either RL or NN. In my view, this attribution isn't important. I'm merely making the claim that RL will become a primary component in many production systems. The comparison of RL vs NNs is an accidental symptom of how I pitched the RL trend. In retrospect, I would have phrased this differently. TIMESTAMP 0:00 The Trend of Reinforcement Learning 2:46 A Six Part Series 3:24 A Finite Markov Decision Process and Our Goal 9:02 An Example MDP 12:49 State and Action Value Functions 15:00 An Example of a State Value Function 16:28 The Assumptions 17:58 Watch the Next Video! CORRECTIONS 1) In the MASE, I have a -16 where there should be a -14 (Thank you rogiervdw).","2022-10-24","2023-08-26 07:25:35","2024-02-09 19:48:01","2023-08-26 07:25:35","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; reinforcement learning","","","","","","","","","","","","","","","","","","","","","","18:18","","","","","","","","","","","","","","","","","","","","","","","","",""
"82Z4K557","videoRecording","2021","Google DeepMind","DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]","","","","","https://www.youtube.com/watch?v=TCCjZe0y4Qc","Research Scientist Hado van Hasselt introduces the reinforcement learning course and explains how reinforcement learning relates to AI. Slides: https://dpmd.ai/introslides  Full video lecture series: https://dpmd.ai/DeepMindxUCL21","2021-09-09","2023-08-26 07:26:09","2024-02-09 19:47:58","2023-08-26 07:26:09","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; reinforcement learning; deep learning","","","","","","","","","","","","","","","","","","","","","","1:29:52","","","","","","","","","","","","","","","","","","","","","","","","",""
"NLXABISU","preprint","2020","Brown, Noam; Bakhtin, Anton; Lerer, Adam; Gong, Qucheng","Combining Deep Reinforcement Learning and Search for Imperfect-Information Games","","","","","http://arxiv.org/abs/2007.13544","The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exempliﬁed by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zerosum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold’em poker, while using far less domain knowledge than any prior poker AI.","2020-11-28","2023-08-26 09:11:38","2024-02-08 16:09:52","2023-08-26 09:11:38","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2007.13544 [cs]","","C:\Users\isido\Zotero\storage\49IYCFKD\Brown e.a. - 2020 - Combining Deep Reinforcement Learning and Search f.pdf","","machine learning; reinforcement learning; game theory; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2007.13544","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRLN4TPJ","preprint","2019","Brown, Noam; Lerer, Adam; Gross, Sam; Sandholm, Tuomas","Deep Counterfactual Regret Minimization","","","","","http://arxiv.org/abs/1811.00164","Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfectinformation games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain speciﬁc, abstraction algorithms may miss important strategic nuances of the game, and there is a chickenand-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the ﬁrst non-tabular variant of CFR to be successful in large games.","2019-05-22","2023-08-26 12:36:52","2024-02-08 16:09:55","2023-08-26 12:36:52","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1811.00164 [cs]","","C:\Users\isido\Zotero\storage\9ACL4P4T\Brown e.a. - 2019 - Deep Counterfactual Regret Minimization.pdf","","machine learning; reinforcement learning; game theory; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:1811.00164","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5CHEBGM","videoRecording","2021","London Machine Learning Meetup","Noam Brown | AI for Imperfect-Information Games: Poker and Beyond","","","","","https://www.youtube.com/watch?v=cn8Sld4xQjg","The field of artificial intelligence has had a number of high-profile successes in the domain of perfect-information games like chess or Go where all participants know the exact state of the world. But real-world strategic interactions typically involve hidden information, such as in negotiations, cybersecurity, and financial markets. Past AI techniques fall apart in these settings, with poker serving as the classic benchmark and challenge problem. In this talk, I will cover the key breakthroughs behind Libratus and Pluribus, the first AI agents to defeat elite human professionals in two-player no-limit poker and multiplayer no-limit poker, respectively. In particular, I will discuss new forms of the counterfactual regret minimization equilibrium-finding algorithm and breakthroughs that enabled depth-limited search for imperfect-information games to be conducted orders of magnitude faster than previous algorithms. Finally, I will conclude with a discussion on recent work combining the previously separate threads of research on perfect-information and imperfect-information games.","2021-06-09","2023-08-26 12:56:09","2024-02-09 19:47:55","2023-08-26 12:56:09","","","","","","","Noam Brown | AI for Imperfect-Information Games","","","","","","","","","","","","YouTube","","","","","","machine learning; reinforcement learning; game theory","","","","","","","","","","","","","","","","","","","","","","1:01:10","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ZMT5GYK","videoRecording","2022","DIMACS CCICADA","Tuomas Sandholm - Keynote: The State of Representing and Solving Games","","","","","https://www.youtube.com/watch?v=sfutYlNXtQc","Tuomas Sandholm delivers his keynote presentation ""The State of Representing and Solving Games"" at the IBM/DIMACS Workshop on Bridging Game Theory and Machine Learning for Multi-party Decision Making held at Rutgers University on October 27-28, 2022.","2022-11-02","2023-08-26 16:45:54","2024-02-09 19:47:52","2023-08-26 16:45:54","","","","","","","Tuomas Sandholm - Keynote","","","","","","","","","","","","YouTube","","","","","","machine learning; reinforcement learning; game theory","","","","","","","","","","","","","","","","","","","","","","1:02:14","","","","","","","","","","","","","","","","","","","","","","","","",""
"WAGUBGT3","preprint","2022","McAleer, Stephen; Farina, Gabriele; Lanctot, Marc; Sandholm, Tuomas","ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret","","","","","http://arxiv.org/abs/2206.04122","Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability in the tabular case. We show that the variance of the estimated regret of a tabular version of ESCHER with an oracle value function is signiﬁcantly lower than that of outcome sampling MCCFR and tabular DREAM with an oracle value function. We then show that a deep learning version of ESCHER outperforms the prior state of the art—DREAM and neural ﬁctitious self play (NFSP)—and the difference becomes dramatic as game size increases.","2022-10-11","2023-08-26 17:04:39","2024-02-08 16:10:00","2023-08-26 17:04:39","","","","","","","ESCHER","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2206.04122 [cs, stat]","","C:\Users\isido\Zotero\storage\7EZ2VXFM\McAleer e.a. - 2022 - ESCHER Eschewing Importance Sampling in Games by .pdf","","machine learning; reinforcement learning; game theory","","","","","","","","","","","","","","","","","","","","arXiv:2206.04122","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PKJKW9C","videoRecording","2021","London Machine Learning Meetup","Xavier Bresson | The Transformer Network for the Traveling Salesman Problem","","","","","https://www.youtube.com/watch?v=BSAMN2vclE4","Sponsored by Evolution AI: https://www.evolution.ai/ Wednesday 14 July 2021 Abstract: The Traveling Salesman Problem (TSP) is the most popular and most studied combinatorial problem, starting with von Neumann in 1951. It has driven the discovery of several optimization techniques such as cutting planes, branch-and-bound, local search, Lagrangian relaxation, and simulated annealing. The last five years have seen the emergence of promising techniques where (graph) neural networks have been capable to learn new combinatorial algorithms. The main question is whether deep learning can learn better heuristics from data, i.e. replacing human-engineered heuristics? This is appealing because developing algorithms to tackle NP-hard problems may require years of research, and many industry problems are combinatorial by nature. In this project, we propose to adapt the recent successful Transformer architecture originally developed for natural language processing to the combinatorial TSP. Training is done by reinforcement learning, hence without TSP training solutions, and decoding uses beam search. We report improved performances over recent learned heuristics. Bio: Xavier Bresson is an Associate Professor in the Department of Computer Science at the National University of Singapore (NUS). His research focuses on Graph Deep Learning, a new framework that combines graph theory and neural network techniques to tackle complex data domains. In 2016, he received the US$2.5M NRF Fellowship, the largest individual grant in Singapore, to develop this new framework. He was also awarded several research grants in the U.S. and Hong Kong. He co-authored one of the most cited works in this field, and he has recently introduced with Yoshua Bengio a benchmark that evaluates graph neural network architectures. He has organized several workshops and tutorials on graph deep learning such as the recent IPAM'21 workshop on ""Deep Learning and Combinatorial Optimization"", the MLSys'21 workshop on ""Graph Neural Networks and Systems"", the IPAM'19 and IPAM'18 workshops on ""New Deep Learning Techniques"", and the NeurIPS'17, CVPR'17 and SIAM'18 tutorials on ""Geometric Deep Learning on Graphs and Manifolds"". He has been a regular invited speaker at universities and companies to share his work. He has also been a speaker at the KDD'21, AAAI'21 and ICML'20 workshops on ""Graph Representation Learning"", and the ICLR'20 workshop on ""Deep Neural Models and Differential Equations"". He has been teaching graduate courses on Graph Neural Networks at NTU, and as a guest lecturer for Yann LeCun's course at NYU.","2021-09-12","2023-08-26 17:59:14","2024-02-09 19:47:49","2023-08-26 17:59:14","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning","","","","","","","","","","","","","","","","","","","","","","59:36","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFKSMAWF","videoRecording","2014","MIT OpenCourseWare","1. Introduction and Scope","","","","","https://www.youtube.com/watch?v=TjZBTDzGeGg","MIT 6.034 Artificial Intelligence, Fall 2010 View the complete course: http://ocw.mit.edu/6-034F10 Instructor: Patrick Winston In this lecture, Prof. Winston introduces artificial intelligence and provides a brief history of the field.  The last ten minutes are devoted to information about the course at MIT. License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu","2014-01-10","2023-08-27 08:04:15","2024-02-09 19:47:46","2023-08-27 08:04:15","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning","","","","","","","","","","","","","","","","","","","","","","47:18","","","","","","","","","","","","","","","","","","","","","","","","",""
"XJFFA7H9","preprint","2023","Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario","Deep reinforcement learning from human preferences","","","","","http://arxiv.org/abs/1706.03741","For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.","2023-02-17","2023-08-27 10:18:23","2024-02-08 16:10:26","2023-08-27 10:18:23","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1706.03741 [cs, stat]","","C:\Users\isido\Zotero\storage\SYAP9UFM\Christiano e.a. - 2023 - Deep reinforcement learning from human preferences.pdf","","machine learning; reinforcement learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:1706.03741","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFJVRQT9","journalArticle","2005","Clarberg, Petrik; Jarosz, Wojciech; Akenine-Moller, Tomas; Jensen, Henrik Wann","Wavelet Importance Sampling:","","","","","","We present a new technique for importance sampling products of complex functions using wavelets. First, we generalize previous work on wavelet products to higher dimensional spaces and show how this product can be sampled on-theﬂy without the need of evaluating the full product. This makes it possible to sample products of high-dimensional functions even if the product of the two functions in itself is too memory consuming. Then, we present a novel hierarchical sample warping algorithm that generates high-quality point distributions, which match the wavelet representation exactly. One application of the new sampling technique is rendering of objects with measured BRDFs illuminated by complex distant lighting — our results demonstrate how the new sampling technique is more than an order of magnitude more eﬃcient than the best previous techniques.","2005-07-01","2023-08-30 07:27:28","2024-08-12 13:32:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\7WBPRW3E\Clarberg e.a. - Wavelet Importance Sampling.pdf","","importance sampling; rendering; wavelets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFHPF62A","videoRecording","2023","polylog","The most powerful (and useless) algorithm","","","","","https://www.youtube.com/watch?v=9ONm1od1QZo","0:00 Intro 2:44 The Algorithm 6:38 Why it works 9:28 Code 10:41 Final Thoughts Our implementation of Universal Search: https://github.com/polylog-cs/univers... Our Patreon: https://www.patreon.com/Polylog Impromptu: https://impromptu.fun/ More about universal search: -- To prove that the universal search is asymptotically optimal, we implicitly used the fact that there exists a linear time algorithm for multiplying two numbers. This is true if you work with standard models of computers like the so-called Word RAM or pointer machine (Schonhage's algorithm, see 4.3.3. in Art of Computer Programming). For arithmetic operations, people also often consider a bit different model, where the time complexity of multiplication is O(n log n) (algorithm by Harvey & van der Hoeven). -- A historical note: Levin presented his universal search in the same paper where he presented his discovery of NP-completeness which was independent of Steve Cook. This is why the main theorem about NP-completeness is called Cook-Levin Theorem. The paper of Levin is from early 1970's. Blum's speedup theorem (see next paragraphs) is even older, from 1960's. You can see how people back then thought about these super fundamental questions like ""Is it clear that there is an asymptotically best algorithm for every problem?"". It turned out that some of the results proven back then (Blum's speedup theorem, universal search) were too abstract to be useful, and it was the concept of NP completeness and related concepts that won and now they form the basis of our theory of algorithms.   -- The way we explained universal search, it is applicable only to ""search problems"" where you are searching for something (factors of a number) and once you find it, it is easy to check that what you found is correct. However, there exists an improved universal search by Hutter that searches not only over all programs, but also over all possible mathematical proofs that analyze those programs. This way, you can get an asymptotically optimal algorithm for any problem you like, except for some absolutely insane cases like when the fastest algorithm cannot be mathematically proven to work (although it works) or the time complexity f(n) is so complicated function that computing the value of f(n) takes more than O(f(n)) time. In fact, Hutter's search can be implemented in such a way that for algorithm with time complexity f(n), it takes only 1.01f(n) + O(1) time (for absolutely humongous O(1)).  -- On the other hand, there is a so-called Blum's speedup theorem that says that there exists a certain problem and a certain function f(n) such that you can have algorithms for this problem with complexities O(f(n)), O(log(f(n))), O(log(log(f(n)))) and so on but there is no fastest algorithm. Why is this not contradicting Hutter's universal search? Because this function f(n) is so weird that computing f(n) takes more than O(f(n)) time. Big thanks to: Tomáš Gavenčiak, Matěj Konečný, Jan Petr, Hanka Rozhoňová, Tom Sláma Also, big thanks to 3blue1brown and the manim community for manim! Credits: To make this video, we used manim, a Python library: https://docs.manim.community/en/stable/ The color palette we use is solarized: https://ethanschoonover.com/solarized/ music: Thannoid by Blue Dot Sessions: https://app.sessions.blue/browse/trac... picture of monkey: DALL-E 2 picture of Leonid Levin: https://www.cs.bu.edu/~lnd/ picture of Weierstrass and Cantor function: Wikimedia Commons Brainfuck suggestion: ChatGPT Levin’s quote taken from here: http://www.hutter1.net/idsia/nipsws.htm He definitely said something similar here: https://www.cs.bu.edu/fac/lnd/expo/qc...","2023-04-04","2023-09-02 13:06:54","2024-02-09 19:47:42","2023-09-02 13:06:54","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","14:39","","","","","","","","","","","","","","","","","","","","","","","","",""
"763IESCT","preprint","2022","Kothari, Robin; O'Donnell, Ryan","Mean estimation when you have the source code; or, quantum Monte Carlo methods","","","","","http://arxiv.org/abs/2208.07544","Suppose y is a real random variable, and one is given access to “the code” that generates it (for example, a randomized or quantum circuit whose output is y). We give a quantum procedure that runs the code O(n) times and returns an estimate µ̂ for µ = E[y] that with high probability satisﬁes µ̂ − µ ≤ σ n, where σ = stddev[y]. This dependence on n is optimal for quantum algorithms. One may compare with classical algorithms, which can only achieve the √ quadratically worse µ̂ − µ ≤ σ n. Our method improves upon previous works, which either made additional assumptions about y, and/or assumed the algorithm knew an a priori bound on σ, and/or used additional logarithmic factors beyond O(n). The central subroutine for our result is essentially Grover’s algorithm but with complex phases.","2022-08-16","2023-09-02 15:20:07","2024-02-08 16:10:45","2023-09-02 15:20:07","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2208.07544 [quant-ph, stat]","Comment: 38 pages; 17 figures","C:\Users\isido\Zotero\storage\INMU8UB6\Kothari en O'Donnell - 2022 - Mean estimation when you have the source code; or,.pdf","","IBC; quantum computing","","","","","","","","","","","","","","","","","","","","arXiv:2208.07544","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMIPJ8UV","preprint","2023","McMahon, Peter L.","The physics of optical computing","","","","","http://arxiv.org/abs/2308.00088","There has been a resurgence of interest in optical computing over the past decade, both in academia and in industry, with much of the excitement centered around special-purpose optical computers for neural-network processing. Optical computing has been a topic of periodic study for over 50 years, including for neural networks three decades ago, and a wide variety of optical-computing schemes and architectures have been proposed. In this paper we provide a systematic explanation of why and how optics might be able to give speed or energy-efficiency benefits over electronics for computing, enumerating 11 features of optics that can be harnessed when designing an optical computer. One often-mentioned motivation for optical computing -- that the speed of light $c$ is fast -- is not a key differentiating physical property of optics for computing; understanding where an advantage could come from is more subtle. We discuss how gaining an advantage over state-of-the-art electronic processors will likely only be achievable by careful design that harnesses more than one of the 11 features, while avoiding a number of pitfalls that we describe.","2023-07-31","2023-09-02 17:04:10","2024-02-08 16:10:59","2023-09-02 17:04:10","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2308.00088 [physics, physics:quant-ph]","Comment: 31 pages; 11 figures","C:\Users\isido\Zotero\storage\CZQTZY5S\2308.00088.pdf","","analog","","","","","","","","","","","","","","","","","","","","arXiv:2308.00088","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EDX7VYFV","journalArticle","1998","Heinrich, S.","Monte Carlo Complexity of Global Solution of Integral Equations","Journal of Complexity","","0885-064X","https://doi.org/10.1006/jcom.1998.0471","https://www.sciencedirect.com/science/article/pii/S0885064X9890471X","The problem of the global solution of Fredholm integral equations is studied. This means that one seeks to approximate the full solution function (as opposed to the local problem, where only the value of the solution in a single point or a functional of the solution is sought). The Monte Carlo complexity, i.e., the complexity of the stochastic solution of this problem, is analyzed. The framework for this analysis is provided by information-based complexity theory. The investigations complement previous ones on the stochastic complexity of the local solution and on deterministic complexity of both local and global solutions. The results show that even in the global case Monte Carlo algorithms can perform better than deterministic ones, although the difference is not as large as in the local case.","1998","2023-09-05 15:47:18","2024-02-08 16:11:38","","151-175","","2","14","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\RG5LPU73\Heinrich - Monte Carlo Complexity of Global Solution of Integ.pdf","","monte carlo; integral equations; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MD96IXC8","journalArticle","1993","Heinrich, Stefan; Mathé, Peter","The Monte Carlo complexity of Fredholm integral equations","mathematics of computation","","","","","A complexity study of Monte Carlo methods for Fredholm integral equations is carried out. We analyze the problem of computing a functional p.(y), where y is the solution of a Fredholm integral equation on the w-dimensional unit cube Im , where the kernel k and right-hand side / are given r times differentiable functions. We permit stochastic numerical methods which can make use of function evaluations of k and / only. All Monte Carlo methods known to the authors for solving the above problem are of the order n~ '/2 , while the optimal deterministic methods yield rate n-r/(2m) ( tmls taking into account the given smoothness of the data. Here, n denotes the (average) number of function evaluations performed. The optimal algorithm we present combines deterministic and stochastic methods in an optimal way. It can be seen that both rates—the standard Monte Carlo rate for general continuous data and the deterministic rate for r-smooth data—multiply. This provides the smallest error that stochastic methods of given computational cost can achieve.","1993","2023-09-05 15:52:55","2024-02-08 16:11:34","","257-278","","201","60","","","","","","","","","","","","","","","","","ISBN: 0025-5718","","C:\Users\isido\Zotero\storage\6RNCHEZU\Heinrich en Mathé - 1993 - The Monte Carlo complexity of Fredholm integral eq.pdf","","monte carlo; integral equations; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2B3URWPH","preprint","2013","Chatterjee, Sourav","Stochastic solutions of the wave equation","","","","","http://arxiv.org/abs/1306.2382","Unlike the heat equation or the Laplace equation, solutions of the wave equation on general domains have no known stochastic representation. This short note gives a simple solution to this well known problem in arbitrary dimensions. The proposed representation has several shortcomings, one of which is that it does not cover all solutions. Still, it is proof that a large class of nontrivial solutions of the wave equation in general dimensions and domains may indeed be represented stochastically.","2013-06-10","2023-09-05 18:24:42","2024-02-08 19:11:07","2023-09-05 18:24:42","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1306.2382 [math]","Comment: 6 pages","C:\Users\isido\Zotero\storage\H466WF5L\Chatterjee - 2013 - Stochastic solutions of the wave equation.pdf","","PDE","","","","","","","","","","","","","","","","","","","","arXiv:1306.2382","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHM8C992","journalArticle","2017","Wang, Kaizheng","Stochastic Representations for the Wave Equation on Graphs and their Scaling Limits","Journal of Mathematical Analysis and Applications","","0022247X","10.1016/j.jmaa.2016.12.003","http://arxiv.org/abs/1612.09025","This paper is devoted to an interacting particle system that provides probabilistic interpretation of the wave equation on graphs. A Feynman-Kac-type formula is established, connecting the expectation of the process with the wave equation on graphs. Non-asymptotic L2 estimates are presented. It is then shown that the high-density hydrodynamic limit of the system is given by the wave equation in Euclidean space. The sharpness of scaling limit result is demonstrated by a phase transition phenomenon.","2017-05","2023-09-05 18:34:34","2024-02-08 19:11:20","2023-09-05 18:34:34","808-828","","1","449","","Journal of Mathematical Analysis and Applications","","","","","","","","en","","","","","arXiv.org","","arXiv:1612.09025 [math]","Comment: 28 pages in J. Math. Anal. Appl. (2017)","C:\Users\isido\Zotero\storage\IUXLGDMP\Wang - 2017 - Stochastic Representations for the Wave Equation o.pdf","","PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36DRJB8N","journalArticle","2016","Acebrón, Juan A.; Ribeiro, Marco A.","A Monte Carlo method for solving the one-dimensional telegraph equations with boundary conditions","Journal of Computational Physics","","00219991","10.1016/j.jcp.2015.10.027","https://linkinghub.elsevier.com/retrieve/pii/S0021999115006919","A Monte Carlo algorithm is derived to solve the one-dimensional telegraph equations in a bounded domain subject to resistive and non-resistive boundary conditions. The proposed numerical scheme is more eﬃcient than the classical Kac’s theory because it does not require the discretization of time. The algorithm has been validated by comparing the results obtained with theory and the Finite-diﬀerence time domain (FDTD) method for a typical two-wire transmission line terminated at both ends with general boundary conditions. We have also tested transmission line heterogeneities to account for wave propagation in multiple media. The algorithm is inherently parallel, since it is based on Monte Carlo simulations, and does not suﬀer from the numerical dispersion and dissipation issues that arise in ﬁnite diﬀerencebased numerical schemes on a lossy medium. This allowed us to develop an eﬃcient numerical method, capable of outperforming the classical FDTD method for large scale problems and high frequency signals.","2016-01","2023-09-09 08:11:32","2024-02-08 19:11:32","2023-09-09 08:11:32","29-43","","","305","","Journal of Computational Physics","","","","","","","","en","","","","","DOI.org (Crossref)","","","<div data-schema-version=""8""><p>acebron</p> <p></p> </div>","C:\Users\isido\Zotero\storage\7A7CJ95A\Acebrón en Ribeiro - 2016 - A Monte Carlo method for solving the one-dimension.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MFM79UP","journalArticle","2019","Zhang, Bolong; Yu, Wenjian; Mascagni, Michael","Revisiting Kac’s method: A Monte Carlo algorithm for solving the Telegrapher’s equations","Mathematics and Computers in Simulation","","03784754","10.1016/j.matcom.2018.08.007","https://linkinghub.elsevier.com/retrieve/pii/S0378475418302052","In this work, we use Kac’s stochastic model to derive a Monte Carlo (MC) algorithm for the numerical solution of the telegrapher’s equation. The major ideas are to use random values under exponential distribution to facilitate the calculation of the random time, and to accelerate the simulation for multiple points through recycling random time simulation. Compared with the MC method recently proposed by Acebrón and Ribeiro, the Kac’s model based method is able to handle two-dimensional (2-D) and higher-dimensional problems with unbounded domain, and 2-D bounded-domain problems with the homogeneous boundary condition. Moreover, it has an efficient algorithmic implementation. With numerical experiments, we have validated the accuracy and efficiency of the proposed algorithms, and their applicability to some 2-D telegrapher’s equations.","2019-02","2023-09-09 09:18:40","2024-02-08 19:12:05","2023-09-09 09:18:40","178-193","","","156","","Mathematics and Computers in Simulation","Revisiting Kac’s method","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BHHKXISN\Zhang e.a. - 2019 - Revisiting Kac’s method A Monte Carlo algorithm f.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IR6PI7YB","journalArticle","2013","Janaswamy, R.","On random time and on the relation between wave and telegraph equations","IEEE Transactions on Antennas and Propagation","","0018-926X, 1558-2221","10.1109/TAP.2013.2237739","http://ieeexplore.ieee.org/document/6401160/","Kac’s conjecture relating the solution of wave and telegraph equations in higher dimensions through a Poissonprocess-driven random time is established through the concepts of stochastic calculus. New expression is derived for the probability density function of the random time. We demonstrate how the relationship between the solution of a lossy wave- and that of a lossless wave equation can be exploited to derive some statistical identities. Relevance of the results presented to the study of pulse propagation in a dispersive medium characterized by a Lorentz or Drude model is discussed and new evolution equations for 2D Maxwell’s equations are presented for the Drude medium. It is shown that the computational time required for updating the elec√tric ﬁeld using the stochastic technique is expected to go up as O( t).","2013-05","2023-09-19 17:27:33","2024-02-08 19:12:31","2023-09-19 17:27:33","2735-2744","","5","61","","IEEE Trans. Antennas Propagat.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\JH9TH8V2\Janaswamy - 2013 - On random time and on the relation between wave an.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBLR7FBD","journalArticle","2007","Gao, Feng; Chi, Chunmei","Unconditionally stable difference schemes for a one-space-dimensional linear hyperbolic equation","Applied Mathematics and Computation","","00963003","10.1016/j.amc.2006.09.057","https://linkinghub.elsevier.com/retrieve/pii/S0096300306012653","A few explicit diﬀerence schemes are discussed for the numerical solution of the linear hyperbolic equation utt + 2a ut + b2u = uxx + f(x, t), a > 0, b > 0, in the region X = {(x, t)ja < x < b, t > 0} subject to appropriate initial and Dirichlet boundary conditions, where a and b are real numbers. The proposed scheme is showed to be unconditionally stable, and numerical result is presented.","2007-04","2023-09-19 17:44:24","2024-02-08 19:13:50","2023-09-19 17:44:24","1272-1276","","2","187","","Applied Mathematics and Computation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\8ET3PCGF\Gao en Chi - 2007 - Unconditionally stable difference schemes for a on.pdf","","PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8RH45WW","videoRecording","2013","kridnix","3.3 Solutions to Maxwell's Equations","","","","","https://www.youtube.com/watch?v=S1SVYYdxfUQ","This video was made for a junior electromagnetics course in electrical engineering at Bucknell University, USA.  The video is designed to be used as the out-of-the-classroom component and combined with active learning exercises in class.  This video briefly outlines how Maxwell's equations can be arranged into the wave equation, and the form of one of the most useful solutions- plane waves.  This video is a little more mathematical, but outlines some assumptions inherent to plane wave solutions.","2013-03-30","2023-09-20 07:29:09","2024-02-09 19:47:39","2023-09-20 07:29:09","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","18:47","","","","","","","","","","","","","","","","","","","","","","","","",""
"BF7PT6CC","videoRecording","2023","Greg Bronevetsky","Building State-of-the-Art Forecast Systems with the Ensemble Kalman Filter","","","","","https://www.youtube.com/watch?v=7QOfp4LL70I","Jeff Anderson @ NCAR https://staff.ucar.edu/users/jla https://sites.google.com/modelingtalk... Abstract: The development of numerical weather prediction was one of the great scientific and computational achievements of the last century. Computer models that approximate solutions of the partial differential equations that govern fluid flow and a comprehensive global observing network are two components of this prediction enterprise. An essential third component is data assimilation, the computational method that combines observations with predictions from previous times to produce initial conditions for subsequent predictions. The best present-day numerical weather prediction systems have evolved over decades and feature model-specific assimilation systems built with nearly a person century of effort. This talk describes the development of a community software facility for ensemble Kalman filter data assimilation, the Data Assimilation Research Testbed (DART). DART can produce high-quality weather predictions but can also be used to build a comprehensive forecast system for any prediction model and observations. The basic ensemble Kalman filter is described and applied to simple example problems. Heuristic extensions to the basic algorithm that are essential for large applications are presented in a historical context. An ensemble forecast system can do much more than just make probabilistic predictions. By confronting a prediction model with observations, it can estimate model parameters and guide general model improvement. It can also evaluate the quality of existing observations and inform the design of future observing systems. Examples of these capabilities are provided for a variety of geophysical applications. Bio: My research career has spanned two decades and has been focused by the common theme to improve predictions of the earth’s atmosphere. I have made research contributions in theoretical geophysical fluid dynamics, seasonal prediction , predictability , ensemble prediction and ensemble data assimilation. My accomplishments in software engineering, applied mathematics and statistics have been directly in support of my goal to improve prediction.","2023-05-05","2023-10-20 09:47:08","2024-02-09 19:47:34","2023-10-20 09:47:08","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","58:16","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTRI4K4K","videoRecording","2023","Peter Whidden","Training AI to Play Pokemon with Reinforcement Learning","","","","","https://www.youtube.com/watch?v=DcYLT37ImBY","Code: https://github.com/PWhiddy/PokemonRed... Collaborations, Sponsors: See channel email Buy me a tuna melt: https://www.buymeacoffee.com/peterwhi... Sections: 0:00 - Intro 1:20 - How it works 2:54 - Let the games begin 4:04 - Exploration, distraction 5:46 - Level reward 6:38 - Viridian Forest 8:06 - A new issue 8:44 - PC Trauma 10:10 - Healing 10:45 - Gym Battle 12:43 - Route 3 14:44 - Mt Moon 15:54 - Map Visualizations 18:53 - RNG manipulation 20:07 - First Outro  20:26 - Technical Intro, Challenges 21:44 - Simplify 22:43 - Efficient Iteration 23:56 - Environment, Reward function 26:26 - Metrics & Visualization 27:46 - Future Improvements 29:24 - Run it yourself 32:58 - Final Outro","2023-10-09","2023-10-20 13:38:25","2024-02-09 19:47:29","2023-10-20 13:38:25","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","33:52","","","","","","","","","","","","","","","","","","","","","","","","",""
"MMLRK653","preprint","2022","Mou, Wenlong; Khamaru, Koulik; Wainwright, Martin J.; Bartlett, Peter L.; Jordan, Michael I.","Optimal variance-reduced stochastic approximation in Banach spaces","","","","","http://arxiv.org/abs/2201.08518","We study the problem of estimating the ﬁxed point of a contractive operator deﬁned on a separable Banach space. Focusing on a stochastic query model that provides noisy evaluations of the operator, we analyze a variance-reduced stochastic approximation scheme, and establish non-asymptotic bounds for both the operator defect and the estimation error, measured in an arbitrary semi-norm. In contrast to worst-case guarantees, our bounds are instance-dependent, and achieve the local asymptotic minimax risk non-asymptotically. For linear operators, contractivity can be relaxed to multi-step contractivity, so that the theory can be applied to problems like average reward policy evaluation problem in reinforcement learning. We illustrate the theory via applications to stochastic shortest path problems, two-player zero-sum Markov games, as well as average-reward policy evaluation.","2022-11-29","2023-10-20 16:25:39","2024-02-08 19:12:41","2023-10-20 16:25:39","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2201.08518 [cs, math, stat]","","C:\Users\isido\Zotero\storage\6W6TRAM5\2201.08518.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2201.08518","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CB3NLF9U","journalArticle","2023","Chen, Evan","An Infinitely Large Napkin","","","","","","The origin of the name “Napkin” comes from the following quote of mine. I’ll be eating a quick lunch with some friends of mine who are still in high school. They’ll ask me what I’ve been up to the last few weeks, and I’ll tell them that I’ve been learning category theory. They’ll ask me what category theory is about. I tell them it’s about abstracting things by looking at just the structure-preserving morphisms between them, rather than the objects themselves. I’ll try to give them the standard example Grp, but then I’ll realize that they don’t know what a homomorphism is. So then I’ll start trying to explain what a homomorphism is, but then I’ll remember that they haven’t learned what a group is. So then I’ll start trying to explain what a group is, but by the time I finish writing the group axioms on my napkin, they’ve already forgotten why I was talking about groups in the first place. And then it’s 1PM, people need to go places, and I can’t help but think: “Man, if I had forty hours instead of forty minutes, I bet I could actually have explained this all”. This book was initially my attempt at those forty hours, but has grown considerably since then","2023-10-17","2023-10-23 14:25:04","2024-08-12 13:27:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\YGYJ9SZ2\Chen - An Infinitely Large Napkin.pdf","","♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVFI8UD9","webpage","2023","","Index—The Stacks project","","","","","https://stacks.math.columbia.edu/","","2023-10-23","2023-10-23 14:30:00","2024-08-12 14:47:54","2023-10-23 14:30:00","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\HQFE26IJ\stacks.math.columbia.edu.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLBPXP88","videoRecording","2019","Centre International de Rencontres Mathématiques","Martin J. Gander: Multigrid and Domain Decomposition: Similarities and Differences","","","","","https://www.youtube.com/watch?v=8fcFI6CgY-4","Both multigrid and domain decomposition methods are so called optimal solvers for Laplace type problems, but how do they compare? I will start by showing in what sense these methods are optimal for the Laplace equation, which will reveal that while both multigrid and domain decomposition are iterative solvers, there are fundamental differences between them. Multigrid for Laplace’s equation is a standalone solver, while classical domain decomposition methods like the additive Schwarz method or Neumann-Neumann and FETI methods need Krylov acceleration to work. I will explain in detail for each case why this is so, and then also present modifications so that Krylov acceleration is not necessary any more. For overlapping methods, this leads to the use of partitions of unity, while for non-overlapping methods, the coarse space can be a remedy. Good coarse spaces in domain decomposition methods are very different from coarse spaces in multigrid, due to the very aggressive coarsening in domain decomposition. I will introduce the concept of optimal coarse spaces for domain decomposition in a sense very different from the optimal above, and then present approximations of this coarse space. Together with optimized transmission conditions, this leads to a two level domain decomposition method of Schwarz type which is competitive with multigrid for Laplace’s equation in wallclock time. Recording during the meeting Parallel Solution Methods for Systems Arising from PDEs"" the September 19, 2019 at the Centre International de Rencontres Mathématiques (Marseille, France) Filmmaker: Guillaume Hennenfent Find this video and other talks given by worldwide mathematicians on CIRM's Audiovisual Mathematics Library: http://library.cirm-math.fr. And discover all its functionalities:  - Chapter markers and keywords to watch the parts of your choice in the video - Videos enriched with abstracts, bibliographies, Mathematics Subject Classification - Multi-criteria search by author, title, tags, mathematical area","2019-10-08","2023-10-30 12:07:23","2024-02-09 19:47:25","2023-10-30 12:07:22","","","","","","","Martin J. Gander","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","56:56","","","","","","","","","","","","","","","","","","","","","","","","",""
"PC3FS3FL","preprint","2023","Häusner, Paul; Öktem, Ozan; Sjölund, Jens","Neural incomplete factorization: learning preconditioners for the conjugate gradient method","","","","","http://arxiv.org/abs/2305.16368","In this paper, we develop a novel data-driven approach to accelerate solving largescale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competitive performance across various metrics even outside the training data distribution.","2023-05-25","2023-10-30 15:27:01","2024-02-08 19:13:18","2023-10-30 15:27:01","","","","","","","Neural incomplete factorization","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2305.16368 [cs, math, stat]","Comment: Under review. 18 pages, 10 figures","C:\Users\isido\Zotero\storage\BBADHAIE\Häusner e.a. - 2023 - Neural incomplete factorization learning precondi.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.16368","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96RN3B7Y","videoRecording","2023","Livermore Lab Events","DDPS | ML for Solving PDEs: Neural Operators on Function Spaces by Anima Anandkumar","","","","","https://www.youtube.com/watch?v=y5EJr4ofGOc","We will present exciting developments in the use of AI for scientific applications. This includes diverse domains such as weather and climate modeling, deep earth modeling, etc. We have developed principled approaches that enables zero-shot generalization beyond the training domain. This includes neural operators that yield 4-5 orders of magnitude speedups over numerical weather models and other scientific simulations. They learn mappings between function spaces that makes them ideal for capturing multi-scale processes. Bio: Anima Anandkumar is a Bren Professor at Caltech and Director of ML Research at NVIDIA. She was previously a Principal Scientist at Amazon Web Services. She has received several honors such as Alfred. P. Sloan Fellowship, NSF Career Award, Young investigator awards from DoD, and Faculty Fellowships from Microsoft, Google, Facebook, and Adobe. She is part of the World Economic Forum's Expert Network. She is passionate about designing principled AI algorithms and applying them in interdisciplinary applications. Her research focus is on unsupervised AI, optimization, and tensor methods. DDPS webinar: https://www.librom.net/ddps.html  💻 LLNL News: https://www.llnl.gov/news  📲 Instagram:   / livermore_lab    🤳 Facebook:   / livermore.lab    🐤 Twitter:   / livermore_lab    🔔 Subscribe:     / livermorelab    About LLNL: Lawrence Livermore National Laboratory has a mission of strengthening the United States’ security through development and application of world-class science and technology to: 1) enhance the nation’s defense, 2) reduce the global threat from terrorism and weapons of mass destruction, and 3) respond with vision, quality, integrity and technical excellence to scientific issues of national importance. Learn more about LLNL: https://www.llnl.gov/. LLNL-VIDEO-848789","2023-05-08","2023-10-30 15:32:08","2024-02-09 19:47:20","2023-10-30 15:32:08","","","","","","","DDPS | ML for Solving PDEs","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","51:33","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIY4MGBE","preprint","2023","Gu, Jiatao; Zhai, Shuangfei; Zhang, Yizhe; Susskind, Josh; Jaitly, Navdeep","Matryoshka Diffusion Models","","","","","http://arxiv.org/abs/2310.15111","Diffusion models are the de-facto approach for generating high-quality images and videos but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space, or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion (MDM), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small scale inputs are nested within those of the large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024 × 1024 pixels, demonstrating strong zero shot generalization using the CC12M dataset, which contains only 12 million images.","2023-10-23","2023-10-30 18:44:12","2024-02-08 19:14:37","2023-10-30 18:44:12","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.15111 [cs]","Comment: 28 pages, 18 figures","C:\Users\isido\Zotero\storage\JEIEWZT4\Gu e.a. - 2023 - Matryoshka Diffusion Models.pdf","","machine learning; diffusion model","","","","","","","","","","","","","","","","","","","","arXiv:2310.15111","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XZD382KF","webpage","2023","","Accelerate Data Science in Python with RAPIDS | NVIDIA On-Demand","NVIDIA","","","","https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51281/","Learn how to port Python data science and data engineering projects to the open-source, GPU-accelerated RAPIDS suite with minimal code changes","2023-10-31","2023-10-31 18:41:14","2024-08-12 14:42:58","2023-10-31 18:41:14","","","","","","","","","","","","","","en-us","","","","","","","","","C:\Users\isido\Zotero\storage\Y7LGV3UQ\gtcspring23-s51281.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJSYV6PW","videoRecording","2022","Enthought","GPU Accelerated Graph Analysis in Python using cuGraph- Brad Rees | SciPy 2022","","","","","https://www.youtube.com/watch?v=F_HnOcvGJqc","Graph analysis is used in a wide range of applications, from computational social science (social network analysis) to fraud detection and marketing. Just like within Machine Learning (ML), being able to load and analyze data quickly is the key to getting to a solution faster. Additionally, like ML, there is a lot of data prep, which we call graph ETL, that needs to be done. Join us for a talk on using RAPIDS and cuGraph to accelerate the full end-to-end graph analysis pipeline. We will dive into a COVID-19 social network example to illustrate the performance gains of using GPUs.","2022-08-01","2023-10-31 19:23:48","2024-02-09 19:47:16","2023-10-31 19:23:48","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","28:14","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNVKEEMF","journalArticle","2016","Davis, Timothy A.; Rajamanickam, Sivasankaran; Sid-Lakhdar, Wissam M.","A survey of direct methods for sparse linear systems","Acta Numerica","","0962-4929, 1474-0508","10.1017/S0962492916000076","https://www.cambridge.org/core/product/identifier/S0962492916000076/type/journal_article","Wilkinson defined a sparse matrix as one with enough zeros that it pays to take advantage of them.               1               This informal yet practical definition captures the essence of the goal of direct methods for solving sparse matrix problems. They exploit the sparsity of a matrix to solve problems economically: much faster and using far less memory than if all the entries of a matrix were stored and took part in explicit computations. These methods form the backbone of a wide range of problems in computational science. A glimpse of the breadth of applications relying on sparse solvers can be seen in the origins of matrices in published matrix benchmark collections (Duff and Reid 1979               a               , Duff, Grimes and Lewis 1989               a               , Davis and Hu 2011). The goal of this survey article is to impart a working knowledge of the underlying theory and practice of sparse direct methods for solving linear systems and least-squares problems, and to provide an overview of the algorithms, data structures, and software available to solve these problems, so that the reader can both understand the methods and know how best to use them.","2016-05-01","2023-11-02 12:27:34","2024-02-08 19:14:46","2023-11-02 12:27:34","383-566","","","25","","Acta Numerica","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\CUI34HBT\Davis e.a. - 2016 - A survey of direct methods for sparse linear syste.pdf","","linear systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJJCK7UF","book","2001","Godsil, Chris; Royle, Gordon","Algebraic Graph Theory","","978-0-387-95220-8 978-1-4613-0163-9","","","http://link.springer.com/10.1007/978-1-4613-0163-9","","2001","2023-11-02 12:55:30","2023-11-02 12:55:30","2023-11-02 12:55:30","","","","207","","","","Graduate Texts in Mathematics","","","","Springer New York","New York, NY","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4613-0163-9","","C:\Users\isido\Zotero\storage\69T4ZBBV\Godsil en Royle - 2001 - Algebraic Graph Theory.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSK24BT5","preprint","2023","Singh, Mukul; Cambronero, José; Gulwani, Sumit; Le, Vu; Negreanu, Carina; Verbruggen, Gust","CodeFusion: A Pre-trained Diffusion Model for Code Generation","","","","","http://arxiv.org/abs/2310.17680","Imagine a developer who can only change their last line of code—how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CODEFUSION, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CODEFUSION on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CODEFUSION (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M–175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy, due to its better balance in diversity versus quality.","2023-11-01","2023-11-06 08:10:49","2024-02-08 19:14:51","2023-11-06 08:10:49","","","","","","","CodeFusion","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.17680 [cs]","Comment: Contains inappropriately sourced conjecture of OpenAI's ChatGPT parameter count from www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat, a citation which was omitted. The authors do not have direct knowledge or verification of this information, and relied solely on this article, which may lead to public confusion","C:\Users\isido\Zotero\storage\HU9XSEJH\Singh e.a. - 2023 - CodeFusion A Pre-trained Diffusion Model for Code.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.17680","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4VVZEM7","preprint","2023","Gong, Shansan; Li, Mukai; Feng, Jiangtao; Wu, Zhiyong; Kong, Lingpeng","DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models","","","","","http://arxiv.org/abs/2210.08933","Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \url{https://github.com/Shark-NLP/DiffuSeq}","2023-02-14","2023-11-06 14:06:25","2024-02-08 19:15:01","2023-11-06 14:06:25","","","","","","","DiffuSeq","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.08933 [cs]","Comment: ICLR 2023 camera ready","C:\Users\isido\Zotero\storage\HX4GQFZW\Gong e.a. - 2023 - DiffuSeq Sequence to Sequence Text Generation wit.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2210.08933","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5KGGLPE","journalArticle","2019","W. L. Wan, Justin; David R. Cheriton School of Computer Science, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada","Multigrid method for pricing European options under the CGMY process","AIMS Mathematics","","2473-6988","10.3934/math.2019.6.1745","http://www.aimspress.com/article/10.3934/math.2019.6.1745","We propose a fast multigrid method for solving the discrete partial integro-diﬀerential equations (PIDEs) arising from pricing European options when the underlying asset is driven by an inﬁnite activity Le´vy process. We consider the CGMY model whose kernel singularity gets worse when the parameter Y approaches two. Due to the integral term, the discretization matrix is dense. In order to obtain an eﬃcient multigrid method, we apply a ﬁxed point iteration as a smoother for multigrid. In each smoothing step, we only need to solve a sparse matrix corresponding to the diﬀerential operator and compute a matrix-vector product involving the integral operator by a fast Fourier transform (FFT). We prove that the ﬁxed point iteration smoother is eﬀective reducing the high frequency components. Moreover, we also prove a two-grid convergence of the multigrid method by a local mode analysis. We demonstrate the eﬀectiveness of the multigrid method by solving the option pricing equation under the CGMY model with ﬁnite and inﬁnite variation processes.","2019","2023-11-12 13:33:56","2024-02-08 19:15:15","2023-11-12 13:33:56","1745-1767","","6","4","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\A4XZZRPP\W. L. Wan en David R. Cheriton School of Computer Science, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada - 2019 - Multigrid method for pricing European options unde.pdf","","PDE; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JLTDLEW","journalArticle","2022","Radford, Alec; Kim, Jong Wook; Xu, Tao; Brockman, Greg; McLeavey, Christine; Sutskever, Ilya","Robust Speech Recognition via Large-Scale Weak Supervision","","","","","","We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.","2022-12-06","2023-11-13 11:52:32","2024-08-12 14:36:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\EAMIHCL5\Radford e.a. - Robust Speech Recognition via Large-Scale Weak Sup.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LASL6KCI","preprint","2023","Di Palo, Norman; Byravan, Arunkumar; Hasenclever, Leonard; Wulfmeier, Markus; Heess, Nicolas; Riedmiller, Martin","Towards A Unified Agent with Foundation Models","","","","","http://arxiv.org/abs/2307.09668","Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.","2023-07-18","2023-11-19 16:35:50","2024-02-08 19:15:30","2023-11-19 16:35:50","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2307.09668 [cs]","","C:\Users\isido\Zotero\storage\X9Q29QL5\Di Palo e.a. - 2023 - Towards A Unified Agent with Foundation Models.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2307.09668","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JSE43DXA","preprint","2023","Zhu, Xizhou; Chen, Yuntao; Tian, Hao; Tao, Chenxin; Su, Weijie; Yang, Chenyu; Huang, Gao; Li, Bin; Lu, Lewei; Wang, Xiaogang; Qiao, Yu; Zhang, Zhaoxiang; Dai, Jifeng","Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory","","","","","http://arxiv.org/abs/2305.17144","The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular ""ObtainDiamond"" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the ""ObtainDiamond"" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the ""ObtainDiamond"" task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/ OpenGVLab/GITM.","2023-06-01","2023-11-19 17:02:53","2024-02-08 19:15:47","2023-11-19 17:02:53","","","","","","","Ghost in the Minecraft","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2305.17144 [cs]","","C:\Users\isido\Zotero\storage\VCTGVDDY\Zhu e.a. - 2023 - Ghost in the Minecraft Generally Capable Agents f.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2305.17144","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8AZZRAN2","preprint","2023","Zheng, Sipeng; Liu, Jiazheng; Feng, Yicheng; Lu, Zongqing","Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds","","","","","http://arxiv.org/abs/2310.13255","Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to “a blindfolded text-based game.” Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model designed to address this limitation. Steve-Eye integrates the LLM with a visual encoder which enables it to process visual-text inputs and generate multimodal feedback. In addition, we use a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, empowering our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out extensive experiments from a wide range of perspectives to validate our model’s capability to strategically act and plan. Codes and datasets will be released.","2023-10-19","2023-11-19 17:21:17","2024-02-08 19:15:53","2023-11-19 17:21:17","","","","","","","Steve-Eye","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.13255 [cs]","Comment: 19 pages, 14 figures","C:\Users\isido\Zotero\storage\G2PRNW5F\Zheng e.a. - 2023 - Steve-Eye Equipping LLM-based Embodied Agents wit.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2310.13255","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"56CB96GI","preprint","2023","Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Santos, Marco Dos; McAleer, Stephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean","Llemma: An Open Language Model For Mathematics","","","","","http://arxiv.org/abs/2310.10631","We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.","2023-10-16","2023-11-20 13:40:54","2024-02-08 19:17:49","2023-11-20 13:40:54","","","","","","","Llemma","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.10631 [cs]","","C:\Users\isido\Zotero\storage\NGAYW5U9\Azerbayev e.a. - 2023 - Llemma An Open Language Model For Mathematics.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2310.10631","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFBQ57YT","videoRecording","2023","The Julia Programming Language","Scientific Machine Learning through Symbolic Numerics | JuliaCon 2023 Keynote Chris Rackauckas","","","","","https://www.youtube.com/watch?v=tynmTkpdAME","Dr. Rackauckas is a Research Affiliate and Co-PI of the Julia Lab at the Massachusetts Institute of Technology, VP of Modeling and Simulation at JuliaHub and Creator / Lead Developer of JuliaSim. He's also the Director of Scientific Research at Pumas-AI and Creator / Lead Developer of Pumas, and Lead Developer of the SciML Open Source Software Organization. Dr. Rackauckas's research and software is focused on Scientific Machine Learning (SciML): the integration of domain models with artificial intelligence techniques like machine learning. By utilizing the structured scientific (differential equation) models together with the unstructured data-driven models of machine learning, our simulators can be accelerated, our science can better approximate the true systems, all while enjoying the robustness and explainability of mechanistic dynamical models. Abstract: The combination of scientific models into deep learning structures, commonly referred to as scientific machine learning (SciML), has made great strides in the last few years in incorporating models such as ODEs and PDEs into deep learning through differentiable simulation. Such SciML methods have been gaining steam due to accelerating the development of high-fidelity models for improving industrial simulation and design. However, many of the methods from the machine learning world lack the robustness required for scaling to industrial tasks. What needs to change about SciML in order to allow for methods which can guarantee accuracy and quantify uncertainty? In this talk we will go through the numerics of the robustness in building and training SciML models. Numerical robustness of algorithms for handling neural networks with stiff dynamics, continuous machine learning methods with certifiably globally-optimal training, alternative loss functions to mitigating local minima, integration of Bayesian estimation with model discovery, and tools for validating the correctness of surrogate models will be discussed to demonstrate a next generation of SciML methods for industrial use. In particular, it will be shown how symbolic-numerics is integrating the compiler into the modeling process as a method to improve numerical robustness, blurring the lines between computer science and numerical analysis. Demonstrations of these methods in applications such as two-phase flow HVAC systems, modeling of sensors in Formula One cars, and lithium-ion battery packs will be used to showcase the improved robustness of these approaches over standard (scientific) machine learning.","2023-08-29","2023-11-21 10:25:37","2024-02-09 19:47:10","2023-11-21 10:25:37","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","43:14","","","","","","","","","","","","","","","","","","","","","","","","",""
"IUZNAE7E","videoRecording","2023","The Julia Programming Language","The Special Math of Translating Theory to Software in Differential Eqs | Chris Rackauckas | ASE60","","","","","https://www.youtube.com/watch?v=s_t6dIKjUUc","Numerical analysis describes a pristine mathematical theory about optimal numerical algorithms under assumptions which do not necessarily hold in practice. For example, in theory good ODE approximations have fast convergence as the step size approaches zero. But, in practice, a good differential equation solver takes as big steps as possible. How must one change their mathematical reasoning to match the software world? This talk will describe the nitty gritty reasoning that comes into play when building mathematical software. We will describe how one of the most important aspects of optimizing a Runge-Kutta method happens to be alternative floating point power approximations, and how ""suboptimal methods"" can be more optimal by taking advantage of certain assembly instructions in modern hardware. This demonstrates how the creation of mathematical software is a discipline unto itself.","2023-09-12","2023-11-21 13:59:57","2024-02-09 19:47:05","2023-11-21 13:59:57","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","31:35","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKSHXW94","videoRecording","2023","The Julia Programming Language","Julia for Scientific HPC: Opportunities and Challenges | Michael Schlottke-Lakemper | FerriteCon","","","","","https://www.youtube.com/watch?v=-D6C9W0acVs","The Julia programming language aims to provide a modern approach to scientific computing and was designed with high-performance computing (HPC) in mind. As a consequence, a growing number of research groups are considering Julia as their language of choice for creating the next generation of computational fluid dynamics or structure mechanics codes. In this talk, we will thus discuss benefits of using Julia for such classical HPC workloads, and examine where challenges remain. For some practical insights, we will take a closer look at Trixi.jl, a numerical simulation framework for hyperbolic conservation laws that has been shown to scale efficiently to more than 50,000 MPI ranks, and share some of the lessons learned during its development.","2023-10-26","2023-11-21 14:37:17","2024-02-09 19:47:00","2023-11-21 14:37:17","","","","","","","Julia for Scientific HPC","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","56:02","","","","","","","","","","","","","","","","","","","","","","","","",""
"GIYUKC2R","videoRecording","2023","The Julia Programming Language","Learning JuMP by example | James D Foster | JuliaCon 2023","","","","","https://www.youtube.com/watch?v=rIan_XbYyaM","Get an up-to-date overview of the modelling capabilities of JuMP through a number of worked examples. I'll cover the types of optimization problems you can solve effectively, give lots of unsolicited advice and briefly look at some extensions. Get an up-to-date overview of the modelling capabilities of JuMP through a number of worked examples. I'll cover the types of optimization problems you can solve effectively, give lots of unsolicited advice and briefly look at some extensions.","2023-09-11","2023-11-21 16:15:37","2024-02-09 19:46:57","2023-11-21 16:15:37","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","23:18","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEWPZWJB","videoRecording","2023","The Julia Programming Language","StochasticAD.jl: Differentiating discrete randomness | Gaurav Arya | JuliaCon 2023","","","","","https://www.youtube.com/watch?v=_irOyB1ODvM","Automatic differentiation (AD) is great: use gradients to optimize, sample faster, or just for fun! But what about coin flips? Agent-based models? Nope, these aren’t differentiable... or are they? StochasticAD.jl is an open-source research package for AD of stochastic programs, implementing AD algorithms for handling programs that can contain discrete randomness. StochasticAD.jl is an open-source research package for automatic differentiation (AD) of stochastic programs. The particular focus is on implementing AD algorithms for handling programs that can contain discrete randomness. But what does this even mean? Derivatives are all about how functions are affected by a tiny change ε in their input. For example, take the function sin(x). Perturb x by ε, and the output changes by approximately cos(x) * ε: tiny change in, tiny change out. And the coefficient cos(x)? That's the derivative! But what happens if your function is discrete and random? For example, take a Bernoulli variable, with probability p of being 1 and probability 1-p of being 0. If we perturb p by ε, the output of the Bernoulli variable cannot change by a tiny amount. But in the probabilistic world, there is another way to change by a tiny amount on average: jump by a large amount, with tiny probability. StochasticAD.jl generalizes the well-known concept of dual numbers by including a third component to describe large perturbations with infinitesimal probability. The resulting object is called a stochastic triple, and StochasticAD.jl develops the algorithms to propagate this triple through user-written code involving discrete randomness. Ultimately, the result is a provably unbiased estimate of the derivative of your program, even if it contains discrete randomness! In this talk, we will discuss the workings of StochasticAD.jl, including the underlying theory and the technical implementation challenges.","2023-08-29","2023-11-21 17:40:00","2024-02-09 19:46:51","2023-11-21 17:40:00","","","","","","","StochasticAD.jl","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","32:58","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZB3GBV2D","preprint","2023","Arya, Gaurav; Schauer, Moritz; Schäfer, Frank; Rackauckas, Chris","Automatic Differentiation of Programs with Discrete Randomness","","","","","http://arxiv.org/abs/2210.08572","Automatic differentiation (AD), a technique for constructing new programs which compute the derivative of an original program, has become ubiquitous throughout scientiﬁc computing and deep learning due to the improved performance afforded by gradient-based optimization. However, AD systems have been restricted to the subset of programs that have a continuous dependence on parameters. Programs that have discrete stochastic behaviors governed by distribution parameters, such as ﬂipping a coin with probability p of being heads, pose a challenge to these systems because the connection between the result (heads vs tails) and the parameters (p) is fundamentally discrete. In this paper we develop a new reparameterizationbased methodology that allows for generating programs whose expectation is the derivative of the expectation of the original program. We showcase how this method gives an unbiased and low-variance estimator which is as automated as traditional AD mechanisms. We demonstrate unbiased forward-mode AD of discrete-time Markov chains, agent-based models such as Conway’s Game of Life, and unbiased reverse-mode AD of a particle ﬁlter. Our code package is available at https://github.com/gaurav-arya/StochasticAD.jl.","2023-01-09","2023-11-21 17:42:32","2024-02-08 19:18:29","2023-11-21 17:42:32","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.08572 [cs, math]","Comment: In Proceedings of NeurIPS 2022","C:\Users\isido\Zotero\storage\9XX9UD9N\2210.08572.pdf","","machine learning; autodiff","","","","","","","","","","","","","","","","","","","","arXiv:2210.08572","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UCPKE7E6","preprint","2023","Arya, Gaurav; Seyer, Ruben; Schäfer, Frank; Chandra, Kartik; Lew, Alexander K.; Huot, Mathieu; Mansinghka, Vikash K.; Ragan-Kelley, Jonathan; Rackauckas, Christopher; Schauer, Moritz","Differentiating Metropolis-Hastings to Optimize Intractable Densities","","","","","http://arxiv.org/abs/2306.07961","We develop an algorithm for automatic differentiation of Metropolis-Hastings samplers, allowing us to differentiate through probabilistic inference, even if the model has discrete components within it. Our approach fuses recent advances in stochastic automatic differentiation with traditional Markov chain coupling schemes, providing an unbiased and low-variance gradient estimator. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.","2023-06-30","2023-11-21 17:45:09","2024-02-08 19:18:26","2023-11-21 17:45:09","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2306.07961 [cs, stat]","Comment: 6 pages, 6 figures; accepted at Differentiable Almost Everything Workshop of ICML 2023","C:\Users\isido\Zotero\storage\UMXNAUPP\Arya e.a. - 2023 - Differentiating Metropolis-Hastings to Optimize In.pdf","","autodiff; MCMC","","","","","","","","","","","","","","","","","","","","arXiv:2306.07961","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKA2GXRQ","webpage","2023","","Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications - MIT Parallel Computing and Scientific Machine Learning (SciML)","","","","","https://book.sciml.ai/","","2023-11-21","2023-11-21 17:58:19","2024-08-12 14:54:59","2023-11-21 17:58:19","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\6HSJU68Z\book.sciml.ai.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LMY5XN5F","preprint","2022","Blackman, David; Vigna, Sebastiano","Scrambled Linear Pseudorandom Number Generators","","","","","http://arxiv.org/abs/1805.01407","$\mathbf F_2$-linear pseudorandom number generators are very popular due to their high speed, to the ease with which generators with a sizable state space can be created, and to their provable theoretical properties. However, they suffer from linear artifacts that show as failures in linearity-related statistical tests such as the binary-rank and the linear-complexity test. In this paper, we give two new contributions. First, we introduce two new $\mathbf F_2$-linear transformations that have been handcrafted to have good statistical properties and at the same time to be programmable very efficiently on superscalar processors, or even directly in hardware. Then, we describe some scramblers, that is, nonlinear functions applied to the state array that reduce or delete the linear artifacts, and propose combinations of linear transformations and scramblers that give extremely fast pseudorandom number generators of high quality. A novelty in our approach is that we use ideas from the theory of filtered linear-feedback shift registers to prove some properties of our scramblers, rather than relying purely on heuristics. In the end, we provide simple, extremely fast generators that use a few hundred bits of memory, have provable properties, and pass strong statistical tests.","2022-03-28","2023-11-21 18:32:13","2024-02-08 19:18:57","2023-11-21 18:32:13","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1805.01407 [cs]","","C:\Users\isido\Zotero\storage\78AKGEN8\Blackman en Vigna - 2022 - Scrambled Linear Pseudorandom Number Generators.pdf","","random number generator","","","","","","","","","","","","","","","","","","","","arXiv:1805.01407","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNBV73TP","preprint","2022","Dao, Tri; Fu, Daniel Y.; Ermon, Stefano; Rudra, Atri; Ré, Christopher","FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness","","","","","http://arxiv.org/abs/2205.14135","Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IOaware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on GPT-2 (seq. length 1K), and 2.4× speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).","2022-06-23","2023-11-22 11:41:15","2024-02-08 19:19:04","2023-11-22 11:41:15","","","","","","","FlashAttention","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2205.14135 [cs]","","C:\Users\isido\Zotero\storage\FTXR26L7\Dao e.a. - 2022 - FlashAttention Fast and Memory-Efficient Exact At.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2205.14135","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87WXADCB","journalArticle","2017","Rackauckas, Christopher; Nie, Qing","DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia","Journal of Open Research Software","","2049-9647","10.5334/jors.151","https://openresearchsoftware.metajnl.com/article/10.5334/jors.151/","DifferentialEquations.jl is a package for solving differential equations in Julia. It covers discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations), ordinary differential equations, stochastic differential equations, algebraic differential equations, delay differential equations, hybrid differential equations, jump diffusions, and (stochastic) partial differential equations. Through extensive use of multiple dispatch, metaprogramming, plot recipes, foreign function interfaces (FFI), and call-overloading, DifferentialEquations.jl offers a unified user interface to solve and analyze various forms of differential equations while not sacrificing features or performance. Many modern features are integrated into the solvers, such as allowing arbitrary user-defined number systems for high-precision and arithmetic with physical units, built-in multithreading and parallelism, and symbolic calculation of Jacobians. Integrated into the package is an algorithm testing and benchmarking suite to both ensure accuracy and serve as an easy way for researchers to develop and distribute their own methods. Together, these features build a highly extendable suite which is feature-rich and highly performant.","2017-05-25","2023-11-26 20:04:06","2024-02-09 19:41:25","2023-11-26 20:04:06","15","","1","5","","JORS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\TZDD4FJG\Rackauckas en Nie - 2017 - DifferentialEquations.jl – A Performant and Featur.pdf","","ODE; julia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WWFLGVGT","preprint","2012","Gürkan, M. Atakan","Fifth Order Runge-Kutta-Nystr\""om Methods with Complex Coefficients","","","","","http://arxiv.org/abs/1203.3279","We present ﬁfth order Runge-Kutta-Nystr¨om methods, where we allow the timestep coeﬃcients to assume complex values. Among the methods with complex timesteps, we focus on the ones with the coeﬃcients that have positive real parts. This property makes them suitable for problems where a negative coeﬃcient is not acceptable. In addition, the leading order terms in the error expansion of these methods are purely imaginary, eﬀectively increasing the order of the methods by one for real variables.","2012-03-15","2023-11-28 08:15:54","2024-02-08 19:19:37","2023-11-28 08:15:54","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1203.3279 [astro-ph, physics:math-ph]","Comment: 11 pages, 3 figures. Submitted to SIAM Journal on Scientific Computing","C:\Users\isido\Zotero\storage\KV9AZB8P\Gürkan - 2012 - Fifth Order Runge-Kutta-Nystrom Methods with Com.pdf","","ODE","","","","","","","","","","","","","","","","","","","","arXiv:1203.3279","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNAF2DSR","preprint","2023","Bou-Rabee, Nawaf; Kleppe, Tore Selland","Randomized Runge-Kutta-Nystr\""om","","","","","http://arxiv.org/abs/2310.07399","We present 5/2- and 7/2-order L2-accurate randomized Runge-Kutta-Nyström methods to approximate the Hamiltonian flow underlying various non-reversible Markov chain Monte Carlo chains including unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin chains. Quantitative 5/2-order L2-accuracy upper bounds are provided under gradient and Hessian Lipschitz assumptions on the potential energy function. The superior complexity of the corresponding Markov chains is numerically demonstrated for a selection of ‘well-behaved’, high-dimensional target distributions.","2023-10-11","2023-11-28 08:22:24","2024-02-06 17:53:40","2023-11-28 08:22:24","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.07399 [cs, math, stat]","","C:\Users\isido\Zotero\storage\N9I82TBE\Bou-Rabee en Kleppe - 2023 - Randomized Runge-Kutta-Nystrom.pdf","","monte carlo; ODE","","","","","","","","","","","","","","","","","","","","arXiv:2310.07399","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGQ34ST4","journalArticle","2010","Blanes, Sergio; Casas, Fernando; Murua, Ander","Splitting methods with complex coefficients","SeMA Journal","","1575-9822, 2254-3902","10.1007/BF03322541","http://link.springer.com/10.1007/BF03322541","Splitting methods for the numerical integration of diﬀerential equations of order greater than two involve necessarily negative coeﬃcients. This order barrier can be overcome by considering complex coeﬃcients with positive real part. In this work we review the composition technique used to construct methods of this class, propose new sixthorder integrators and analyze their main features on a pair of numerical examples, in particular how the errors are propagated along the evolution.","2010-03","2023-11-28 08:48:04","2024-02-08 19:20:21","2023-11-28 08:48:04","47-60","","1","50","","SeMA","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\3ZH5KLLC\Blanes e.a. - 2010 - Splitting methods with complex coefficients.pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBJRGXS2","blogPost","2017","Rackauckas, Christopher","A Comparison Between Differential Equation Solver Suites In MATLAB, R, Julia, Python, C, Mathematica, Maple, and Fortran","Stochastic Lifestyle","","","","https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/","Many times a scientist is choosing a programming language or a software for a specific purpose. For the field of scientific computing, the methods for solving differential equations are one of the important areas. What I would like to do is take the time to compare and contrast between the most popular offerings. This is a good way to reflect upon what’s available and find out where there is room for improvement. I hope that by giving you the details for how each suite was put together (and the “why”, as gathered from software publications) you can come to your own conclusion as to which suites are right for you. (Full disclosure, I am the lead developer of DifferentialEquations.jl. You will see at the end that DifferentialEquations.jl does offer pretty much everything from the other suite combined, but that’s no accident: ... READ MORE","2017-09-26","2023-11-28 16:28:44","2024-02-08 19:20:36","2023-11-28 16:28:44","","","","","","","","","","","","","","en-US","","","","","","","","","C:\Users\isido\Zotero\storage\67MU3GEC\comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran.html","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IUJ2BZSF","book","2003","Butcher, J. C.","Numerical methods for ordinary differential equations","","978-0-471-96758-3","","","","The study of numerical methods for solving ordinary differential equations is constantly developing and regenerating, and this third edition of a popular classic volume, written by one of the world’s leading experts in the field, presents an account of the subject which reflects both its historical and well-established place in computational science and its vital role as a cornerstone of modern applied mathematics. In addition to serving as a broad and comprehensive study of numerical methods for initial value problems, this book contains a special emphasis on Runge-Kutta methods by the mathematician who transformed the subject into its modern form dating from his classic 1963 and 1972 papers. A second feature is general linear methods which have now matured and grown from being a framework for a unified theory of a wide range of diverse numerical schemes to a source of new and practical algorithms in their own right. As the founder of general linear method research, John Butcher has been a leading contributor to its development; his special role is reflected in the text. The book is written in the lucid style characteristic of the author, and combines enlightening explanations with rigorous and precise analysis. In addition to these anticipated features, the book breaks new ground by including the latest results on the highly efficient G-symplectic methods which compete strongly with the well-known symplectic Runge-Kutta methods for long-term integration of conservative mechanical systems. This third edition of Numerical Methods for Ordinary Differential Equations will serve as a key text for senior undergraduate and graduate courses in numerical analysis, and is an essential resource for research workers in applied mathematics, physics and engineering.","2003","2023-11-29 14:44:18","2024-02-09 19:41:00","","","425","","","","","","","","","","J. Wiley","Chichester, West Sussex, England ; Hoboken, NJ","","","","","","Library of Congress ISBN","QA372 .B94 2003","","","C:\Users\isido\Zotero\storage\IDCXRKFM\Butcher - 2003 - Numerical methods for ordinary differential equati.pdf","","ODE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"66PL8I6H","journalArticle","2016","Juba, Derek; Keyrouz, Walid; Mascagni, Michael; Brady, Mary","Acceleration and Parallelization of ZENO/Walk-on-Spheres","Procedia Computer Science","","18770509","10.1016/j.procs.2016.05.319","https://linkinghub.elsevier.com/retrieve/pii/S1877050916306834","This paper describes our on-going work to accelerate ZENO, a software tool based on Monte Carlo methods (MCMs), used for computing material properties at nanoscale. ZENO employs three main algorithms: (1) Walk on Spheres (WoS), (2) interior sampling, and (3) surface sampling. We have accelerated the ﬁrst two algorithms. For the sake of brevity, the paper will discuss our work on the ﬁrst one only as it is the most commonly used and the acceleration techniques were similar in both cases.","2016","2023-11-30 20:30:49","2024-02-08 19:21:11","2023-11-30 20:30:49","269-278","","","80","","Procedia Computer Science","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\F5MN4NJB\main.pdf","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGSQIR9X","journalArticle","1991","Heinrich, Stefan; Kern, Jörg-Detlef","Parallel information-based complexity","Journal of Complexity","","0885064X","10.1016/0885-064X(91)90024-R","https://linkinghub.elsevier.com/retrieve/pii/0885064X9190024R","We develop the theory of information-based complexity from a parallel point of view. For a model of computation with p processors, each being capable of arithmetic operations and decisions, we analyze the complexity of function approximation, numerical integration, and solution of Fredholm integral equations. We obtain tight bounds on the complexity, considered as a function of three variables simultaneously: the number of processors, the required precision, and (in the case of approximation and integral equations) the number of points, in which the approximate solution is to be determined.","1991-12","2023-12-01 17:24:55","2024-02-09 19:39:02","2023-12-01 17:24:55","339-370","","4","7","","Journal of Complexity","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BQWEEXZK\Heinrich en Kern - 1991 - Parallel information-based complexity.pdf","","IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXDZZX4D","preprint","2011","Mahoney, Michael W.","Randomized algorithms for matrices and data","","","","","http://arxiv.org/abs/1104.5557","Randomized algorithms for very large matrix problems have received a great deal of attention in recent years. Much of this work was motivated by problems in large-scale data analysis, largely since matrices are popular structures with which to model data drawn from a wide range of application domains, and this work was performed by individuals from many diﬀerent research communities. While the most obvious beneﬁt of randomization is that it can lead to faster algorithms, either in worst-case asymptotic theory and/or numerical implementation, there are numerous other beneﬁts that are at least as important. For example, the use of randomization can lead to simpler algorithms that are easier to analyze or reason about when applied in counterintuitive settings; it can lead to algorithms with more interpretable output, which is of interest in applications where analyst time rather than just computational time is of interest; it can lead implicitly to regularization and more robust output; and randomized algorithms can often be organized to exploit modern computational architectures better than classical numerical methods.","2011-11-15","2023-12-03 17:07:49","2024-02-08 19:21:56","2023-12-03 17:07:49","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1104.5557 [cs]","Comment: Review article, 54 pages, 198 references. Version appearing as a monograph in Now Publishers' ""Foundations and Trends in Machine Learning"" series","C:\Users\isido\Zotero\storage\2B5IWQQF\Mahoney - 2011 - Randomized algorithms for matrices and data.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","arXiv:1104.5557","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4V8SR7R","journalArticle","2023","Gao, Jianhua; Ji, Weixing; Chang, Fangli; Han, Shiyu; Wei, Bingxin; Liu, Zeming; Wang, Yizhuo","A Systematic Survey of General Sparse Matrix-Matrix Multiplication","ACM Computing Surveys","","0360-0300, 1557-7341","10.1145/3571157","http://arxiv.org/abs/2002.11273","General Sparse Matrix-Matrix Multiplication (SpGEMM) has attracted much attention from researchers in graph analyzing, scientific computing, and deep learning. Many optimization techniques have been developed for different applications and computing architectures over the past decades. The objective of this article is to provide a structured and comprehensive overview of the researches on SpGEMM. Existing researches have been grouped into different categories based on target architectures and design choices. Covered topics include typical applications, compression formats, general formulations, key problems and techniques, architectureoriented optimizations, and programming models. The rationales of different algorithms are analyzed and summarized. This survey sufficiently reveals the latest progress of SpGEMM research to 2021. Moreover, a thorough performance comparison of existing implementations is presented. Based on our findings, we highlight future research directions, which encourage better design and implementations in later studies. CCS Concepts: • Mathematics of computing → Computations on matrices; • Computing methodologies → Shared memory algorithms; Vector / streaming algorithms.","2023-12-31","2023-12-03 17:11:23","2024-02-08 19:21:50","2023-12-03 17:11:23","1-36","","12","55","","ACM Comput. Surv.","","","","","","","","en","","","","","arXiv.org","","arXiv:2002.11273 [cs]","Comment: 37 pages, 20 figures, 11 tables, 1 algorithm","C:\Users\isido\Zotero\storage\EUZNSDYJ\Gao e.a. - 2023 - A Systematic Survey of General Sparse Matrix-Matri.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DJKPU4MP","preprint","2023","Liang, Tianyu; Murray, Riley; Buluç, Aydın; Demmel, James","Fast multiplication of random dense matrices with fixed sparse matrices","","","","","http://arxiv.org/abs/2310.15419","This work focuses on accelerating the multiplication of a dense random matrix with a (fixed) sparse matrix, which is frequently used in sketching algorithms. We develop a novel scheme that takes advantage of blocking and recomputation (onthe-fly random number generation) to accelerate this operation. The techniques we propose decrease memory movement, thereby increasing the algorithm’s parallel scalability in shared memory architectures. On the Intel Frontera architecture, our algorithm can achieve 2x speedups over libraries such as Eigen and Intel MKL on some examples. In addition, with 32 threads, we can obtain a parallel efficiency of up to approximately 45%.","2023-10-23","2023-12-03 18:17:55","2024-02-08 19:22:02","2023-12-03 18:17:55","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.15419 [cs]","","C:\Users\isido\Zotero\storage\2WPKEM56\Liang e.a. - 2023 - Fast multiplication of random dense matrices with .pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","arXiv:2310.15419","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2ETZ6WS","journalArticle","2020","Ling, Shuyang","Approximate Matrix Multiplication via Random Sampling","","","","","","Matrix multiplication is a fundamental operation. Therefore, providing a fast method to do matrix multiplication is extremely important. However, in many applications, matrix can be very huge, i.e., m, n, and p are all very large. Then even matrix multiplication is challenging: expensive computation and limited memory issue. There is a recent surge of research on obtaining an approximate matrix multiplication: sacrifice precision/accuracy for cheaper computational costs.","2020-04-22","2023-12-03 18:21:00","2024-08-12 14:31:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\HAAK8VL2\Ling - Approximate Matrix Multiplication via Random Sampl.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IP7DZIYR","preprint","2018","Freksen, Casper Benjamin; Kamma, Lior; Larsen, Kasper Green","Fully Understanding the Hashing Trick","","","","","http://arxiv.org/abs/1805.08539","Feature hashing, also known as the hashing trick, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix A : Rn → Rm (where m n) in order to reduce the dimension of the data from n to m while approximately preserving the Euclidean norm. Every column of A contains exactly one non-zero entry, equals to either −1 or 1.","2018-05-22","2023-12-03 18:44:31","2024-02-06 17:53:15","2023-12-03 18:44:31","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1805.08539 [cs, math, stat]","","C:\Users\isido\Zotero\storage\QSBZDJM5\Freksen e.a. - 2018 - Fully Understanding the Hashing Trick.pdf","","machine learning; data structures and algorithms","","","","","","","","","","","","","","","","","","","","arXiv:1805.08539","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FGCL7XRG","videoRecording","2019","PyData","Gianluca Campanella: The unreasonable effectiveness of feature hashing | PyData London 2019","","","","","https://www.youtube.com/watch?v=XelrzDtEnPY","Feature hashing is a computationally efficient pre-processing technique for sparse, high-dimensional features. Starting from an overview of the method, this talk covers: the impact of hash functions, hash size and collisions on statistical performance; three libraries for model training with feature hashing; hash reversibility and its implications for model interpretability. www.pydata.org PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.  PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.  00:00 Welcome! 1:00 - Introduction 1:53 - Background- Supervised ML 3:29 - Categorical Features 4:00 - One-hot encoding 4:36 - Bag of words 5:20 - High dimensional feature space 9:20 - Feature Hashing 11:26 - Hash function 12:24 - Feature Hashing in Python 13:27 - Hashing of Unicode Strings 15:05 - Projection 15:06 - Collisions 17:58 - Sign Functions 20:26 - Feature Hashing- Example 26:40 - Feature Hashing- Use Case 30:10 - Library Support 31:27 - Recap 32:20 - Q&A S/o to https://github.com/Cyborg-vs-Droids for the video timestamps! Want to help add timestamps to our YouTube videos to help with discoverability? Find out more here: https://github.com/numfocus/YouTubeVi...","2019-07-18","2023-12-03 19:21:31","2024-02-09 19:46:46","2023-12-03 19:21:30","","","","","","","Gianluca Campanella","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","35:29","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9SDRYGB","computerProgram","2023","Vincent-Sivadon","WalkOnSpheres.jl","","","","","https://github.com/Vincent-Sivadon/WalkOnSpheres.jl","Walk On Spheres simulation tools","2023-09-18","2023-12-04 17:04:48","2023-12-04 17:04:48","2023-12-04 17:04:48","","","","","","","","","","","","","","","","","","","GitHub","","original-date: 2023-08-23T08:11:05Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Julia","","","","","","","","",""
"JSU4DT69","journalArticle","2022","Croci, Matteo; Fasi, Massimiliano; Higham, Nicholas J.; Mary, Theo; Mikaitis, Mantas","Stochastic rounding: implementation, error analysis and applications","Royal Society Open Science","","2054-5703","10.1098/rsos.211631","https://royalsocietypublishing.org/doi/10.1098/rsos.211631","Stochastic rounding (SR) randomly maps a real number               x               to one of the two nearest values in a finite precision number system. The probability of choosing either of these two numbers is 1 minus their relative distance to               x               . This rounding mode was first proposed for use in computer arithmetic in the 1950s and it is currently experiencing a resurgence of interest. If used to compute the inner product of two vectors of length               n               in floating-point arithmetic, it yields an error bound with constant                                                                        n                                      u                                               with high probability, where               u               is the unit round-off. This is not necessarily the case for round to nearest (RN), for which the worst-case error bound has constant               nu               . A particular attraction of SR is that, unlike RN, it is immune to the phenomenon of stagnation, whereby a sequence of tiny updates to a relatively large quantity is lost. We survey SR by discussing its mathematical properties and probabilistic error analysis, its implementation, and its use in applications, with a focus on machine learning and the numerical solution of differential equations.","2022-03","2023-12-05 19:40:35","2024-02-08 19:22:31","2023-12-05 19:40:35","211631","","3","9","","R. Soc. open sci.","Stochastic rounding","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\P22GVN5U\Croci e.a. - 2022 - Stochastic rounding implementation, error analysi.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7D5G3H9G","preprint","2023","Arar, El-Mehdi El; Sohier, Devan; Castro, Pablo de Oliveira; Petit, Eric","Stochastic rounding variance and probabilistic bounds: A new approach","","","","","http://arxiv.org/abs/2207.10321","Stochastic rounding (SR) offers an alternative to the deterministic IEEE-754 floating-point rounding modes. In some applications such as PDEs, ODEs, and neural networks, SR empirically improves the numerical behavior and convergence to accurate solutions while the theoretical background remains partial. Recent works by Ipsen, Zhou, Higham, and Mary have computed SR probabilistic error bounds for basic linear algebra ke√rnels. For example, the inner product SR probabilistic bound of the forward error is proportional to nu instead of nu for the default rounding mode. To compute the bounds, these works show that the errors accumulated in computation form a martingale.","2023-06-05","2023-12-05 18:21:08","2024-02-08 19:22:26","2023-12-05 18:21:08","","","","","","","Stochastic rounding variance and probabilistic bounds","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2207.10321 [cs, math]","","C:\Users\isido\Zotero\storage\EL9TYWQH\Arar e.a. - 2023 - Stochastic rounding variance and probabilistic bou.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","arXiv:2207.10321","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKFY26AU","journalArticle","2016","Karney, Charles F. F.","Sampling exactly from the normal distribution","ACM Transactions on Mathematical Software","","0098-3500, 1557-7295","10.1145/2710016","http://arxiv.org/abs/1303.6257","An algorithm for sampling exactly from the normal distribution is given. The algorithm reads some number of uniformly distributed random digits in a given base and generates an initial portion of the representation of a normal deviate in the same base. Thereafter, uniform random digits are copied directly into the representation of the normal deviate. Thus, in contrast to existing methods, it is possible to generate normal deviates exactly rounded to any precision with a mean cost that scales linearly in the precision. The method performs no extended precision arithmetic, calls no transcendental functions, and, indeed, uses no floating point arithmetic whatsoever; it uses only simple integer operations. It can easily be adapted to sample exactly from the discrete normal distribution whose parameters are rational numbers.","2016-03","2023-12-09 09:00:38","2024-02-08 19:22:41","2023-12-09 09:00:38","1-14","","1","42","","ACM Trans. Math. Softw.","","","","","","","","en","","","","","arXiv.org","","arXiv:1303.6257 [physics]","Comment: LaTeX, 8 pages, 1 figure. Revision includes algorithm for sampling discrete normal distribution. An implementation of the algorithms is available at http://exrandom.sf.net","C:\Users\isido\Zotero\storage\95NYA6C7\Karney - 2016 - Sampling exactly from the normal distribution.pdf","","random number generator","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSKV83UA","forumPost","2013","AXH","Answer to ""Black-Scholes PDE to heat equation, nonconstant coefficients""","Mathematics Stack Exchange","","","","https://math.stackexchange.com/a/418080","","2013-06-12","2023-12-11 16:50:16","2023-12-11 16:50:16","2023-12-11 16:50:16","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\GJWK93S4\black-scholes-pde-to-heat-equation-nonconstant-coefficients.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GP9NGIP","preprint","2019","Acebron, Juan A.; Herrero, Jose R.; Monteiro, Jose","A highly parallel algorithm for computing the action of a matrix exponential on a vector based on a multilevel Monte Carlo method","","","","","http://arxiv.org/abs/1904.12754","A novel algorithm for computing the action of a matrix exponential over a vector is proposed. The algorithm is based on a multilevel Monte Carlo method, and the vector solution is computed probabilistically generating suitable random paths which evolve through the indices of the matrix according to a suitable probability law. The computational complexity is proved in this paper to be signiﬁcantly better than the classical Monte Carlo method, which allows the computation of much more accurate solutions. Furthermore, the positive features of the algorithm in terms of parallelism were exploited in practice to develop a highly scalable implementation capable of solving some test problems very eﬃciently using high performance supercomputers equipped with a large number of cores. For the speciﬁc case of shared memory architectures the performance of the algorithm was compared with the results obtained using an available Krylov-based algorithm, outperforming the latter in all benchmarks analyzed so far.","2019-07-04","2023-12-17 10:33:15","2024-02-08 19:23:30","2023-12-17 10:33:15","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1904.12754 [cs, math]","","C:\Users\isido\Zotero\storage\WMVA6WP3\Acebron e.a. - 2019 - A highly parallel algorithm for computing the acti.pdf","","monte carlo; exponential integrators; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1904.12754","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKTV2MS9","preprint","2023","Guidotti, Nicolas L.; Acebrón, Juan A.; Monteiro, José","A Fast Monte Carlo algorithm for evaluating matrix functions with application in complex networks","","","","","http://arxiv.org/abs/2308.01037","We propose a novel stochastic algorithm that randomly samples entire rows and columns of the matrix as a way to approximate an arbitrary matrix function. This contrasts with the “classical” Monte Carlo method which only works with one entry at a time, resulting in a significant better convergence rate than the “classical” approach. To assess the applicability of our method, we compute the subgraph centrality and total communicability of several large networks. In all benchmarks analyzed so far, the performance of our method was significantly superior to the competition, being able to scale up to 64 CPU cores with a remarkable efficiency.","2023-08-02","2023-12-17 10:34:15","2024-02-08 19:23:53","2023-12-17 10:34:15","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2308.01037 [cs]","Comment: Submitted to the Journal of Scientific Computing","C:\Users\isido\Zotero\storage\JSP222QB\Guidotti e.a. - 2023 - A Fast Monte Carlo algorithm for evaluating matrix.pdf","","monte carlo; exponential integrators","","","","","","","","","","","","","","","","","","","","arXiv:2308.01037","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KC2DZFFZ","journalArticle","2005","Acebrón, Juan A.; Busico, Maria Pia; Lanucara, Piero; Spigler, Renato","Domain Decomposition Solution of Elliptic Boundary-Value Problems via Monte Carlo and Quasi-Monte Carlo Methods","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/030600692","http://epubs.siam.org/doi/10.1137/030600692","Domain decomposition of two-dimensional domains on which boundary-value elliptic problems are formulated, is accomplished by probabilistic (Monte Carlo) as well as by quasi-Monte Carlo methods, generating only few interfacial values and interpolating on them. Continuous approximations for the trace of solution are thus obtained, to be used as boundary data for the sub-problems. The numerical treatment can then proceed by standard deterministic algorithms, separately in each of the so-obtained subdomains. Monte Carlo and quasi-Monte Carlo simulations may naturally exploit multiprocessor architectures, leading to parallel computing, as well as the ensuing domain decomposition does. The advantage such as scalability obtained increasing the number of processors is shown, both theoretically and experimentally, in a number of test examples, and the possibility of using clusters of computers (grid computing) emphasized.","2005-01","2023-12-17 12:43:55","2024-02-08 19:24:03","2023-12-17 12:43:55","440-457","","2","27","","SIAM J. Sci. Comput.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\ZYR8RJCB\Acebrón e.a. - 2005 - Domain Decomposition Solution of Elliptic Boundary.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YM3XTFQI","journalArticle","2023","Bernal, Francisco; Morón-Vidal, Jorge; Acebrón, Juan A.","A hybrid probabilistic domain decomposition algorithm suited for very large-scale elliptic PDEs","Computers & Mathematics with Applications","","08981221","10.1016/j.camwa.2023.07.004","https://linkinghub.elsevier.com/retrieve/pii/S0898122123002985","State of the art domain decomposition algorithms for large-scale boundary value problems (with M ≫ 1 degrees of freedom) suffer from bounded strong scalability because they involve the synchronisation and communication of workers inherent to iterative linear algebra. Here, we introduce PDDSparse, a different approach to scientific supercomputing which relies on a ”Feynman-Kac formula for domain decomposition”. Concretely, the interfacial values (only) are det√ermined by a stochastic, highly sparse linear system G(ω)u⃗ = ⃗b(ω) of size O( M), whose coefficients are constructed with Monte Carlo simulations—hence embarrassingly in parallel. In addition to a wider scope for strong scalability in the deep supercomputing regime, PDDSparse has built-in fault tolerance and is ideally suited for GPUs. A proof of concept example with up to 1536 cores is discussed in detail.","2023-09","2023-12-17 12:57:38","2023-12-17 12:57:38","2023-12-17 12:57:38","294-308","","","146","","Computers & Mathematics with Applications","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\GXGBR99E\Bernal e.a. - 2023 - A hybrid probabilistic domain decomposition algori.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MNU3D3C","book","2023","Yu, Wenjian; Mascagni, Michael","Monte Carlo Methods for Partial Differential Equations With Applications to Electronic Design Automation","","978-981-19324-9-6 978-981-19325-0-2","","","https://link.springer.com/10.1007/978-981-19-3250-2","The main purpose of this book is to present a systematic and algorithmic perspective on the Monte Carlo method for solving partial differential equations (PDEs), especially their fast algorithms and practical applications. We will focus mainly on elliptic PDEs, i.e., the Laplace and Poisson equations describing equilibrium states. Moreover, the applications that motivate us will mainly come from microelectronic integrated circuit. The Monte Carlo method in the Russian literature is often referred to (in translation) as “the method of statistical sampling.” This phrase has much more meaning than “the Monte Carlo method,” which was chosen due to the famous casino in Monte Carlo. Monte Carlo can be used to solve a variety of problems, and is the basis of the areas of stochastic ordinary differential equations (ODEs) and PDEs, and uncertainty quantification. In this book, we focus on the Monte Carlo method for solving partial differential and integral equations based on random walks. The random walk method essentially involves the Markov process. Historically it is most interesting to realize that there are certain application areas where Monte Carlo has become the dominant numerical method. One such area is in financial computations. Another is a topic in this book, the title of which is Monte Carlo Methods for Partial Differential Equations With Applications to Electronic Design Automation. Electronic Design Automation (EDA) is the process of designing microelectronic circuits. It is a key technology in an enormously important component of modern information technology. When one has a putative circuit design, before it can be processed for fabrication various validation computations are done to make sure that the design is not flawed. One such computation is based on electrostatics and insures that electronic crosstalk or even more catastrophically, electric arching does not make the device malfunction. The various physical components of the circuit, especially interconnect wires, have their mutual (coupling) capacitances which must be computed. This process is known as capacitance extraction or capacitance calculation, and is a common computation in electrostatics. Since the equations of electrostatics are the Laplace or Poisson equations, methods for the solution of elliptic PDEs are employed. For many years, deterministic techniques were employed in capacitance extraction. However, it was noticed that computing capacitance via vi Preface Monte Carlo methods was a very rapid technique. Eventually, the commercial codes for capacitance extraction began replacing deterministic with Monte Carlo methods. This book is meant to give the reader the background and state-of-the-art computational knowledge to understand and use Monte Carlo methods for capacitance calculation problems in particular, and more generally for the solution of elliptic PDEs in EDA. The work presented in this book are mostly from research projects undertaken by the Numbda group led by Wenjian Yu at the Department of Computer Science and Technology, Tsinghua University, China. All chapters except Chap. 3 are composed/edited by Wenjian Yu. Chapter 3 is from the collaborative work with Michael Mascagni of Florida State University and the National Institute of Standards and Technology (NIST). It is interesting to note that this chapter deals with a Monte Carlo methods for the telegrapher’s equation, which is a hyperbolic PDE and definitely not describing an equilibrium problem. Most content of this book is based on the original publications shared at https://numbda.cs.tsinghua.edu.cn/paper.html. We want to emphasize that the book is by no means intended to be comprehensive. The absence of coverage of related work should by no means diminish their value and contribution. Many academic or industrial experts have made significant contributions in the field, and the reader is encouraged to investigate their work. Publications that publish results in the area of Monte Carlo methods for PDEs include international journals on applied mathematics such as Journal of Computational Physics, SIAM Journal on Scientific Computing, and Monte Carlo Methods and Applications.","2023","2023-12-17 13:31:03","2024-02-09 19:36:32","2023-12-17 13:31:03","","","","","","","","","","","","Springer Nature Singapore","Singapore","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-981-19-3250-2","","C:\Users\isido\Zotero\storage\BLFXXP3R\Yu en Mascagni - 2023 - Monte Carlo Methods for Partial Differential Equat.pdf","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A9NPKAKX","journalArticle","2023","Romera-Paredes, Bernardino; Barekatain, Mohammadamin; Novikov, Alexander; Balog, Matej; Kumar, M. Pawan; Dupont, Emilien; Ruiz, Francisco J. R.; Ellenberg, Jordan S.; Wang, Pengming; Fawzi, Omar; Kohli, Pushmeet; Fawzi, Alhussein","Mathematical discoveries from program search with large language models","Nature","","0028-0836, 1476-4687","10.1038/s41586-023-06924-6","https://www.nature.com/articles/s41586-023-06924-6","Large Language Models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations) which can result in them making plausible but incorrect statements (Bang et al., 2023; Borji, 2023). This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pre-trained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best known results in important problems, pushing the boundary of existing LLM-based approaches (Lehman et al., 2022). Applying FunSearch to a central problem in extremal combinatorics — the cap set problem — we discover new constructions of large cap sets going beyond the best known ones, both in finite dimensional and asymptotic cases. This represents the first discoveries made for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve upon widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.","2023-12-14","2023-12-20 10:12:22","2024-02-08 19:25:02","2023-12-20 10:12:22","","","","","","Nature","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\VGEVMJG9\Romera-Paredes e.a. - 2023 - Mathematical discoveries from program search with .pdf","","machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ARC7FKGM","preprint","2020","Zhang, Bolong; Mascagni, Michael","Pass-Efficient Randomized LU Algorithms for Computing Low-Rank Matrix Approximation","","","","","http://arxiv.org/abs/2002.07138","Low-rank matrix approximation is extremely useful in the analysis of data that arises in scientific computing, engineering applications, and data science. However, as data sizes grow, traditional low-rank matrix approximation methods, such as SVD and CPQR, are either prohibitively expensive or cannot provide sufficiently accurate results. A solution is to use randomized low-rank matrix approximation methods such as randomized SVD , and randomized LU decomposition on extremely large data sets. In this paper, we focus on the randomized LU decomposition method. First, we employ a reorthogonalization procedure to perform the power iteration of the existing randomized LU algorithm to compensate for the rounding errors caused by the power method. Then we propose a novel randomized LU algorithm, called PowerLU, for the fixed low-rank approximation problem. PowerLU allows for an arbitrary number of passes of the input matrix, $v \geq 2$. Recall that the existing randomized LU decomposition only allows an even number of passes. We prove the theoretical relationship between PowerLU and the existing randomized LU. Numerical experiments show that our proposed PowerLU is generally faster than the existing randomized LU decomposition, while remaining accurate. We also propose a version of PowerLU, called PowerLU_FP, for the fixed precision low-rank matrix approximation problem. PowerLU_FP is based on an efficient blocked adaptive rank determination Algorithm 4.1 proposed in this paper. We present numerical experiments that show that PowerLU_FP can achieve almost the same accuracy and is faster than the randomized blocked QB algorithm by Martinsson and Voronin. We finally propose a single-pass algorithm based on LU factorization. Tests show that the accuracy of our single-pass algorithm is comparable with the existing single-pass algorithms.","2020-02-17","2023-12-26 09:05:33","2024-02-08 19:25:08","2023-12-26 09:05:33","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2002.07138 [cs, math]","Comment: 28 pages, 9 figures","C:\Users\isido\Zotero\storage\Y7ZMGZ6D\2002.html; C:\Users\isido\Zotero\storage\D9T56GWF\Zhang en Mascagni - 2020 - Pass-Efficient Randomized LU Algorithms for Comput.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","arXiv:2002.07138","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2F4YXFJV","webpage","2010","M. Bradley, Andrew","PDE-constrained optimization and the adjoint method","","","","","https://cs.stanford.edu/~ambrad/adjoint_tutorial.pdf","PDE-constrained optimization and the adjoint method for solving these and related problems appear in a wide range of application domains. Often the adjoint method is used in an application without explanation. The purpose of this tutorial is to explain the method in detail in a general setting that is kept as simple as possible. We use the following notation: the total derivative (gradient) is denoted dx (usually denoted d(·)/dx or ∇x); the partial derivative, ∂x (usually, ∂(·)/∂x); the differential, d. We also use the notation fx for both partial and total derivatives when we think the meaning is clear from context. Recall that a gradient is a row vector, and this convention induces sizing conventions for the other operators. We use only real numbers in this presentation.","2010-11-16","2023-12-29 16:33:47","2023-12-29 16:34:31","2023-12-29 16:32:01","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\Z5LL5UKL\adjoint_tutorial.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJ2LK9QX","journalArticle","2022","Misso, Zackary; Bitterli, Benedikt; Georgiev, Iliyan; Jarosz, Wojciech","Unbiased and consistent rendering using biased estimators","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3528223.3530160","https://dl.acm.org/doi/10.1145/3528223.3530160","We introduce a general framework for transforming biased estimators into unbiased and consistent estimators for the same quantity. We show how several existing unbiased and consistent estimation strategies in rendering are special cases of this framework, and are part of a broader debiasing principle. We provide a recipe for constructing estimators using our generalized framework and demonstrate its applicability by developing novel unbiased forms of transmittance estimation, photon mapping, and finite differences.","2022-07","2023-12-30 10:34:20","2024-02-08 19:25:37","2023-12-30 10:34:20","1-13","","4","41","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\4VXWM6II\Misso e.a. - 2022 - Unbiased and consistent rendering using biased est.pdf","","monte carlo; rendering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2D9EM7UK","journalArticle","2015","Pauli, Stefan; Gantner, Robert Nicholas; Arbenz, Peter; Adelmann, Andreas","Multilevel Monte Carlo for the Feynman–Kac formula for the Laplace equation","BIT Numerical Mathematics","","0006-3835, 1572-9125","10.1007/s10543-014-0543-8","http://link.springer.com/10.1007/s10543-014-0543-8","Since its formulation in the late 1940s, the Feynman-Kac formula has proven to be an eﬀective tool for both theoretical reformulations and practical simulations of diﬀerential equations. The link it establishes between such equations and stochastic processes can be exploited to develop Monte Carlo sampling methods that are eﬀective, especially in high dimensions. There exist many techniques of improving standard Monte Carlo sampling methods, a relatively new development being the so-called Multilevel Monte Carlo method. This paper investigates the applicability of multilevel ideas to the stochastic representation of partial diﬀerential equations by the FeynmanKac formula, using the Walk on Sphere algorithm to generate the required random paths. We focus on the Laplace equation, the simplest elliptic PDE, while mentioning some extension possibilities.","2015-12","2023-12-30 20:26:58","2024-02-08 19:25:46","2023-12-30 20:26:58","1125-1143","","4","55","","Bit Numer Math","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\YWEY84MK\Pauli e.a. - 2015 - Multilevel Monte Carlo for the Feynman–Kac formula.pdf","","monte carlo; PDE; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYZ5VMXF","journalArticle","2006","Petras, Knut; Ritter, Klaus","On the complexity of parabolic initial-value problems with variable drift","Journal of Complexity","","0885064X","10.1016/j.jco.2005.07.005","https://linkinghub.elsevier.com/retrieve/pii/S0885064X05000907","We study the intrinsic difﬁculty of solving linear parabolic initial-value problems numerically at a single point. We present a worst-case analysis for deterministic as well as for randomized (or Monte Carlo) algorithms, assuming that the drift coefﬁcients and the potential vary in given function spaces. We use fundamental solutions (parametrix method) for equations with unbounded coefﬁcients to relate the initialvalue problem to multivariate integration and weighted approximation problems. Hereby we derive lower and upper bounds for the minimal errors. The upper bounds are achieved by algorithms that use Smolyak formulas and, in the randomized case, variance reduction. We apply our general results to equations with coefﬁcients from Hölder classes, and here, in many cases, the upper and lower bounds almost coincide and our algorithms are almost optimal.","2006-02","2023-12-31 14:16:15","2024-02-08 19:26:25","2023-12-31 14:16:15","118-145","","1","22","","Journal of Complexity","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\A27C4A67\Petras en Ritter - 2006 - On the complexity of parabolic initial-value probl.pdf","","monte carlo; PDE; IBC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LQSVSRKH","journalArticle","2021","Løvbak, Emil; Samaey, Giovanni; Vandewalle, Stefan","A multilevel Monte Carlo method for asymptotic-preserving particle schemes in the diffusive limit","Numerische Mathematik","","0029-599X, 0945-3245","10.1007/s00211-021-01201-y","http://arxiv.org/abs/1907.04610","Kinetic equations model distributions of particles in position-velocity phase space. Often, one is interested in studying the long-time behavior of particles in high-collisional regimes in which an approximate (advection)-diffusion model holds. In this paper we consider the diffusive scaling. Classical particle-based techniques suffer from a strict time-step restriction in this limit, to maintain stability. Asymptotic-preserving schemes avoid this problem, but introduce an additional time discretization error, possibly resulting in an unacceptably large bias for larger time steps. Here, we present and analyze a multilevel Monte Carlo scheme that reduces this bias by combining estimates using a hierarchy of different time step sizes. We demonstrate how to correlate trajectories from this scheme, using different time steps. We also present a strategy for selecting the levels in the multilevel scheme. Our approach significantly reduces the computation required to perform accurate simulations of the considered kinetic equations, compared to classical Monte Carlo approaches.","2021-05","2024-01-02 19:32:32","2024-02-08 19:29:58","2024-01-02 19:32:32","141-186","","1","148","","Numer. Math.","","","","","","","","","","","","","arXiv.org","","arXiv:1907.04610 [cs, math]","Comment: 41 page article (incl. 6 pages of appendix), 11 figures, 8 tables, 3 pages of additional supplementary material, updated title in metadata. A final peer-reviewed version of this article has been published in Numerische Mathematik","C:\Users\isido\Zotero\storage\SNTF3YQ5\1907.html; C:\Users\isido\Zotero\storage\FXYW3JQH\Løvbak e.a. - 2021 - A multilevel Monte Carlo method for asymptotic-pre.pdf","","monte carlo; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSBWP2LE","document","2023","","The EIRENE Code User Manual","","","","","","The EIRENE neutral particle (incl. photons) transport code [kn:Reiter92a] is described. This code resorts to a combinatorial discretization of general 3 dimensional computational domains. It is a multi-species code solving simultaneously a system of time dependent (optional) or stationary (default) linear or non-linear kinetic transport equations of almost arbitrary complexity. A rather crude model for transport of ionized particles in an externally specified magnetic field lines is also included. EIRENE is coupled to external databases for atomic and molecular data and for surface reflection data, and it calls various user supplied routines, e.g. for exchange of data with other (fluid-) transport codes. The main goal of code development was to provide a tool to investigate neutral gas transport in magnetically confined plasmas. But, due to its flexibility, it also can be used to solve more general linear kinetic transport equations, by applying a stochastic rather than a numerical or analytical method of solution. In particular, options are retained to reduce the model equations to the theoretically important case of the one speed transport problem. Major applications of EIRENE are in connection with plasma fluid codes, in particular with the various versions of the B2 code [kn:Braams]. The semi-implicit iterative coupling method of B2-EIRENE [kn:Reiter91b], [kn:Reiter92b] and it’s implementation (code segment: EIRCOP) are also described. Both, in its stand alone form and also non-linearly integrated into plasma fluid transport solvers the EIRENE code has become a widely spread ”community code” since the nineties of the last century. A short overall summary of the code package, including some historical notes, is given in [kn:Reiter2004].","2023-04-28","2024-01-02 18:32:37","2024-08-12 14:59:02","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\759AQS5Q\eirene.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KM9UIR6D","preprint","2022","Wang, Guanyang; Wang, Tianze","Unbiased Multilevel Monte Carlo methods for intractable distributions: MLMC meets MCMC","","","","","http://arxiv.org/abs/2204.04808","Constructing unbiased estimators from Markov chain Monte Carlo (MCMC) outputs is a diﬃcult problem that has recently received a lot of attention in the statistics and machine learning communities. However, the current unbiased MCMC framework only works when the quantity of interest is an expectation, which excludes many practical applications. In this paper, we propose a general method for constructing unbiased estimators for functions of expectations and extend it to construct unbiased estimators for nested expectations. Our approach combines and generalizes the unbiased MCMC and Multilevel Monte Carlo (MLMC) methods. In contrast to traditional sequential methods, our estimator can be implemented on parallel processors. We show that our estimator has a ﬁnite variance and computational complexity and can achieve ε-accuracy within the optimal O(1/ε2) computational cost under mild conditions. Our numerical experiments conﬁrm our theoretical ﬁndings and demonstrate the beneﬁts of unbiased estimators in the massively parallel regime.","2022-12-23","2024-01-01 17:59:15","2024-02-08 19:29:32","2024-01-01 17:59:15","","","","","","","Unbiased Multilevel Monte Carlo methods for intractable distributions","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2204.04808 [cs, math, stat]","Comment: add additional experiments, rewrite part of the introduction, add more references","C:\Users\isido\Zotero\storage\Y6PKQ7XD\Wang en Wang - 2022 - Unbiased Multilevel Monte Carlo methods for intrac.pdf","","monte carlo; MCMC; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2204.04808","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJMNEAVP","conferencePaper","2015","Blanchet, Jose H.; Glynn, Peter W.","Unbiased Monte Carlo for optimization and functions of expectations via multi-level randomization","2015 Winter Simulation Conference (WSC)","978-1-4673-9743-8","","10.1109/WSC.2015.7408524","http://ieeexplore.ieee.org/document/7408524/","We present general principles for the design and analysis of unbiased Monte Carlo estimators for quantities such as α = g (E (X)), where E (X) denotes the expectation of a (possibly multidimensional) random variable X, and g (·) is a given deterministic function. Our estimators possess ﬁnite work-normalized variance under mild regularity conditions such as local twice differentiability of g (·) and suitable growth and ﬁnite-moment assumptions. We apply our estimator to various settings of interest, such as optimal value estimation in the context of Sample Average Approximations, and unbiased steady-state simulation of regenerative processes. Other applications include unbiased estimators for particle ﬁlters and conditional expectations.","2015-12","2024-01-01 17:19:56","2024-02-08 19:28:57","2024-01-01 17:19:56","3656-3667","","","","","","","","","","","IEEE","Huntington Beach, CA, USA","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\5AYWPZUE\Blanchet en Glynn - 2015 - Unbiased Monte Carlo for optimization and function.pdf","","monte carlo; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 Winter Simulation Conference (WSC)","","","","","","","","","","","","","","",""
"U9VBJYB9","conferencePaper","2015","Blanchet, Jose H.; Nan Chen; Glynn, Peter W.","Unbiased Monte Carlo computation of smooth functions of expectations via Taylor expansions","2015 Winter Simulation Conference (WSC)","978-1-4673-9743-8","","10.1109/WSC.2015.7408178","http://ieeexplore.ieee.org/document/7408178/","Many Monte Carlo computations involve computing quantities that can be expressed as g(EX), where g is nonlinear and smooth, and X is an easily simulatable random variable. The nonlinearity of g makes the conventional Monte Carlo estimator for such quantities biased. In this paper, we show how such quantities can be estimated without bias. However, our approach typically increases the variance. Thus, our approach is primarily of theoretical interest in the above setting. However, our method can also be applied to the computation of the inner expectation associated with Eg(EX|Z)), and in this setting, the application of this method can have a signiﬁcant positive effect on improving the rate of convergence relative to conventional “nested schemes” for carrying out such calculations.","2015-12","2024-01-01 17:18:24","2024-02-08 19:27:11","2024-01-01 17:18:24","360-367","","","","","","","","","","","IEEE","Huntington Beach, CA, USA","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\PZ2NTYAP\Blanchet e.a. - 2015 - Unbiased Monte Carlo computation of smooth functio.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 Winter Simulation Conference (WSC)","","","","","","","","","","","","","","",""
"TTLE6HZI","journalArticle","2001","Heinrich, Stefan","Multilevel Monte Carlo Methods","","","","","","We study Monte Carlo approximations to high dimensional parameter dependent integrals. We survey the multilevel variance reduction technique introduced by the author in [4] and present extensions and new developments of it. The tools needed for the convergence analysis of vector-valued Monte Carlo methods are discussed, as well. Applications to stochastic solution of integral equations are given for the case where an approximation of the full solution function or a family of functionals of the solution depending on a parameter of a certain dimension is sought.","2001-12-20","2024-01-01 17:07:11","2024-08-12 14:28:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\FDCDNUW2\Heinrich - Multilevel Monte Carlo Methods.pdf","","MLMC; monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3T8ZKFH4","preprint","2023","Giles, Michael B.","MLMC techniques for discontinuous functions","","","","","http://arxiv.org/abs/2301.02882","The Multilevel Monte Carlo (MLMC) approach usually works well when estimating the expected value of a quantity which is a Lipschitz function of intermediate quantities, but if it is a discontinuous function it can lead to a much slower decay in the variance of the MLMC correction. This article reviews the literature on techniques which can be used to overcome this challenge in a variety of different contexts, and discusses recent developments using either a branching diffusion or adaptive sampling.","2023-09-04","2024-01-01 14:34:53","2024-02-08 19:26:36","2024-01-01 14:34:53","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.02882 [cs, math]","Comment: 15 pages, 6 figures, submitted to proceedings of MCQMC22 conference; Comment: 15 pages, 6 figures, submitted to proceedings of MCQMC22 conference","C:\Users\isido\Zotero\storage\9DGTDKK7\2301.html; C:\Users\isido\Zotero\storage\D9LQEPKM\Giles - 2023 - MLMC techniques for discontinuous functions.pdf; C:\Users\isido\Zotero\storage\C624SYW7\Giles - 2023 - MLMC techniques for discontinuous functions.pdf","","monte carlo; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2301.02882","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJFXVZ93","journalArticle","2010","Doucet, Arnaud; Johansen, Adam M.; Tadić, Vladislav B.","On solving integral equations using Markov chain Monte Carlo methods","Applied Mathematics and Computation","","00963003","10.1016/j.amc.2010.03.138","https://linkinghub.elsevier.com/retrieve/pii/S0096300310004042","In this paper, we propose an original approach to the solution of Fredholm equations of the second kind. We interpret the standard Von Neumann expansion of the solution as an expectation with respect to a probability distribution deﬁned on a union of subspaces of variable dimension. Based on this representation, it is possible to use trans-dimensional Markov chain Monte Carlo (MCMC) methods such as Reversible Jump MCMC to approximate the solution numerically. This can be an attractive alternative to standard Sequential Importance Sampling (SIS) methods routinely used in this context. To motivate our approach, we sketch an application to value function estimation for a Markov decision process. Two computational examples are also provided.","2010-07","2024-01-15 12:34:33","2024-02-08 19:34:23","2024-01-15 12:34:33","2869-2880","","10","216","","Applied Mathematics and Computation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\AXC8U5SB\Doucet e.a. - 2010 - On solving integral equations using Markov chain M.pdf","","monte carlo; integral equations; MCMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJABBT3W","document","2011","","[Chapman & Hall_CRC Handbooks of Modern Statistical Methods] Steve Brooks, Andrew Gelman, Galin Jones, Xiao-Li Meng - Handbook of Markov Chain Monte Carlo (2011, Chapman and Hall_CRC) - libgen.li.pdf","","","","","","Over the past 20 years or so, Markov Chain Monte Carlo (MCMC) methods have revolutionized statistical computing. They have impacted the practice of Bayesian statistics profoundly by allowing intricate models to be posited and used in an astonishing array of disciplines as diverse as fisheries science and economics. Of course, Bayesians are not the only ones to benefit from using MCMC, and there continues to be increasing use of MCMC in other statistical settings. The practical importance of MCMC has also sparked expansive and deep investigation into fundamental Markov chain theory. As the use of MCMC methods mature, we see deeper theoretical questions addressed, more complex applications undertaken and their use spreading to new fields of study. It seemed to us that it was a good time to try to collect an overview of MCMC research and its applications. This book is intended to be a reference (not a text) for a broad audience and to be of use both to developers and users of MCMC methodology. There is enough introductory material in the book to help graduate students as well as researchers new to MCMC who wish to become acquainted with the basic theory, algorithms and applications. The book should also be of particular interest to those involved in the development or application of new and advanced MCMC methods. Given the diversity of disciplines that use MCMC, it seemed prudent to have many of the chapters devoted to detailed examples and case studies of realistic scientific problems. Those wanting to see current practice in MCMC will find a wealth of material to choose from here. Roughly speaking, we can divide the book into two parts. The first part encompasses 12 chapters concerning MCMC foundations, methodology and algorithms. The second part consists of 12 chapters which consider the use of MCMC in practical applications. Within the first part, the authors take such a wide variety of approaches that it seems pointless to try to classify the chapters into subgroups. For example, some chapters attempt to appeal to a broad audience by taking a tutorial approach while other chapters, even if introductory, are either more specialized or present more advanced material. Yet others present original research. In the second part, the focus shifts to applications. Here again, we see a variety of topics, but there are two basic approaches taken by the authors of these chapters. The first is to provide an overview of an application area with the goal of identifying best MCMC practice in the area through extended examples. The second approach is to provide detailed case studies of a given problem while clearly identifying the statistical and MCMC-related issues encountered in the application. When we were planning this book, we quickly realized that no single source can give a truly comprehensive overview of cutting-edge MCMC research and applications—there is just too much of it and its development is moving too fast. Instead, the editorial goal was to obtain contributions of high quality that may stand the test of time. To this end, all of the contributions (including those written by members of the editorial panel) were submitted to a rigorous peer review process and many underwent several revisions. Some contributions, even after revisions, were deemed unacceptable for publication here, and we certainly welcome constructive feedback on the chapters that did survive our editorial process. We thank all the authors for their efforts and patience in this process, and we ask for understanding from those whose contributions are not included in this book. We believe the breadth and depth of the contributions to this book, including some diverse opinions expressed, imply a continuously bright and dynamic future for MCMC research. We hope","2011-05-24","2024-01-11 17:41:17","2024-08-12 14:47:08","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\RSDCEF76\[Chapman & Hall_CRC Handbooks of Modern Statistical Methods] Steve Brooks, Andrew Gelman, Galin Jones, Xiao-Li Meng - Handbook of Markov Chain Monte Carlo (2011, Chapman and Hall_CRC) - libgen.li.pdf","","♥♥; MCMC; monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LB4N55HA","bookSection","2012","Hoel, Håkon; Von Schwerin, Erik; Szepessy, Anders; Tempone, Raúl","Adaptive Multilevel Monte Carlo Simulation","Numerical Analysis of Multiscale Computations","978-3-642-21942-9 978-3-642-21943-6","","","https://link.springer.com/10.1007/978-3-642-21943-6_10","Abstract This work generalizes a multilevel forward Euler Monte Carlo method introduced in Michael B. Giles. (Michael Giles. Oper. Res. 56(3):607–617, 2008.) for the approximation of expected values depending on the solution to an Itˆ o stochastic differential equation. The work (Michael Giles. Oper. Res. 56(3):607617, 2008.) proposed and analyzed a forward Euler multilevel Monte Carlo method based on a hierarchy of uniform time discretizations and control variates to reduce the computational effort required by a standard, single level, Forward Euler Monte Carlo method. This work introduces an adaptive hierarchy of non uniform time discretizations, generated by an adaptive algorithm introduced in (Anna Dzougoutov et al. Ra ́ ul Tempone. Adaptive Monte Carlo algorithms for stopped diffusion. In Multiscale methods in science and engineering, volume 44 of Lect. Notes Comput. Sci. Eng., pages 59–88. Springer, Berlin, 2005; Kyoung-Sook Moon et al. Stoch. Anal. Appl. 23(3):511–558, 2005; Kyoung-Sook Moon et al. An adaptive algorithm for ordinary, stochastic and partial differential equations. In Recent advances in adaptive computation, volume 383 of Contemp. Math., pages 325–343. Amer. Math. Soc., Providence, RI, 2005.). This form of the adaptive algorithm generates stochastic, path dependent, time steps and is based on a posteriori error expansions first developed in (Anders Szepessy et al. Comm. Pure Appl. Math. 54(10):11691214, 2001). Our numerical results for a stopped diffusion problem, exhibit savings in the computational cost to achieve an accuracy of O .TOL/, from O  TOL 3  using a single level version of the adaptive algorithm to O   TOL 1 log .TOL/ 2  .","2012","2024-01-10 11:34:51","2024-02-09 19:34:36","2024-01-10 11:34:51","217-234","","","82","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computational Science and Engineering DOI: 10.1007/978-3-642-21943-6_10","","C:\Users\isido\Zotero\storage\PQ3YU9A5\Hoel e.a. - 2012 - Adaptive Multilevel Monte Carlo Simulation.pdf","","MLMC","","Engquist, Björn; Runborg, Olof; Tsai, Yen-Hsi R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8BKWZLKX","journalArticle","2017","Hammouda, Chiheb Ben; Moraes, Alvaro; Tempone, Raul","Multilevel Hybrid Split-Step Implicit Tau-Leap","Numerical Algorithms","","1017-1398, 1572-9265","10.1007/s11075-016-0158-z","http://arxiv.org/abs/1512.00721","In biochemically reactive systems with small copy numbers of one or more reactant molecules, the dynamics is dominated by stochastic eﬀects. To approximate those systems, discrete state-space and stochastic simulation approaches have been shown to be more relevant than continuous state-space and deterministic ones. In systems characterized by having simultaneously fast and slow timescales, existing discrete space-state stochastic path simulation methods, such as the stochastic simulation algorithm (SSA) and the explicit tau-leap (explicit-TL ) method, can be very slow. Implicit approximations have been developed to improve numerical stability and provide eﬃcient simulation algorithms for those systems. Here, we propose an eﬃcient Multilevel Monte Carlo (MLMC) method in the spirit of the work by Anderson and Higham (2012). This method uses split-step implicit tau-leap (SSI-TL ) at levels where the explicit-TL method is not applicable due to numerical stability issues. We present numerical examples that illustrate the performance of the proposed method.","2017-02","2024-01-10 11:31:59","2024-02-08 19:33:54","2024-01-10 11:31:59","527-560","","2","74","","Numer Algor","","","","","","","","en","","","","","arXiv.org","","arXiv:1512.00721 [math]","","C:\Users\isido\Zotero\storage\PRUJ552S\Hammouda e.a. - 2017 - Multilevel Hybrid Split-Step Implicit Tau-Leap.pdf","","MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RD3ELMXJ","preprint","2023","Bayer, Christian; Hammouda, Chiheb Ben; Tempone, Raul","Multilevel Monte Carlo with Numerical Smoothing for Robust and Efficient Computation of Probabilities and Densities","","","","","http://arxiv.org/abs/2003.05708","The multilevel Monte Carlo (MLMC) method is highly efficient for estimating expectations of a functional of a solution to a stochastic differential equation (SDE). However, MLMC estimators may be unstable and have a poor (noncanonical) complexity in the case of low regularity of the functional. To overcome this issue, we extend our previously introduced idea of numerical smoothing in (Quantitative Finance, 23(2), 209-227, 2023), in the context of deterministic quadrature methods to the MLMC setting. The numerical smoothing technique is based on root-finding methods combined with one-dimensional numerical integration with respect to a single well-chosen variable. This study is motivated by the computation of probabilities of events, pricing options with a discontinuous payoff, and density estimation problems for dynamics where the discretization of the underlying stochastic processes is necessary. The analysis and numerical experiments reveal that the numerical smoothing significantly improves the strong convergence, and consequently, the complexity and robustness (by making the kurtosis at deep levels bounded) of the MLMC method. In particular, we show that numerical smoothing enables recovering the MLMC complexities obtained for Lipschitz functionals due to the optimal variance decay rate when using the Euler--Maruyama scheme. For the Milstein scheme, numerical smoothing recovers the canonical MLMC complexity even for the nonsmooth integrand mentioned above. Finally, our approach efficiently estimates univariate and multivariate density functions.","2023-10-02","2024-01-10 11:27:54","2024-02-08 19:33:44","2024-01-10 11:27:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2003.05708 [q-fin]","","C:\Users\isido\Zotero\storage\4M74RGA3\2003.html; C:\Users\isido\Zotero\storage\946V6738\Bayer e.a. - 2023 - Multilevel Monte Carlo with Numerical Smoothing fo.pdf","","MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2003.05708","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4P5J93RH","journalArticle","2022","Pillai, Shyam Mohan Subbiah","Importance Sampling Methods for McKean-Vlasov type Stochastic Differential Equations","","","","","","In this thesis, we are interested in Monte Carlo methods for estimating probabilities of rare events associated with solutions to the McKean-Vlasov stochastic differential equation (MV-SDE), whose drift and diffusion coefficients depend on the law of the solution itself. The MV-SDE is approximated using the stochastic interacting P-particle system, which is a set of P coupled d-dimensional stochastic differential equations. Importance sampling is used to reduce high variance in Monte Carlo estimators of rare event probabilities. Optimal change of measure is methodically derived from variance minimization, yielding a P × d-dimensional partial differential control equation which is cumbersome to solve. This problem is circumvented by using a decoupling approach, resulting in a lower dimensional control PDE. The decoupling approach necessitates the use of a double loop Monte Carlo estimator. In this context, we formulate an adaptive double loop Monte Carlo method for estimating rare event probabilities. Significant variance reduction is observed and the computational runtime for estimating rare event probabilities up to a given relative tolerance, TOL, is reduced by multiple orders, when compared to standard Monte Carlo estimators. We also formulate a novel multilevel double loop Monte Carlo (MLDLMC) method combined with importance sampling, to estimate rare events in the MV-SDE context. This reduces the order of optimal work complexity from O   TOL−4  to O   TOL−3  . Our numerical experiments are carried out on the Kuramoto model from statistical physics, which models a system of coupled oscillators.","2022-05-10","2024-01-10 11:16:41","2024-08-12 14:35:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\79CA8HQE\Pillai - Importance Sampling Methods for McKean-Vlasov type.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RST5S33W","journalArticle","2023","Ben Hammouda, Chiheb; Ben Rached, Nadhir; Tempone, Raúl; Wiechert, Sophia","Learning-based importance sampling via stochastic optimal control for stochastic reaction networks","Statistics and Computing","","0960-3174, 1573-1375","10.1007/s11222-023-10222-6","https://link.springer.com/10.1007/s11222-023-10222-6","We explore efﬁcient estimation of statistical quantities, particularly rare event probabilities, for stochastic reaction networks. Consequently, we propose an importance sampling (IS) approach to improve the Monte Carlo (MC) estimator efﬁciency based on an approximate tau-leap scheme. The crucial step in the IS framework is choosing an appropriate change of probability measure to achieve substantial variance reduction. This task is typically challenging and often requires insights into the underlying problem. Therefore, we propose an automated approach to obtain a highly efﬁcient path-dependent measure change based on an original connection in the stochastic reaction network context between ﬁnding optimal IS parameters within a class of probability measures and a stochastic optimal control formulation. Optimal IS parameters are obtained by solving a variance minimization problem. First, we derive an associated dynamic programming equation. Analytically solving this backward equation is challenging, hence we propose an approximate dynamic programming formulation to ﬁnd near-optimal control parameters. To mitigate the curse of dimensionality, we propose a learning-based method to approximate the value function using a neural network, where the parameters are determined via a stochastic optimization algorithm. Our analysis and numerical experiments verify that the proposed learning-based IS approach substantially reduces MC estimator variance, resulting in a lower computational complexity in the rare event regime, compared with standard tau-leap MC estimators.","2023-06","2024-01-10 11:12:05","2024-01-10 11:12:05","2024-01-10 11:12:05","58","","3","33","","Stat Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\2MZQVHQY\Ben Hammouda e.a. - 2023 - Learning-based importance sampling via stochastic .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDRS3HPY","journalArticle","2022","Sabelfeld, Karl","A new randomized vector algorithm for iterative solution of large linear systems","Applied Mathematics Letters","","08939659","10.1016/j.aml.2021.107830","https://linkinghub.elsevier.com/retrieve/pii/S089396592100450X","In this letter we suggest a new randomized scalable stochastic-matrix-based algorithms for calculation of large matrix iterations. Special focus is on positive or irreducible nonnegative class of matrices. As an application, a new randomized vector algorithm for iterative solution of large linear systems of algebraic equations governed by M-matrices is constructed. The idea behind these stochastic methods is in a randomized vector representation of matrix iterations. The iterations are performed by sampling random columns only, thus avoiding not only matrix but also matrix vector multiplications. As a result, the algorithm is highly efficient for solving linear equations of high dimension, its computational cost depends linearly on the dimension. Extensions of the suggested randomized iteration method to general classes of matrices are also discussed.","2022-04","2024-01-10 07:07:40","2024-02-08 19:33:21","2024-01-10 07:07:40","107830","","","126","","Applied Mathematics Letters","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\J7HBC9HE\Sabelfeld - 2022 - A new randomized vector algorithm for iterative so.pdf","","monte carlo; linear systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNFMENE3","journalArticle","2020","Sabelfeld, Karl","Mesh-free stochastic algorithms for systems of drift–diffusion–reaction equations and anisotropic diffusion flux calculations","Probabilistic Engineering Mechanics","","02668920","10.1016/j.probengmech.2020.103065","https://linkinghub.elsevier.com/retrieve/pii/S0266892020300527","We suggest in this paper two new random walk based stochastic algorithms for solving high-dimensional PDEs for domains with complicated geometrical structure. The first one, a Random Walk on Spheres (RWS) algorithm is developed for solving systems of coupled drift–diffusion–reaction equations where the random walk is living both on randomly sampled spheres and inside the relevant balls. The second method suggested solves transient anisotropic diffusion equations, where the random walk is carried out on random rectangular parallelepipeds inside the domain. The two methods are mesh-free both in space and time, and are well applied to solve high-dimensional problems with complicated domains. The algorithms are based on tracking the trajectories of the diffusing particles exactly in accordance with the probabilistic distributions derived from the explicit representation of the relevant Green functions for a sphere and a parallelepiped. They can be conveniently used not only for the solutions, but also for a direct calculation of fluxes to any part of the boundary without calculating the whole solution in the domain. Applications to exciton transport in semiconductors and related cathodoluminescence imaging of a set of randomly distributed threading dislocations are presented.","2020-07","2024-01-10 06:50:16","2024-02-09 14:46:45","2024-01-10 06:50:16","103065","","","61","","Probabilistic Engineering Mechanics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\I8H6KY4Y\Sabelfeld - 2020 - Mesh-free stochastic algorithms for systems of dri.pdf","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVWUVLE8","journalArticle","2017","Simonov, Nikolai A.","Walk-on-spheres algorithm for solving third boundary value problem","Applied Mathematics Letters","","08939659","10.1016/j.aml.2016.09.008","https://linkinghub.elsevier.com/retrieve/pii/S0893965916302816","We propose a new Monte Carlo method for solving the third boundary value problem for the Laplace equation. The algorithm provides a possibility to construct an unbiased estimator for the solution.","2017-02","2024-01-10 05:58:27","2024-02-09 14:46:49","2024-01-10 05:58:27","156-161","","","64","","Applied Mathematics Letters","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\PMNUFJUN\Simonov - 2017 - Walk-on-spheres algorithm for solving third bounda.pdf","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMESBP9B","webpage","2023","","Oskar Lappi / Eiron · GitLab","GitLab","","","","https://version.helsinki.fi/lapposka/eiron","","2023-12-22","2024-01-09 17:55:43","2024-02-09 19:35:27","2024-01-09 17:55:43","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\RH9NYJAS\eiron.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNDEBY8D","journalArticle","1992","Van Dijk, Nico M.","Approximate uniformization for continuous-time Markov chains with an application to performability analysis","Stochastic Processes and their Applications","","03044149","10.1016/0304-4149(92)90018-L","https://linkinghub.elsevier.com/retrieve/pii/030441499290018L","","1992-03","2024-01-09 16:52:47","2024-01-09 16:52:47","2024-01-09 16:52:47","339-357","","2","40","","Stochastic Processes and their Applications","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\EVIUZ95B\Van Dijk - 1992 - Approximate uniformization for continuous-time Mar.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NNWS7J4","preprint","2021","Rupp, Kevin; Schill, Rudolf; Süskind, Jonas; Georg, Peter; Klever, Maren; Lösch, Andreas; Grasedyck, Lars; Wettig, Tilo; Spang, Rainer","Differentiated uniformization: A new method for inferring Markov chains on combinatorial state spaces including stochastic epidemic models","","","","","http://arxiv.org/abs/2112.10971","Motivation: We consider continuous-time Markov chains that describe the stochastic evolution of a dynamical system by a transition-rate matrix $Q$ which depends on a parameter $\theta$. Computing the probability distribution over states at time $t$ requires the matrix exponential $\exp(tQ)$, and inferring $\theta$ from data requires its derivative $\partial\exp\!(tQ)/\partial\theta$. Both are challenging to compute when the state space and hence the size of $Q$ is huge. This can happen when the state space consists of all combinations of the values of several interacting discrete variables. Often it is even impossible to store $Q$. However, when $Q$ can be written as a sum of tensor products, computing $\exp(tQ)$ becomes feasible by the uniformization method, which does not require explicit storage of $Q$. Results: Here we provide an analogous algorithm for computing $\partial\exp\!(tQ)/\partial\theta$, the differentiated uniformization method. We demonstrate our algorithm for the stochastic SIR model of epidemic spread, for which we show that $Q$ can be written as a sum of tensor products. We estimate monthly infection and recovery rates during the first wave of the COVID-19 pandemic in Austria and quantify their uncertainty in a full Bayesian analysis. Availability: Implementation and data are available at https://github.com/spang-lab/TenSIR.","2021-12-20","2024-01-09 16:49:47","2024-02-06 17:52:44","2024-01-09 16:49:47","","","","","","","Differentiated uniformization","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2112.10971 [cs, q-bio, stat]","","C:\Users\isido\Zotero\storage\N6DVIR3W\2112.html; C:\Users\isido\Zotero\storage\QWM72BGD\Rupp e.a. - 2021 - Differentiated uniformization A new method for in.pdf","","machine learning","","","","","","","","","","","","","","","","","","","","arXiv:2112.10971","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUBA6YI8","journalArticle","2018","van Dijk, N.M.; van Brummelen, S.P.J.; Boucherie, R.J.","Uniformization: Basics, extensions and applications","Performance Evaluation","","01665316","10.1016/j.peva.2017.09.008","https://linkinghub.elsevier.com/retrieve/pii/S0166531617301451","","2018-02","2024-01-09 16:46:02","2024-01-09 16:46:02","2024-01-09 16:46:02","8-32","","","118","","Performance Evaluation","Uniformization","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\AGJ5Q7YN\van Dijk e.a. - 2018 - Uniformization Basics, extensions and application.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EW98IZR6","preprint","2024","Bossuyt, Ignace; Vandewalle, Stefan; Samaey, Giovanni","Micro-macro Parareal, from ODEs to SDEs and back again","","","","","http://arxiv.org/abs/2401.01798","In this paper, we are concerned with the micro-macro Parareal algorithm for the simulation of initial-value problems. In this algorithm, a coarse (fast) solver is applied sequentially over the time domain, and a fine (time-consuming) solver is applied as a corrector in parallel over smaller chunks of the time interval. Moreover, the coarse solver acts on a reduced state variable, which is coupled to the fine state variable through appropriate coupling operators. We first provide a contribution to the convergence analysis of the micro-macro Parareal method for multiscale linear ordinary differential equations (ODEs). Then, we extend a variant of the micro-macro Parareal algorithm for scalar stochastic differential equations (SDEs) to higher-dimensional SDEs.","2024-01-03","2024-01-09 16:20:41","2024-02-08 19:32:08","2024-01-09 16:20:41","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2401.01798 [cs, math, stat]","","C:\Users\isido\Zotero\storage\GGT46R3H\2401.html; C:\Users\isido\Zotero\storage\XYRDIMHJ\Bossuyt e.a. - 2024 - Micro-macro Parareal, from ODEs to SDEs and back a.pdf","","ODE","","","","","","","","","","","","","","","","","","","","arXiv:2401.01798","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9ALA2KD","preprint","2023","Bossuyt, Ignace; Vandewalle, Stefan; Samaey, Giovanni","Monte-Carlo/Moments micro-macro Parareal method for unimodal and bimodal scalar McKean-Vlasov SDEs","","","","","http://arxiv.org/abs/2310.11365","We propose a micro-macro parallel-in-time Parareal method for scalar McKean-Vlasov stochastic differential equations (SDEs). In the algorithm, the fine Parareal propagator is a Monte Carlo simulation of an ensemble of particles, while an approximate ordinary differential equation (ODE) description of the mean and the variance of the particle distribution is used as a coarse Parareal propagator to achieve speedup. We analyse the convergence behaviour of our method for a linear problem and provide numerical experiments indicating the parallel weak scaling of the algorithm on a set of examples. We show that convergence typically takes place in a low number of iterations, depending on the quality of the ODE predictor. For bimodal SDEs, we avoid quality deterioration of the coarse predictor (compared to unimodal SDEs) through the usage of multiple ODEs, each describing the mean and variance of the particle distribution in locally unimodal regions of the phase space. The benefit of the proposed algorithm can be viewed through two lenses: (i) through the parallel-in-time lens, speedup is obtained through the use of a very cheap coarse integrator (an ODE moment model), and (ii) through the moment models lens, accuracy is iteratively gained through the use of parallel machinery as a corrector. In contrast to the isolated use of a moment model, the proposed method (iteratively) converges to the true distribution generated by the SDE.","2023-10-17","2024-01-09 16:12:32","2024-02-08 19:31:51","2024-01-09 16:12:32","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.11365 [physics, stat]","","C:\Users\isido\Zotero\storage\5KQIBQEF\2310.html; C:\Users\isido\Zotero\storage\QRTLPRV2\Bossuyt e.a. - 2023 - Monte-CarloMoments micro-macro Parareal method fo.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2310.11365","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G8SQLUZM","journalArticle","2020","Mortier, Bert; Baelmans, Martine; Samaey, Giovanni","Kinetic-diffusion asymptotic-preserving Monte Carlo algorithm for Boltzmann-BGK in the diffusive scaling","Contributions to Plasma Physics","","0863-1042, 1521-3986","10.1002/ctpp.201900134","http://arxiv.org/abs/2012.08985","We develop a novel Monte Carlo strategy for the simulation of the Boltzmann-BGK model with both low-collisional and high-collisional regimes present. The presented solution to maintain accuracy in low-collisional regimes and remove exploding simulation costs in high-collisional regimes uses hybridized particles that exhibit both kinetic behaviour and diffusive behaviour depending on the local collisionality. In this work, we develop such a method that maintains the correct mean, variance, and correlation of the positional increments over multiple time steps of fixed step size for all values of the collisionality, under the condition of spatial homogeneity during the time step. In the low-collisional regime, the method reverts to the standard velocity-jump process. In the high-collisional regime, the method collapses to a standard random walk process. We analyze the error of the presented scheme in the low-collisional regime for which we obtain the order of convergence in the time step size. We furthermore provide an analysis in the high-collisional regime that demonstrates the asymptotic-preserving property.","2020-06","2024-01-09 08:51:24","2024-02-08 19:31:36","2024-01-09 08:51:24","e201900134","","5-6","60","","Contributions to Plasma Physics","","","","","","","","","","","","","arXiv.org","","arXiv:2012.08985 [cs, math]","","C:\Users\isido\Zotero\storage\BBFGPM78\2012.html; C:\Users\isido\Zotero\storage\RIVSK9K5\Mortier e.a. - 2020 - Kinetic-diffusion asymptotic-preserving Monte Carl.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AEFX9XD","preprint","2017","Dimarco, G.; Pareschi, L.; Samaey, G.","Asymptotic-Preserving Monte Carlo methods for transport equations in the diffusive limit","","","","","http://arxiv.org/abs/1707.09672","We develop a new Monte Carlo method that solves hyperbolic transport equations with stiﬀ terms, characterized by a (small) scaling parameter. In particular, we focus on systems which lead to a reduced problem of parabolic type in the limit when the scaling parameter tends to zero. Classical Monte Carlo methods suﬀer of severe time step limitations in these situations, due to the fact that the characteristic speeds go to inﬁnity in the diﬀusion limit. This makes the problem a real challenge, since the scaling parameter may diﬀer by several orders of magnitude in the domain. To circumvent these time step limitations, we construct a new, asymptotic-preserving Monte Carlo method that is stable independently of the scaling parameter and degenerates to a standard probabilistic approach for solving the limiting equation in the diﬀusion limit. The method uses an implicit time discretization to formulate a modiﬁed equation in which the characteristic speeds do not grow indeﬁnitely when the scaling factor tends to zero. The resulting modiﬁed equation can readily be discretized by a Monte Carlo scheme, in which the particles combine a ﬁnite propagation speed with a time-step dependent diﬀusion term. We show the performance of the method by comparing it with standard (deterministic) approaches in the literature.","2017-07-30","2024-01-09 08:37:03","2024-02-08 19:31:30","2024-01-09 08:37:03","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1707.09672 [math]","","C:\Users\isido\Zotero\storage\UW3QQL4W\Dimarco e.a. - 2017 - Asymptotic-Preserving Monte Carlo methods for tran.pdf","","monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:1707.09672","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KPS7TUI8","journalArticle","2014","Dimarco, G.; Pareschi, L.","Numerical methods for kinetic equations","Acta Numerica","","0962-4929, 1474-0508","10.1017/S0962492914000063","https://www.cambridge.org/core/product/identifier/S0962492914000063/type/journal_article","In this survey we consider the development and mathematical analysis of numerical methods for kinetic partial differential equations. Kinetic equations represent a way of describing the time evolution of a system consisting of a large number of particles. Due to the high number of dimensions and their intrinsic physical properties, the construction of numerical methods represents a challenge and requires a careful balance between accuracy and computational complexity. Here we review the basic numerical techniques for dealing with such equations, including the case of semi-Lagrangian methods, discrete-velocity models and spectral methods. In addition we give an overview of the current state of the art of numerical methods for kinetic equations. This covers the derivation of fast algorithms, the notion of asymptotic-preserving methods and the construction of hybrid schemes.","2014-05","2024-01-09 07:53:11","2024-01-09 07:53:11","2024-01-09 07:53:11","369-520","","","23","","Acta Numerica","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\5GPNQ4LU\Dimarco en Pareschi - 2014 - Numerical methods for kinetic equations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RARVBR5P","journalArticle","2005","Reiter, D.; Baelmans, M.; Börner, P.","The EIRENE and B2-EIRENE Codes","Fusion Science and Technology","","1536-1055, 1943-7641","10.13182/FST47-172","https://www.tandfonline.com/doi/full/10.13182/FST47-172","The EIRENE neutral gas transport Monte Carlo code has been developed initially for TEXTOR since the early 1980s. It is currently applied worldwide in most fusion laboratories for a large variety of different purposes. The main goal of code development was to provide a tool to investigate neutral gas transport in magnetically confined plasmas. But, due to its flexibility, it also can be used to solve more general linear kinetic transport equations by applying a stochastic rather than a numerical or analytical method of solution. Major applications of EIRENE are in connection with plasma fluid codes, in particular with the various versions of the B2 twodimensional plasma edge fluid code. The combined code package B2-EIRENE was developed, again initially for TEXTOR applications, in the late 1980s. It too has become a standard tool in plasma edge science. It is currently mainly used for divertor configurations, such as by the ITER central team, to assist the design of the ITER divertor. Both the EIRENE and B2-EIRENE concepts are introduced and illustrated with sample applications.","2005-02","2024-01-08 16:48:29","2024-01-08 16:48:29","2024-01-08 16:48:29","172-186","","2","47","","Fusion Science and Technology","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\NXVFMMG5\Reiter e.a. - 2005 - The EIRENE and B2-EIRENE Codes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WJCQ34K","journalArticle","2023","Løvbak, Emil; Samaey, Giovanni","Accelerated simulation of Boltzmann-BGK equations near the diffusive limit with asymptotic-preserving multilevel Monte Carlo","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/22M1498498","http://arxiv.org/abs/2205.12130","Kinetic equations model the position-velocity distribution of particles subject to transport and collision effects. Under a diffusive scaling, these combined effects converge to a diffusion equation for the position density in the limit of an infinite collision rate. Despite this well-defined limit, numerical simulation is expensive when the collision rate is high but finite, as small time steps are then required. In this work, we present an asymptotic-preserving multilevel Monte Carlo particle scheme that makes use of this diffusive limit to accelerate computations. In this scheme, we first sample the diffusive limiting model to compute a biased initial estimate of a Quantity of Interest, using large time steps. We then perform a limited number of finer simulations with transport and collision dynamics to correct the bias. The efficiency of the multilevel method depends on being able to perform correlated simulations of particles on a hierarchy of discretization levels. We present a method for correlating particle trajectories and present both an analysis and numerical experiments. We demonstrate that our approach significantly reduces the cost of particle simulations in high-collisional regimes, compared with prior work, indicating significant potential for adopting these schemes in various areas of active research.","2023-08-31","2024-01-08 14:04:45","2024-02-08 19:31:08","2024-01-08 14:04:45","A1862-A1889","","4","45","","SIAM J. Sci. Comput.","","","","","","","","","","","","","arXiv.org","","arXiv:2205.12130 [cs, math]","Comment: 28 page paper with 5 figures, 5 tables and 2 algorithms, 45 pages of additional supplementary material, minor textual changes following reviewer comments","C:\Users\isido\Zotero\storage\R4C2Y8PI\2205.html; C:\Users\isido\Zotero\storage\FWL23K3V\Løvbak en Samaey - 2023 - Accelerated simulation of Boltzmann-BGK equations .pdf","","monte carlo; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KM6T4QEC","journalArticle","2023","Nicolet, Baptiste; Rousselle, Fabrice; Novak, Jan; Keller, Alexander; Jakob, Wenzel; Müller, Thomas","Recursive Control Variates for Inverse Rendering","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3592139","https://dl.acm.org/doi/10.1145/3592139","We present a method for reducing errors---variance and bias---in physically based differentiable rendering (PBDR). Typical applications of PBDR repeatedly render a scene as part of an optimization loop involving gradient descent. The actual change introduced by each gradient descent step is often relatively small, causing a significant degree of redundancy in this computation. We exploit this redundancy by formulating a gradient estimator that employs a               recursive control variate               , which leverages information from previous optimization steps. The control variate reduces variance in gradients, and, perhaps more importantly, alleviates issues that arise from differentiating loss functions with respect to noisy inputs, a common cause of drift to bad local minima or divergent optimizations. We experimentally evaluate our approach on a variety of path-traced scenes containing surfaces and volumes and observe that primal rendering efficiency improves by a factor of up to 10.","2023-08","2024-01-07 09:59:48","2024-02-08 19:30:48","2024-01-07 09:59:48","1-13","","4","42","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\TA9MWMBN\Nicolet e.a. - 2023 - Recursive Control Variates for Inverse Rendering.pdf","","monte carlo; rendering; inverse problem; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LYZ86PCN","journalArticle","2022","Grohs, Philipp; Jentzen, Arnulf; Salimova, Diyora","Deep neural network approximations for solutions of PDEs based on Monte Carlo algorithms","Partial Differential Equations and Applications","","2662-2963, 2662-2971","10.1007/s42985-021-00100-z","https://link.springer.com/10.1007/s42985-021-00100-z","In the past few years deep artiﬁcial neural networks (DNNs) have been successfully employed in a large number of computational problems including, e.g., language processing, image recognition, fraud detection, and computational advertisement. Recently, it has also been proposed in the scientiﬁc literature to reformulate high-dimensional partial differential equations (PDEs) as stochastic learning problems and to employ DNNs together with stochastic gradient descent methods to approximate the solutions of such high-dimensional PDEs. There are also a few mathematical convergence results in the scientiﬁc literature which show that DNNs can approximate solutions of certain PDEs without the curse of dimensionality in the sense that the number of real parameters employed to describe the DNN grows at most polynomially both in the PDE dimension d ∈ N and the reciprocal of the prescribed approximation accuracy ε > 0. One key argument in most of these results is, ﬁrst, to employ a Monte Carlo approximation scheme which can approximate the solution of the PDE under consideration at a ﬁxed space-time point without the curse of dimensionality and, thereafter, to prove then that DNNs are ﬂexible enough to mimic the behaviour of the employed approximation scheme. Having this in mind, one could aim for a general abstract result which shows under suitable assumptions that if a certain function can be approximated by any kind of (Monte Carlo) approximation scheme without the curse of dimensionality, then the function can also be approximated with DNNs without the curse of dimensionality. It is a subject of this article to make a ﬁrst step towards this direction. In particular, the main result of this paper, roughly speaking, shows that if a function can be approximated by means of some suitable discrete approximation scheme without the curse of dimensionality and if there exist DNNs which satisfy certain regularity properties and which approximate this discrete approximation scheme without the curse of dimensionality, then the function itself can also be approximated with DNNs without the curse of dimensionality. Moreover, for the number of real parameters used to describe such approximating DNNs we provide an explicit upper bound for the optimal exponent of the dimension d ∈ N of the function under consideration as well as an explicit lower bound for the optimal exponent of the prescribed approximation accuracy ε > 0. As an application of this result we derive that solutions of suitable Kolmogorov PDEs can be approximated with DNNs without the curse of dimensionality.","2022-08","2024-01-18 13:14:02","2024-02-08 19:34:47","2024-01-18 13:14:02","45","","4","3","","Partial Differ. Equ. Appl.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\V3BZRWL6\Grohs e.a. - 2022 - Deep neural network approximations for solutions o.pdf","","monte carlo; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQJ4WJZ4","journalArticle","2022","Sabelfeld, Karl; Kireeva, Anastasya","A Global Random Walk on Spheres algorithm for calculating the solution and its derivatives of the drift-diffusion-reaction equations","Mathematical Methods in the Applied Sciences","","1099-1476","10.1002/mma.7861","https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.7861","A Global Random Walk on Spheres (GRWS) algorithm for calculating the solution and its derivatives of drift-diffusion-reaction equations in any desired set of points is suggested. The GRWS algorithm is able to find the solution and derivative fields in any family of points simultaneously, using only one ensemble of random walks which drastically decreases the computer time compared to the standard random walk-based methods. The method in its nature is stochastic and meshless; the cost is of the order |log(ε)|/ε2 independent of the space dimension and complexity of the boundary shape, where ε is the desired accuracy. The variance, accuracy, and the cost of the method suggested are given. For illustration, we present simulation results for exciton drift-diffusion-recombination transport in a 3D prism and compare them with the exact solution. Calculations are also given for derivatives of a 2D diffusion problem which show the same accuracy as for the solution itself; the simulations are compared against the exact results.","2022","2024-01-25 15:11:12","2024-02-09 14:47:12","2024-01-25 15:11:11","1420-1431","","3","45","","","","","","","","","","en","","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mma.7861","","C:\Users\isido\Zotero\storage\KWJSMCYY\Sabelfeld en Kireeva - 2022 - A Global Random Walk on Spheres algorithm for calc.pdf; C:\Users\isido\Zotero\storage\68QW4MJV\mma.html","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IS3H89RP","preprint","2024","Yang, Shangda; Zankin, Vitaly; Balandat, Maximilian; Scherer, Stefan; Carlberg, Kevin; Walton, Neil; Law, Kody J. H.","Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need","","","","","http://arxiv.org/abs/2402.02111","We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. The complexity rate of naive Monte Carlo degrades for nested operations, whereas MLMC is capable of achieving the canonical Monte Carlo convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for one- and two-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available here https://github.com/Shangda-Yang/MLMCBO.","2024-02-03","2024-02-09 18:13:18","2024-02-09 18:14:03","2024-02-09 18:13:18","","","","","","","Accelerating Look-ahead in Bayesian Optimization","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.02111 [cs, math, stat]","","C:\Users\isido\Zotero\storage\KDHDDGGP\2402.html; C:\Users\isido\Zotero\storage\DIN7JU8Z\Yang e.a. - 2024 - Accelerating Look-ahead in Bayesian Optimization .pdf","","monte carlo; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2402.02111","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87HPA5ZB","preprint","2023","Ishikawa, Kei","On the Parallel Complexity of Multilevel Monte Carlo in Stochastic Gradient Descent","","","","","http://arxiv.org/abs/2310.02402","In the stochastic gradient descent (SGD) for sequential simulations such as the neural stochastic differential equations, the Multilevel Monte Carlo (MLMC) method is known to offer better theoretical computational complexity compared to the naive Monte Carlo approach. However, in practice, MLMC scales poorly on massively parallel computing platforms such as modern GPUs, because of its large parallel complexity which is equivalent to that of the naive Monte Carlo method. To cope with this issue, we propose the delayed MLMC gradient estimator that drastically reduces the parallel complexity of MLMC by recycling previously computed gradient components from earlier steps of SGD. The proposed estimator provably reduces the average parallel complexity per iteration at the cost of a slightly worse per-iteration convergence rate. In our numerical experiments, we use an example of deep hedging to demonstrate the superior parallel complexity of our method compared to the standard MLMC in SGD.","2023-10-10","2024-02-09 18:17:42","2024-02-09 18:19:41","2024-02-09 18:17:42","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.02402 [cs, stat]","Comment: Fixed a typo in the title and added acknowledgement","C:\Users\isido\Zotero\storage\JG9SRHMK\2310.html; C:\Users\isido\Zotero\storage\3ZQ6PK83\Ishikawa - 2023 - On the Parallel Complexity of Multilevel Monte Car.pdf","","monte carlo; SGD; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2310.02402","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBZKW48W","preprint","2022","Karppa, Matti; Pagh, Rasmus","HyperLogLogLog: Cardinality Estimation With One Log More","","","","","http://arxiv.org/abs/2205.11327","We present HyperLogLogLog, a practical compression of the HyperLogLog sketch that compresses the sketch from $O(m\log\log n)$ bits down to $m \log_2\log_2\log_2 m + O(m+\log\log n)$ bits for estimating the number of distinct elements~$n$ using $m$~registers. The algorithm works as a drop-in replacement that preserves all estimation properties of the HyperLogLog sketch, it is possible to convert back and forth between the compressed and uncompressed representations, and the compressed sketch maintains mergeability in the compressed domain. The compressed sketch can be updated in amortized constant time, assuming $n$ is sufficiently larger than $m$. We provide a C++ implementation of the sketch, and show by experimental evaluation against well-known implementations by Google and Apache that our implementation provides small sketches while maintaining competitive update and merge times. Concretely, we observed approximately a 40% reduction in the sketch size. Furthermore, we obtain as a corollary a theoretical algorithm that compresses the sketch down to $m\log_2\log_2\log_2\log_2 m+O(m\log\log\log m/\log\log m+\log\log n)$ bits.","2022-05-23","2024-02-10 08:43:11","2024-05-17 07:00:07","2024-02-10 08:43:11","","","","","","","HyperLogLogLog","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.11327 [cs] version: 1","Comment: 10 pages, 7 figures, KDD '22","C:\Users\isido\Zotero\storage\48XDY5VF\2205.html; C:\Users\isido\Zotero\storage\G36TIVV7\Karppa en Pagh - 2022 - HyperLogLogLog Cardinality Estimation With One Lo.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.11327","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JHNHGZNU","conferencePaper","2013","Heule, Stefan; Nunkesser, Marc; Hall, Alexander","HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm","Proceedings of the 16th International Conference on Extending Database Technology","978-1-4503-1597-5","","10.1145/2452376.2452456","https://dl.acm.org/doi/10.1145/2452376.2452456","Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and signiﬁcantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.","2013-03-18","2024-02-10 08:43:27","2024-05-17 06:59:59","2024-02-10 08:43:27","683-692","","","","","","HyperLogLog in practice","","","","","ACM","Genoa Italy","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\RU9HSQD4\Heule e.a. - 2013 - HyperLogLog in practice algorithmic engineering o.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EDBT/ICDT '13: Joint 2013 EDBT/ICDT Conferences","","","","","","","","","","","","","","",""
"8HIKDPFM","preprint","2024","Ertl, Otmar","UltraLogLog: A Practical and More Space-Efficient Alternative to HyperLogLog for Approximate Distinct Counting","","","","","http://arxiv.org/abs/2308.16862","Since its invention HyperLogLog has become the standard algorithm for approximate distinct counting. Due to its space efficiency and suitability for distributed systems, it is widely used and also implemented in numerous databases. This work presents UltraLogLog, which shares the same practical properties as HyperLogLog. It is commutative, idempotent, mergeable, and has a fast guaranteed constant-time insert operation. At the same time, it requires 28% less space to encode the same amount of distinct count information, which can be extracted using the maximum likelihood method. Alternatively, a simpler and faster estimator is proposed, which still achieves a space reduction of 24%, but at an estimation speed comparable to that of HyperLogLog. In a non-distributed setting where martingale estimation can be used, UltraLogLog is able to reduce space by 17%. Moreover, its smaller entropy and its 8-bit registers lead to better compaction when using standard compression algorithms. All this is verified by experimental results that are in perfect agreement with the theoretical analysis which also outlines potential for even more space-efficient data structures. A production-ready Java implementation of UltraLogLog has been released as part of the open-source Hash4j library.","2024-01-15","2024-02-10 10:08:38","2024-05-17 06:59:49","2024-02-10 10:08:38","","","","","","","UltraLogLog","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.16862 [cs]","Comment: 25 pages, revision","C:\Users\isido\Zotero\storage\PHZI5I4C\2308.html; C:\Users\isido\Zotero\storage\92L5IUIU\Ertl - 2024 - UltraLogLog A Practical and More Space-Efficient .pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2308.16862","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XPDW3Z7","encyclopediaArticle","2024","","Bloom filter","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Bloom_filter&oldid=1193100955","A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not – in other words, a query returns either ""possibly in set"" or ""definitely not in set"". Elements can be added to the set, but not removed (though this can be addressed with the counting Bloom filter variant); the more items added, the larger the probability of false positives. The high level idea is to map elements x∈X to values                         y         =         h         (         x         )         ∈         Y                 {\displaystyle y=h(x)\in Y}    using a hash function h, and then test for membership of                                    x           ′                  ∈         X                 {\displaystyle x'\in X}    by checking whether                                    y           ′                  =         h         (                    x           ′                  )         ∈         Y                 {\displaystyle y'=h(x')\in Y}   , and do that using multiple hash functions h. Bloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if ""conventional"" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses.More generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set.","2024-01-02","2024-02-11 13:48:32","2024-02-11 13:48:34","2024-02-11 13:48:32","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1193100955","","C:\Users\isido\Zotero\storage\DR4YNWWP\Bloom_filter.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YRTKVRRI","encyclopediaArticle","2024","","Cuckoo filter","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Cuckoo_filter&oldid=1198796664","A cuckoo filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set, like a Bloom filter does. False positive matches are possible, but false negatives are not – in other words, a query returns either ""possibly in set"" or ""definitely not in set"". A cuckoo filter can also delete existing items, which is not supported by Bloom filters. In addition, for applications that store many items and target moderately low false positive rates, cuckoo filters can achieve lower space overhead than space-optimized Bloom filters.Cuckoo filters were first described in 2014.","2024-01-25","2024-02-11 14:07:23","2024-02-11 14:07:23","2024-02-11 14:07:23","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1198796664","","C:\Users\isido\Zotero\storage\3KZANEI7\Cuckoo_filter.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2NHALHJ","encyclopediaArticle","2023","","Median trick","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Median_trick&oldid=1175115452","The median trick is a generic approach that increases the chances of a probabilistic algorithm to succeed. Apparently first used in 1986 by Jerrum et al. for approximate counting algorithms, the technique was later applied to a broad selection of classification and regression problems.The idea of median trick is very simple: run the randomized algorithm with  numeric output multiple times, and use the median of the obtained results as a final answer. For example, for sublinear in time algorithms the same algorithm can be run repeatedly (or in parallel) over random subsets of input data, and, per Chernoff inequality, the median of the results will converge to solution very fast. For the algorithms that are sublinear in space (e.g., counting the distinct elements of a stream), different randomizations of the algorithm (say, with different hash functions) should be used for repeated runs over the same data.","2023-09-12","2024-02-11 14:15:22","2024-02-11 14:15:22","2024-02-11 14:15:22","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1175115452","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4P8DNPG","encyclopediaArticle","2024","","Chernoff bound","Wikipedia","","","","https://en.wikipedia.org/w/index.php?title=Chernoff_bound&oldid=1204574015","In probability theory, a Chernoff bound is an exponentially decreasing upper bound on the tail of a random variable based on its moment generating function. The minimum of all such exponential bounds forms the Chernoff or Chernoff-Cramér bound, which may decay faster than exponential (e.g. sub-Gaussian). It is especially useful for sums of independent random variables, such as sums of Bernoulli random variables.The bound is commonly named after Herman Chernoff who described the method in a 1952 paper, though Chernoff himself attributed it to Herman Rubin. In 1938 Harald Cramér had published an almost identical concept now known as Cramér's theorem. It is a sharper bound than the first- or second-moment-based tail bounds such as Markov's inequality or Chebyshev's inequality, which only yield power-law bounds on tail decay. However, when applied to sums the Chernoff bound requires the random variables to be independent, a condition that is not required by either Markov's inequality or Chebyshev's inequality (although Chebyshev's inequality does require the random variables to be pairwise independent). The Chernoff bound is related to the Bernstein inequalities. It is also used to prove Hoeffding's inequality, Bennett's inequality, and McDiarmid's inequality.","2024-02-07","2024-02-11 14:19:35","2024-02-11 14:19:35","2024-02-11 14:19:35","","","","","","","","","","","","","","en","Creative Commons Attribution-ShareAlike License","","","","Wikipedia","","Page Version ID: 1204574015","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NHBFMD85","webpage","2024","","Open Syllabus: Galaxy","Open Syllabus","","","","https://galaxy.opensyllabus.org/","The citation graph of the ~1M most-assigned books and articles (node2vec -> UMAP).","2024-02-23","2024-02-23 17:42:20","2024-08-12 14:51:17","2024-02-23 17:42:20","","","","","","","Open Syllabus","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\NM5YXFHX\galaxy.opensyllabus.org.html","","♥♥♥; dimension reduction; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVZBDE95","journalArticle","2023","Lam, Remi; Sanchez-Gonzalez, Alvaro; Willson, Matthew; Wirnsberger, Peter; Fortunato, Meire; Alet, Ferran; Ravuri, Suman; Ewalds, Timo; Eaton-Rosen, Zach; Hu, Weihua; Merose, Alexander; Hoyer, Stephan; Holland, George; Vinyals, Oriol; Stott, Jacklynn; Pritzel, Alexander; Mohamed, Shakir; Battaglia, Peter","Learning skillful medium-range global weather forecasting","Science","","0036-8075, 1095-9203","10.1126/science.adi2336","https://www.science.org/doi/10.1126/science.adi2336","Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning–based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25° resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems.           ,              Editor’s summary                            The numerical models used to predict weather are large, complex, and computationally demanding and do not learn from past weather patterns. Lam               et al               . introduced a machine learning–based method that has been trained directly from reanalysis data of past atmospheric conditions. In this way, the authors were able to quickly predict hundreds of weather variables globally up to 10 days in advance and at high resolution. Their predictions were more accurate than those of traditional weather models in 90% of tested cases and displayed better severe event prediction for tropical cyclones, atmospheric rivers, and extreme temperatures. —H. Jesse Smith                        ,              Machine learning leads to better, faster, and cheaper weather forecasting.","2023-12-22","2024-02-23 18:13:50","2024-05-17 06:59:23","2024-02-23 18:13:50","1416-1421","","6677","382","","Science","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\TBIB95L4\Lam e.a. - 2023 - Learning skillful medium-range global weather fore.pdf","","deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B3L6672A","preprint","2023","Wang, Hongyu; Ma, Shuming; Dong, Li; Huang, Shaohan; Wang, Huaijie; Ma, Lingxiao; Yang, Fan; Wang, Ruiping; Wu, Yi; Wei, Furu","BitNet: Scaling 1-bit Transformers for Large Language Models","","","","","http://arxiv.org/abs/2310.11453","The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.","2023-10-17","2024-03-06 12:03:41","2024-05-17 06:59:06","2024-03-06 12:03:41","","","","","","","BitNet","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.11453 [cs]","Comment: Work in progress","C:\Users\isido\Zotero\storage\CFRF6XUK\2310.html; C:\Users\isido\Zotero\storage\JSPCGCZ9\Wang e.a. - 2023 - BitNet Scaling 1-bit Transformers for Large Langu.pdf","","deep learning; llms; quantization","","","","","","","","","","","","","","","","","","","","arXiv:2310.11453","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBFM8ELF","preprint","2024","Ma, Shuming; Wang, Hongyu; Ma, Lingxiao; Wang, Lei; Wang, Wenhui; Huang, Shaohan; Dong, Li; Wang, Ruiping; Xue, Jilong; Wei, Furu","The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","","","","","http://arxiv.org/abs/2402.17764","Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.","2024-02-27","2024-03-06 12:03:51","2024-05-17 06:58:48","2024-03-06 12:03:51","","","","","","","The Era of 1-bit LLMs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.17764 [cs]","Comment: Work in progress","C:\Users\isido\Zotero\storage\ZZE7WLUH\2402.html; C:\Users\isido\Zotero\storage\MTXLSG6M\Ma e.a. - 2024 - The Era of 1-bit LLMs All Large Language Models a.pdf","","deep learning; llms; quantization","","","","","","","","","","","","","","","","","","","","arXiv:2402.17764","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWBLP7NZ","preprint","2018","Ma, Shuai; Yu, Jia Yuan","Transition-based versus State-based Reward Functions for MDPs with Value-at-Risk","","","","","http://arxiv.org/abs/1612.02088","In reinforcement learning, the reward function on current state and action is widely used. When the objective is about the expectation of the (discounted) total reward only, it works perfectly. However, if the objective involves the total reward distribution, the result will be wrong. This paper studies Value-at-Risk (VaR) problems in short- and long-horizon Markov decision processes (MDPs) with two reward functions, which share the same expectations. Firstly we show that with VaR objective, when the real reward function is transition-based (with respect to action and both current and next states), the simplified (state-based, with respect to action and current state only) reward function will change the VaR. Secondly, for long-horizon MDPs, we estimate the VaR function with the aid of spectral theory and the central limit theorem. Thirdly, since the estimation method is for a Markov reward process with the reward function on current state only, we present a transformation algorithm for the Markov reward process with the reward function on current and next states, in order to estimate the VaR function with an intact total reward distribution.","2018-11-29","2024-03-23 07:28:29","2024-05-17 06:58:32","2024-03-23 07:28:29","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1612.02088 [cs] version: 3","Comment: 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","C:\Users\isido\Zotero\storage\CVVTZGUS\1612.html; C:\Users\isido\Zotero\storage\4YPXXZ53\Ma en Yu - 2018 - Transition-based versus State-based Reward Functio.pdf","","reinforcement learning; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1612.02088","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RTXZ8J4","journalArticle","2001","Longstaff, Francis A.; Schwartz, Eduardo S.","Valuing American Options by Simulation: A Simple Least-Squares Approach","Review of Financial Studies","","0893-9454, 1465-7368","10.1093/rfs/14.1.113","https://academic.oup.com/rfs/article-lookup/doi/10.1093/rfs/14.1.113","","2001-01","2024-03-23 10:54:53","2024-05-17 06:58:27","2024-03-23 10:54:53","113-147","","1","14","","Rev. Financ. Stud.","Valuing American Options by Simulation","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\J77VHX24\Longstaff en Schwartz - 2001 - Valuing American Options by Simulation A Simple L.pdf","","american option","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEBMZ6DE","webpage","2017","Turkedjiev, Plamen","Least squares regression Monte Carlo for approximating BSDES and semilinear PDES","CEMRACS - Summer school: Numerical methods for stochastic models: control, uncertainty quantification, mean-field;CEMRACS - École d'été : Méthodes numériques pour équations stochastiques : contrôle, incertitude, champ moyen ; http://conferences.cirm-math.fr/1556.html","","","","http://library.cirm-math.fr/Record.htm?idlist=5&record=19283916124910011989","In this lecture, we shall discuss the key steps involved in the use of least squares regression for approximating the solution to BSDEs. This includes how to obtain explicit error estimates, and how these error estimates can be used to tune the parameters of the numerical scheme based on complexity considerations.<br>The algorithms are based on a two stage approximation process. Firstly, a suitable discrete time process is chosen to approximate the of the continuous time solution of the BSDE. The nodes of the discrete time processes can be expressed as conditional expectations. As we shall demonstrate, the choice of discrete time process is very important, as its properties will impact the performance of the overall numerical scheme. In the second stage, the conditional expectation is approximated in functional form using least squares regression on synthetically generated data – Monte Carlo simulations drawn from a suitable probability distribution. A key feature of the regression step is that the explanatory variables are built on a user chosen finite dimensional linear space of functions, which the user specifies by setting basis functions. The choice of basis functions is made on the hypothesis that it contains the solution, so regularity and boundedness assumptions are used in its construction. The impact of the choice of the basis functions is exposed in error estimates.<br>In addition to the choice of discrete time approximation and the basis functions, the Markovian structure of the problem gives significant additional freedom with regards to the Monte Carlo simulations. We demonstrate how to use this additional freedom to develop generic stratified sampling approaches that are independent of the underlying transition density function. Moreover, we demonstrate how to leverage the stratification method to develop a HPC algorithm for implementation on GPUs.<br>Thanks to the Feynmann-Kac relation between the the solution of a BSDE and its associated semilinear PDE, the approximation of the BSDE can be directly used to approximate the solution of the PDE. Moreover, the smoothness properties of the PDE play a crucial role in the selection of the hypothesis space of regressions functions, so this relationship is vitally important for the numerical scheme.<br>We conclude with some draw backs of the regression approach, notably the curse of dimensionality.","2017-07-20","2024-03-24 21:13:53","2024-03-26 10:35:00","2024-03-24 21:13:53","","","","","","","","","","","","","","Anglais","CC BY NC ND","video","","","","","Publisher: CIRM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NDEEHYCH","preprint","2022","Wang, Yutian; Ni, Yuan-Hua","Deep BSDE-ML Learning and Its Application to Model-Free Optimal Control","","","","","http://arxiv.org/abs/2201.01318","A modified Deep BSDE (backward differential equation) learning method with measurability loss, called Deep BSDE-ML method, is introduced in this paper to solve a kind of linear decoupled forward-backward stochastic differential equations (FBSDEs), which is encountered in the policy evaluation of learning the optimal feedback policies of a class of stochastic control problems. The measurability loss is characterized via the measurability of BSDE's state at the forward initial time, which differs from that related to terminal state of the known Deep BSDE method. Though the minima of the two loss functions are shown to be equal, this measurability loss is proved to be equal to the expected mean squared error between the true diffusion term of BSDE and its approximation. This crucial observation extends the application of the Deep BSDE method -- approximating the gradients of the solution of a partial differential equation (PDE) instead of the solution itself. Simultaneously, a learning-based framework is introduced to search an optimal feedback control of a deterministic nonlinear system. Specifically, by introducing Gaussian exploration noise, we are aiming to learn a robust optimal controller under this stochastic case. This reformulation sacrifices the optimality to some extent, but as suggested in reinforcement learning (RL) exploration noise is essential to enable the model-free learning.","2022-01-04","2024-03-26 09:12:20","2024-05-17 06:58:09","2024-03-26 09:12:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2201.01318 [math]","","C:\Users\isido\Zotero\storage\6IRNFHSB\2201.html; C:\Users\isido\Zotero\storage\KR3U4MVG\Wang en Ni - 2022 - Deep BSDE-ML Learning and Its Application to Model.pdf","","deep learning; BSDE","","","","","","","","","","","","","","","","","","","","arXiv:2201.01318","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZB5SMH3","journalArticle","2017","E, Weinan; Han, Jiequn; Jentzen, Arnulf","Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations","Communications in Mathematics and Statistics","","2194-6701, 2194-671X","10.1007/s40304-017-0117-6","http://arxiv.org/abs/1706.04702","We propose a new algorithm for solving parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimension, by making an analogy between the BSDE and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the BSDE. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using TensorFlow illustrate the efficiency and accuracy of the proposed algorithms for several 100-dimensional nonlinear PDEs from physics and finance such as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a nonlinear pricing model for financial derivatives.","2017-12","2024-03-26 09:14:00","2024-05-17 06:57:57","2024-03-26 09:13:59","349-380","","4","5","","Commun. Math. Stat.","","","","","","","","","","","","","arXiv.org","","arXiv:1706.04702 [cs, math, stat]","Comment: 39 pages, 15 figures","C:\Users\isido\Zotero\storage\DXEZK8Z5\1706.html; C:\Users\isido\Zotero\storage\KSK3VRY6\E e.a. - 2017 - Deep learning-based numerical methods for high-dim.pdf","","PDE; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23DTUDJ9","videoRecording","2017","","Plamen Turkedjiev: Least squares regression Monte Carlo for approximating BSDES and semilinear PDES - YouTube","","","","","https://www.youtube.com/watch?v=E9VfVd6-OjA","Abstract: In this lecture, we shall discuss the key steps involved in the use of least squares regression for approximating the solution to BSDEs. This includes how to obtain explicit error estimates, and how these error estimates can be used to tune the parameters of the numerical scheme based on complexity considerations. The algorithms are based on a two stage approximation process. Firstly, a suitable discrete time process is chosen to approximate the of the continuous time solution of the BSDE. The nodes of the discrete time processes can be expressed as conditional expectations. As we shall demonstrate, the choice of discrete time process is very important, as its properties will impact the performance of the overall numerical scheme. In the second stage, the conditional expectation is approximated in functional form using least squares regression on synthetically generated data k Monte Carlo simulations drawn from a suitable probability distribution. A key feature of the regression step is that the explanatory variables are built on a user chosen finite dimensional linear space of functions, which the user specifies by setting basis functions. The choice of basis functions is made on the hypothesis that it contains the solution, so regularity and boundedness assumptions are used in its construction. The impact of the choice of the basis functions is exposed in error estimates. In addition to the choice of discrete time approximation and the basis functions, the Markovian structure of the problem gives significant additional freedom with regards to the Monte Carlo simulations. We demonstrate how to use this additional freedom to develop generic stratified sampling approaches that are independent of the underlying transition density function. Moreover, we demonstrate how to leverage the stratification method to develop a HPC algorithm for implementation on GPUs. Thanks to the Feynmann-Kac relation between the the solution of a BSDE and its associated semilinear PDE, the approximation of the BSDE can be directly used to approximate the solution of the PDE. Moreover, the smoothness properties of the PDE play a crucial role in the selection of the hypothesis space of regressions functions, so this relationship is vitally important for the numerical scheme. We conclude with some draw backs of the regression approach, notably the curse of dimensionality. Recording during the CEMRACS Summer school 2017 ""Numerical Methods for Stochastic Models: Control, Uncertainty Quantification, Mean-field "" the July 20, 2017 at the Centre International de Rencontres Mathématiques (Marseille, France) Filmmaker: Guillaume Hennenfent Find this video and other talks given by worldwide mathematicians on CIRM's Audiovisual Mathematics Library: http://library.cirm-math.fr. And discover all its functionalities:  - Chapter markers and keywords to watch the parts of your choice in the video - Videos enriched with abstracts, bibliographies, Mathematics Subject Classification - Multi-criteria search by author, title, tags, mathematical area","2017-08-11","2024-03-26 10:33:31","2024-08-12 14:56:26","2024-03-26 10:33:31","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\WJDEA3L3\watch.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YD4XCF3S","journalArticle","2017","Turkedjiev, Plamen","Least-square regression Monte Carlo for approximating BSDEs and semilinear PDEs","","","","","","","2017-07-20","2024-03-26 10:36:27","2024-08-12 14:39:28","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\ZBJXBW2U\Turkedjiev - Least-square regression Monte Carlo for approximat.pdf","","BSDE; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"REJA8M2F","bookSection","2023","Dunn, William L.; Shultis, J. Kenneth","Linear Operator Equations","Exploring Monte Carlo Methods","978-0-12-819739-4","","","https://linkinghub.elsevier.com/retrieve/pii/B9780128197394000160","","2023","2024-03-31 08:24:01","2024-03-31 08:24:01","2024-03-31 08:24:01","291-368","","","","","","","","","","","Elsevier","","en","https://www.elsevier.com/tdm/userlicense/1.0/","","","","DOI.org (Crossref)","","DOI: 10.1016/B978-0-12-819739-4.00016-0","","C:\Users\isido\Zotero\storage\XNYCTUHG\Dunn en Shultis - 2023 - Linear Operator Equations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RAFWMANU","journalArticle","2018","Han, Jiequn; Jentzen, Arnulf; E, Weinan","Solving high-dimensional partial differential equations using deep learning","Proceedings of the National Academy of Sciences","","0027-8424, 1091-6490","10.1073/pnas.1718942115","http://arxiv.org/abs/1707.02568","Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the ""curse of dimensionality"". This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.","2018-08-21","2024-04-01 16:17:39","2024-05-17 06:55:17","2024-04-01 16:17:39","8505-8510","","34","115","","Proc. Natl. Acad. Sci. U.S.A.","","","","","","","","","","","","","arXiv.org","","arXiv:1707.02568 [cs, math]","Comment: 13 pages, 6 figures","C:\Users\isido\Zotero\storage\LEAWXB4F\1707.html; C:\Users\isido\Zotero\storage\5FH5LMXT\Han e.a. - 2018 - Solving high-dimensional partial differential equa.pdf","","PDE; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPM7V6LQ","journalArticle","2016","Klimsiak, Tomasz; Rozkosz, Andrzej; Ziemkiewicz, Bartosz","Valuing American options by simulation: A BSDEs approach","Mathematics and Computers in Simulation","","03784754","10.1016/j.matcom.2015.11.009","https://linkinghub.elsevier.com/retrieve/pii/S0378475415002724","We provide probabilistic proofs of convergence of several easy to implement schemes for computing the value function of American (call and put) options written on a dividend paying stock governed by the geometric Brownian motion. The proofs are based on representations of the value function by means of solutions of some backward stochastic diﬀerential equations. Despite the probabilistic nature of the proofs the numerical schemes are nevertheless deterministic. Simulation results are also presented.","2016-05","2024-04-01 16:18:52","2024-05-17 06:55:08","2024-04-01 16:18:52","1-18","","","123","","Mathematics and Computers in Simulation","Valuing American options by simulation","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\D3KFSMYS\Klimsiak e.a. - 2016 - Valuing American options by simulation A BSDEs ap.pdf","","american option; BSDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2XMUZN6E","preprint","2013","Klimsiak, Tomasz; Rozkosz, Andrzej","On backward stochastic differential equations approach to valuation of American options","","","","","http://arxiv.org/abs/1012.4442","We consider the problem of valuation of American (call and put) options written on a dividend paying stock governed by the geometric Brownian motion. We show that the value function has two different but related representations: by means of a solution of some nonlinear backward stochastic differential equation and weak solution to some semilinear partial differential equation.","2013-02-11","2024-04-02 14:48:38","2024-05-17 06:54:58","2024-04-02 14:48:38","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1012.4442 [math]","","C:\Users\isido\Zotero\storage\IFL2JSPM\1012.html; C:\Users\isido\Zotero\storage\MAU46LKV\Klimsiak en Rozkosz - 2013 - On backward stochastic differential equations appr.pdf","","american option; BSDE; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1012.4442","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4AQMPTH","journalArticle","2016","Klimsiak, Tomasz; Rozkosz, Andrzej","The Early Exercise Premium Representation for American Options on Multiply Assets","Applied Mathematics & Optimization","","0095-4616, 1432-0606","10.1007/s00245-015-9293-5","http://link.springer.com/10.1007/s00245-015-9293-5","In the paper we consider the problem of valuation of American options written on dividend-paying assets whose price dynamics follow the classical multidimensional Black and Scholes model. We provide a general early exercise premium representation formula for options with payoff functions which are convex or satisfy mild regularity assumptions. Examples include index options, spread options, call on max options, put on min options, multiply strike options and power-product options. In the proof of the formula we exploit close connections between the optimal stopping problems associated with valuation of American options, obstacle problems and reﬂected backward stochastic differential equations.","2016-02","2024-04-02 16:42:51","2024-05-17 06:54:40","2024-04-02 16:42:51","99-114","","1","73","","Appl Math Optim","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\S6RIVHJY\Klimsiak en Rozkosz - 2016 - The Early Exercise Premium Representation for Amer.pdf","","american option; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VGWU6GD4","journalArticle","2021","Becker, Sebastian; Cheridito, Patrick; Jentzen, Arnulf; Welti, Timo","Solving high-dimensional optimal stopping problems using deep learning","European Journal of Applied Mathematics","","0956-7925, 1469-4425","10.1017/S0956792521000073","http://arxiv.org/abs/1908.01602","Nowadays many financial derivatives, such as American or Bermudan options, are of early exercise type. Often the pricing of early exercise options gives rise to high-dimensional optimal stopping problems, since the dimension corresponds to the number of underlying assets. High-dimensional optimal stopping problems are, however, notoriously difficult to solve due to the well-known curse of dimensionality. In this work, we propose an algorithm for solving such problems, which is based on deep learning and computes, in the context of early exercise option pricing, both approximations of an optimal exercise strategy and the price of the considered option. The proposed algorithm can also be applied to optimal stopping problems that arise in other areas where the underlying stochastic process can be efficiently simulated. We present numerical results for a large number of example problems, which include the pricing of many high-dimensional American and Bermudan options, such as Bermudan max-call options in up to 5000 dimensions. Most of the obtained results are compared to reference values computed by exploiting the specific problem design or, where available, to reference values from the literature. These numerical results suggest that the proposed algorithm is highly effective in the case of many underlyings, in terms of both accuracy and speed.","2021-06","2024-04-03 21:22:46","2024-05-17 06:54:29","2024-04-03 21:22:46","470-514","","3","32","","Eur. J. Appl. Math","","","","","","","","","","","","","arXiv.org","","arXiv:1908.01602 [cs, math, q-fin]","Comment: 54 pages, 1 figure","C:\Users\isido\Zotero\storage\ANKENT9I\1908.html; C:\Users\isido\Zotero\storage\KDSQQELX\Becker e.a. - 2021 - Solving high-dimensional optimal stopping problems.pdf; C:\Users\isido\Zotero\storage\FCEWUK86\Becker e.a. - 2021 - Solving high-dimensional optimal stopping problems.pdf","","PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPTFJ48S","preprint","2022","Boussange, Victor; Becker, Sebastian; Jentzen, Arnulf; Kuckuck, Benno; Pellissier, Loïc","Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions","","","","","http://arxiv.org/abs/2205.03672","Nonlinear partial differential equations (PDEs) are used to model dynamical processes in a large number of scientific fields, ranging from finance to biology. In many applications standard local models are not sufficient to accurately account for certain non-local phenomena such as, e.g., interactions at a distance. In order to properly capture these phenomena non-local nonlinear PDE models are frequently employed in the literature. In this article we propose two numerical methods based on machine learning and on Picard iterations, respectively, to approximately solve non-local nonlinear PDEs. The proposed machine learning-based method is an extended variant of a deep learning-based splitting-up type approximation method previously introduced in the literature and utilizes neural networks to provide approximate solutions on a subset of the spatial domain of the solution. The Picard iterations-based method is an extended variant of the so-called full history recursive multilevel Picard approximation scheme previously introduced in the literature and provides an approximate solution for a single point of the domain. Both methods are mesh-free and allow non-local nonlinear PDEs with Neumann boundary conditions to be solved in high dimensions. In the two methods, the numerical difficulties arising due to the dimensionality of the PDEs are avoided by (i) using the correspondence between the expected trajectory of reflected stochastic processes and the solution of PDEs (given by the Feynman-Kac formula) and by (ii) using a plain vanilla Monte Carlo integration to handle the non-local term. We evaluate the performance of the two methods on five different PDEs arising in physics and biology. In all cases, the methods yield good results in up to 10 dimensions with short run times. Our work extends recently developed methods to overcome the curse of dimensionality in solving PDEs.","2022-05-07","2024-04-04 10:26:04","2024-05-17 06:54:14","2024-04-04 10:26:03","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.03672 [cs, math]","Comment: 59 pages","C:\Users\isido\Zotero\storage\HMSMAYH8\2205.html; C:\Users\isido\Zotero\storage\4AC7HA5R\Boussange e.a. - 2022 - Deep learning approximations for non-local nonline.pdf","","PDE; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2205.03672","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ZTD9I8Z","journalArticle","2020","Du, Jiayi; Jin, Muyang; Kolm, Petter N.; Ritter, Gordon; Wang, Yixuan; Zhang, Bofei","Deep Reinforcement Learning for Option Replication and Hedging","The Journal of Financial Data Science","","2640-3943","10.3905/jfds.2020.1.045","http://pm-research.com/lookup/doi/10.3905/jfds.2020.1.045","The authors propose models for the solution of the fundamental problem of option replication subject to discrete trading, round lotting, and nonlinear transaction costs using state-of-the-art methods in deep reinforcement learning (DRL), including deep Q-learning, deep Q-learning with Pop-Art, and proximal policy optimization (PPO). Each DRL model is trained to hedge a whole range of strikes, and no retraining is needed when the user changes to another strike within the range. The models are general, allowing the user to plug in any option pricing and simulation library and then train them with no further modifications to hedge arbitrary option portfolios. Through a series of simulations, the authors show that the DRL models learn similar or better strategies as compared to delta hedging. Out of all models, PPO performs the best in terms of profit and loss, training time, and amount of data needed for training.","2020-10-31","2024-04-04 12:59:14","2024-05-17 06:53:47","2024-04-04 12:59:14","44-57","","4","2","","JFDS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\T5MUQU4C\Du e.a. - 2020 - Deep Reinforcement Learning for Option Replication.pdf","","reinforcement learning; deep learning; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K2X2RYIB","journalArticle","2019","Buehler, Hans; Gonon, Lukas; Teichmann, Josef; Wood, Ben; Mohan, Baranidharan; Kochems, Jonathan","Deep Hedging: Hedging Derivatives Under Generic Market Frictions Using Reinforcement Learning","SSRN Electronic Journal","","1556-5068","10.2139/ssrn.3355706","https://www.ssrn.com/abstract=3355706","This article discusses a new application of reinforcement learning: to the problem of hedging a portfolio of “over-the-counter” derivatives under under market frictions such as trading costs and liquidity constraints. It is an extended version of our recent work [3], here using notation more common in the machine learning literature.","2019","2024-04-04 13:08:03","2024-05-17 06:53:52","2024-04-04 13:08:03","","","","","","SSRN Journal","Deep Hedging","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\PAB85JUX\Buehler e.a. - 2019 - Deep Hedging Hedging Derivatives Under Generic Ma.pdf","","reinforcement learning; deep learning; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6H2PCX6B","journalArticle","2020","Hutzenthaler, Martin; Jentzen, Arnulf; Kruse, Thomas; Nguyen, Tuan Anh; von Wurstemberger, Philippe","Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations","Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences","","1364-5021, 1471-2946","10.1098/rspa.2019.0630","http://arxiv.org/abs/1807.01212","For a long time it is well-known that high-dimensional linear parabolic partial differential equations (PDEs) can be approximated by Monte Carlo methods with a computational effort which grows polynomially both in the dimension and in the reciprocal of the prescribed accuracy. In other words, linear PDEs do not suffer from the curse of dimensionality. For general semilinear PDEs with Lipschitz coefficients, however, it remained an open question whether these suffer from the curse of dimensionality. In this paper we partially solve this open problem. More precisely, we prove in the case of semilinear heat equations with gradient-independent and globally Lipschitz continuous nonlinearities that the computational effort of a variant of the recently introduced multilevel Picard approximations grows polynomially both in the dimension and in the reciprocal of the required accuracy.","2020-12","2024-04-04 15:50:39","2024-05-17 06:53:25","2024-04-04 15:50:39","20190630","","2244","476","","Proc. R. Soc. A.","","","","","","","","","","","","","arXiv.org","","arXiv:1807.01212 [cs, math]","","C:\Users\isido\Zotero\storage\7BA94XXE\1807.html; C:\Users\isido\Zotero\storage\M52TXUY8\Hutzenthaler e.a. - 2020 - Overcoming the curse of dimensionality in the nume.pdf","","monte carlo; PDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVRKA5VG","journalArticle","2019","Peng, Zhao; Shan, Hongming; Liu, Tianyu; Pei, Xi; Zhou, Jieping; Wang, Ge; George, X","Deep learning for accelerating Monte Carlo radiation transport simulation in intensity-modulated radiation therapy","","","","","","Cancer is a primary cause of morbidity and mortality worldwide. The radiotherapy plays a more and more important role in cancer treatment. In the radiotherapy, the dose distribution maps in patient need to be calculated and evaluated for the purpose of killing tumor and protecting healthy tissue. Monte Carlo (MC) radiation transport calculation is able to account for all aspects of radiological physics within 3D heterogeneous media such as the human body and generate the dose distribution maps accurately. However, an MC calculation for doses in radiotherapy usually takes a great mass of time to achieve acceptable statistical uncertainty, impeding the MC methods from wider clinic applications. Here we introduce a convolutional neural network (CNN), termed as Monte Carlo Denoising Net (MCDNet), to achieve the acceleration of the MC dose calculations in radiotherapy, which is trained to directly predict the high-photon (noise-free) dose maps from the low-photon (noise-much) dose maps. Thirty patients with postoperative rectal cancer who accepted intensity-modulated radiation therapy (IMRT) were enrolled in this study. 3D Gamma Index Passing Rate (GIPR) is used to evaluate the performance of predicted dose maps. The experimental results demonstrate that the MCDNet can improve the GIPR of dose maps of 1×107 photons over that of 1×108 photons, yielding over 10× speed-up in terms of photon numbers used in the MC simulations of IMRT. It is of great potential to investigate the performance of this method on the other tumor sites and treatment modalities.","2019-10-17","2024-04-04 17:05:15","2024-08-12 14:34:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\HBJGPD6C\Peng e.a. - Deep learning for accelerating Monte Carlo radiati.pdf","","deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPQG2HZD","preprint","2023","Zhang, Rui; Meng, Qi; Zhu, Rongchan; Wang, Yue; Shi, Wenlei; Zhang, Shihua; Ma, Zhi-Ming; Liu, Tie-Yan","Monte Carlo Neural PDE Solver for Learning PDEs via Probabilistic Representation","","","","","http://arxiv.org/abs/2302.05104","In scenarios with limited available or high-quality data, training the function-to-function neural PDE solver in an unsupervised manner is essential. However, the efficiency and accuracy of existing methods are constrained by the properties of numerical algorithms, such as finite difference and pseudo-spectral methods, integrated during the training stage. These methods necessitate careful spatiotemporal discretization to achieve reasonable accuracy, leading to significant computational challenges and inaccurate simulations, particularly in cases with substantial spatiotemporal variations. To address these limitations, we propose the Monte Carlo Neural PDE Solver (MCNP Solver) for training unsupervised neural solvers via the PDEs’ probabilistic representation, which regards macroscopic phenomena as ensembles of random particles. Compared to other unsupervised methods, MCNP Solver naturally inherits the advantages of the Monte Carlo method, which is robust against spatiotemporal variations and can tolerate coarse step size. In simulating the random walk of particles, we employ Heun’s method for the convection process and calculate the expectation via the probability density function of neighbouring grid points during the diffusion process. These techniques enhance accuracy and circumvent the computational memory and time issues associated with Monte Carlo sampling, offering an improvement over traditional Monte Carlo methods. Our numerical experiments on convection-diffusion, Allen-Cahn, and Navier-Stokes equations demonstrate significant improvements in accuracy and efficiency compared to other unsupervised baselines. The source code will be publicly available at: https://github.com/optray/MCNP.","2023-11-23","2024-04-04 19:51:22","2024-05-17 06:50:43","2024-04-04 19:51:22","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.05104 [cs, math]","","C:\Users\isido\Zotero\storage\3FSEGW45\Zhang e.a. - 2023 - Monte Carlo Neural PDE Solver for Learning PDEs vi.pdf","","PDE; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2302.05104","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXZGNNFS","journalArticle","2017","Henry-Labordere, Pierre","Deep Primal-Dual Algorithm for BSDEs: Applications of Machine Learning to CVA and IM","SSRN Electronic Journal","","1556-5068","10.2139/ssrn.3071506","https://www.ssrn.com/abstract=3071506","Building heavily on the recent nice paper [18], we introduce a primal-dual method for solving BSDEs based on the use of neural networks, stochastic gradient descent and a dual formulation of stochastic control problems [9]. Our algorithm is illustrated with two examples relevant in Mathematical Finance: the pricing of counterparty risk and the computation of initial margin.","2017","2024-04-04 20:02:40","2024-05-17 06:50:09","2024-04-04 20:02:40","","","","","","SSRN Journal","Deep Primal-Dual Algorithm for BSDEs","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\9VZBVEVH\Henry-Labordere - 2017 - Deep Primal-Dual Algorithm for BSDEs Applications.pdf","","deep learning; BSDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DK6V9P8","journalArticle","2002","Rogers, L. C. G.","Monte Carlo valuation of American options","Mathematical Finance","","0960-1627, 1467-9965","10.1111/1467-9965.02010","https://onlinelibrary.wiley.com/doi/10.1111/1467-9965.02010","This paper introduces a dual way to price American options, based on simulating the paths of the option payoﬀ, and of a judiciously chosen Lagrangian martingale. Taking the pathwise maximum of the payoﬀ less the martingale provides an upper bound for the price of the option, and this bound is sharp for the optimal choice of Lagrangian martingale. As a ﬁrst exploration of this method, four examples are investigated numerically; the accuracy achieved with even very simple choices of Lagrangian martingale is surprising. The method also leads naturally to candidate hedging policies for the option, and estimates of the risk involved in using them.","2002-07","2024-04-04 20:31:09","2024-05-17 06:49:51","2024-04-04 20:31:09","271-286","","3","12","","Mathematical Finance","","","","","","","","en","http://onlinelibrary.wiley.com/termsAndConditions#vor","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\3EUVSBWK\Rogers - 2002 - Monte Carlo valuation of American options.pdf","","monte carlo; american option; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6EELJ6V","preprint","2018","Raissi, Maziar","Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations","","","","","http://arxiv.org/abs/1804.07010","Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions.","2018-04-19","2024-04-04 21:07:57","2024-05-17 06:49:37","2024-04-04 21:07:57","","","","","","","Forward-Backward Stochastic Neural Networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1804.07010 [cs, math, stat]","","C:\Users\isido\Zotero\storage\HWUW6MY4\1804.html; C:\Users\isido\Zotero\storage\2SNDTGW2\Raissi - 2018 - Forward-Backward Stochastic Neural Networks Deep .pdf","","PDE; deep learning; BSDE","","","","","","","","","","","","","","","","","","","","arXiv:1804.07010","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LYAYN3RX","webpage","2023","Leung, Jonathan","Exploring backward stochastic differential equations and deep learning for high-dimensional partial differential equations and European option pricing","","","","","https://mdh.diva-portal.org/smash/get/diva2:1762991/FULLTEXT01.pdf","","2023-05-31","2024-04-04 21:19:08","2024-05-17 06:49:10","2024-04-04 21:18:44","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\THHMLKH2\FULLTEXT01.pdf","","PDE; BSDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5VVFRH7","journalArticle","2010","Donnelly, Catherine; Embrechts, Paul","THE DEVIL IS IN THE TAILS: ACTUARIAL MATHEMATICS AND THE SUBPRIME MORTGAGE CRISIS","","","","","","In the aftermath of the 2007-2008 financial crisis, there has been criticism of mathematics and the mathematical models used by the finance industry. We answer these criticisms through a discussion of some of the actuarial models used in the pricing of credit derivatives. As an example, we focus in particular on the Gaussian copula model and its drawbacks. To put this discussion into its proper context, we give a synopsis of the financial crisis and a brief introduction to some of the common credit derivatives and highlight the difficulties in valuing some of them.","2010-05","2024-04-05 09:18:27","2024-08-12 14:21:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\ZBR9M92P\Donnelly en Embrechts - THE DEVIL IS IN THE TAILS ACTUARIAL MATHEMATICS A.pdf","","copulas; risk management","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVP64MAS","journalArticle","1998","Frees, Edward W.; Valdez, Emiliano A.","Understanding Relationships Using Copulas","North American Actuarial Journal","","1092-0277, 2325-0453","10.1080/10920277.1998.10595667","http://www.tandfonline.com/doi/abs/10.1080/10920277.1998.10595667","This article introduces actuaries to the concept of ‘‘copulas,’’ a tool for understanding relationships among multivariate outcomes. A copula is a function that links univariate marginals to their full multivariate distribution. Copulas were introduced in 1959 in the context of probabilistic metric spaces. The literature on the statistical properties and applications of copulas has been developing rapidly in recent years. This article explores some of these practical applications, including estimation of joint life mortality and multidecrement models. In addition, we describe basic properties of copulas, their relationships to measures of dependence, and several families of copulas that have appeared in the literature. An annotated bibliography provides a resource for researchers and practitioners who wish to continue their study of copulas. For those who wish to use copulas for statistical inference, we illustrate statistical inference procedures by using insurance company data on losses and expenses. For these data, we (1) show how to ﬁt copulas and (2) describe their usefulness by pricing a reinsurance contract and estimating expenses for pre-speciﬁed losses.","1998-01","2024-04-05 09:17:52","2024-05-17 06:48:47","2024-04-05 09:17:52","1-25","","1","2","","North American Actuarial Journal","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\5KE5E94Y\Frees en Valdez - 1998 - Understanding Relationships Using Copulas.pdf","","copulas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNQCIDXA","journalArticle","2021","Jiang, Yifan; Li, Jinfeng","Convergence of the Deep BSDE method for FBSDEs with non-Lipschitz coefficients","Probability, Uncertainty and Quantitative Risk","","2095-9672, 2367-0126","10.3934/puqr.2021019","http://arxiv.org/abs/2101.01869","This paper is dedicated to solving high-dimensional coupled FBSDEs with non-Lipschitz diffusion coefficients numerically. Under mild conditions, we provided a posterior estimate of the numerical solution that holds for any time duration. This posterior estimate validates the convergence of the recently proposed Deep BSDE method. In addition, we developed a numerical scheme based on the Deep BSDE method and presented numerical examples in financial markets to demonstrate the high performance.","2021","2024-04-06 11:51:49","2024-05-17 06:48:36","2024-04-06 11:51:48","391","","4","6","","PUQR","","","","","","","","","","","","","arXiv.org","","arXiv:2101.01869 [cs, math]","Comment: 19 pages, 2 figures","C:\Users\isido\Zotero\storage\CN2GIDE7\2101.html; C:\Users\isido\Zotero\storage\ID92WC9A\Jiang en Li - 2021 - Convergence of the Deep BSDE method for FBSDEs wit.pdf","","BSDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XFF6U65","journalArticle","2022","Glau, Kathrin; Wunderlich, Linus","The deep parametric PDE method and applications to option pricing","Applied Mathematics and Computation","","00963003","10.1016/j.amc.2022.127355","https://linkinghub.elsevier.com/retrieve/pii/S0096300322004295","","2022-11","2024-04-06 20:54:40","2024-05-17 06:48:30","2024-04-06 20:54:40","127355","","","432","","Applied Mathematics and Computation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\37XNY24B\Glau en Wunderlich - 2022 - The deep parametric PDE method and applications to.pdf","","deep learning; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUQJUJNP","journalArticle","2003","Detemple, Jerome","Nonlinear Option Pricing Derivatives American-Style","","","","","","","2003-05","2024-04-06 21:00:35","2024-08-12 14:18:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\W2PLGWVL\Derivatives en Detemple - Nonlinear Option Pricing.pdf","","BSDE; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4I8VXB2","journalArticle","2018","Shanmugam, Ramalingam","Elements of causal inference: foundations and learning algorithms","Journal of Statistical Computation and Simulation","","0094-9655, 1563-5163","10.1080/00949655.2018.1505197","https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197","Causality is a fascinating topic of research. Its mathematization has only relatively recently started, and many conceptual problems are still being debated — often with considerable intensity. While this book summarizes the results of spending a decade assaying causality, others have studied this problem much longer than we have, and there already exist books about causality, including the comprehensive treatments of Pearl [2009], Spirtes et al. [2000], and Imbens and Rubin [2015]. We hope that our book is able to complement existing work in two ways. First, the present book represents a bias toward a subproblem of causality that may be considered both the most fundamental and the least realistic. This is the cause-effect problem, where the system under analysis contains only two observables. We have studied this problem in some detail during the last decade. We report much of this work, and try to embed it into a larger context of what we consider fundamental for gaining a selective but profound understanding of the issues of causality. Although it might be instructive to study the bivariate case first, following the sequential chapter order, it is also possible to directly start reading the multivariate chapters; see Figure I. And second, our treatment is motivated and influenced by the fields of machine learning and computational statistics. We are interested in how methods thereof can help with the inference of causal structures, and even more so whether causal reasoning can inform the way we should be doing machine learning. Indeed, we feel that some of the most profound open issues of machine learning are best understood if we do not take a random experiment described by a probability distribution as our starting point, but instead we consider causal structures underlying the distribution. We try to provide a systematic introduction into the topic that is accessible to readers familiar with the basics of probability theory and statistics or machin xii Preface learning (for completeness, the most important concepts are summarized in Appendices A.1 and A.2). While we build on the graphical approach to causality as represented by the work of Pearl [2009] and Spirtes et al. [2000], our personal taste influenced the choice of topics. To keep the book accessible and focus on the conceptual issues, we were forced to devote regrettably little space to a number of significant issues in causality, be it advanced theoretical insights for particular settings or various methods of practical importance. We have tried to include references to the literature for some of the most glaring omissions, but we may have missed important topics. Our book has a number of shortcomings. Some of them are inherited from the field, such as the tendency that theoretical results are often restricted to the case where we have infinite amounts of data. Although we do provide algorithms and methodology for the finite data case, we do not discuss statistical properties of such methods. Additionally, at some places we neglect measure theoretic issues, often by assuming the existence of densities. We find all of these questions both relevant and interesting but made these choices to keep the book short and accessible to a broad audience. Another disclaimer is in order. Computational causality methods are still in their infancy, and in particular, learning causal structures from data is only doable in rather limited situations. We have tried to include concrete algorithms wherever possible, but we are acutely aware that many of the problems of causal inference are harder than typical machine learning problems, and we thus make no promises as to whether the algorithms will work on the reader’s problems. Please do not feel discouraged by this remark — causal learning is a fascinating topic and we hope that reading this book may convince you to start working on it. We would have not been able to finish this book without the support of various people. We gratefully acknowledge support for a Research in Pairs stay of the three authors at the Mathematisches Forschungsinstitut Oberwolfach, during which a substantial part of this book was written. We thank Michel Besserve, Peter B ̈ uhlmann, Rune Christiansen, Frederick Eberhardt, Jan Ernest, Philipp Geiger, Niels Richard Hansen, Alain Hauser, Biwei Huang, Marek Kaluba, Hansruedi K ̈ unsch, Steffen Lauritzen, Jan Lemeire, David Lopez-Paz, Marloes Maathuis, Nicolai Meinshausen, Søren Wengel Mogensen, Joris Mooij, Krikamol Muandet, Judea Pearl, Niklas Pfister, Thomas Richardson, Mateo Rojas-Carulla, Eleni Sgouritsa, Carl Johann Simon-Gabriel, Xiaohai Sun, Ilya Tolstikhin, Kun Zhang, and Jakob Zscheischler for many helpful comments and interesting discussions during the time this book was written. In particular,","2018-11-02","2024-04-06 21:06:05","2024-05-17 06:47:35","2024-04-06 21:06:05","3248-3248","","16","88","","Journal of Statistical Computation and Simulation","Elements of causal inference","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\JHGT5TEE\Shanmugam - 2018 - Elements of causal inference foundations and lear.PDF","","causal ml","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QL8S2LCC","preprint","2023","Assabumrungrat, Rawin; Minami, Kentaro; Hirano, Masanori","Error Analysis of Option Pricing via Deep PDE Solvers: Empirical Study","","","","","http://arxiv.org/abs/2311.07231","Option pricing, a fundamental problem in finance, often requires solving non-linear partial differential equations (PDEs). When dealing with multi-asset options, such as rainbow options, these PDEs become high-dimensional, leading to challenges posed by the curse of dimensionality. While deep learning-based PDE solvers have recently emerged as scalable solutions to this high-dimensional problem, their empirical and quantitative accuracy remains not well-understood, hindering their real-world applicability. In this study, we aimed to offer actionable insights into the utility of Deep PDE solvers for practical option pricing implementation. Through comparative experiments, we assessed the empirical performance of these solvers in high-dimensional contexts. Our investigation identified three primary sources of errors in Deep PDE solvers: (i) errors inherent in the specifications of the target option and underlying assets, (ii) errors originating from the asset model simulation methods, and (iii) errors stemming from the neural network training. Through ablation studies, we evaluated the individual impact of each error source. Our results indicate that the Deep BSDE method (DBSDE) is superior in performance and exhibits robustness against variations in option specifications. In contrast, some other methods are overly sensitive to option specifications, such as time to expiration. We also find that the performance of these methods improves inversely proportional to the square root of batch size and the number of time steps. This observation can aid in estimating computational resources for achieving desired accuracies with Deep PDE solvers.","2023-11-13","2024-04-06 21:23:04","2024-05-17 06:47:00","2024-04-06 21:23:04","","","","","","","Error Analysis of Option Pricing via Deep PDE Solvers","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2311.07231 [cs, math, q-fin]","Comment: 11 pages, 6 figures","C:\Users\isido\Zotero\storage\V8HA6FXJ\2311.html; C:\Users\isido\Zotero\storage\WWMAJGWG\Assabumrungrat e.a. - 2023 - Error Analysis of Option Pricing via Deep PDE Solv.pdf","","deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2311.07231","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QX72FUHF","preprint","2019","Chen, Yangang; Wan, Justin W. L.","Deep Neural Network Framework Based on Backward Stochastic Differential Equations for Pricing and Hedging American Options in High Dimensions","","","","","http://arxiv.org/abs/1909.11532","We propose a deep neural network framework for computing prices and deltas of American options in high dimensions. The architecture of the framework is a sequence of neural networks, where each network learns the difference of the price functions between adjacent timesteps. We introduce the least squares residual of the associated backward stochastic differential equation as the loss function. Our proposed framework yields prices and deltas on the entire spacetime, not only at a given point. The computational cost of the proposed approach is quadratic in dimension, which addresses the curse of dimensionality issue that state-of-the-art approaches suffer. Our numerical simulations demonstrate these contributions, and show that the proposed neural network framework outperforms state-of-the-art approaches in high dimensions.","2019-09-25","2024-04-06 21:29:37","2024-05-17 06:46:48","2024-04-06 21:29:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.11532 [cs, q-fin, stat]","Comment: 35 pages, 11 figures, 15 tables","C:\Users\isido\Zotero\storage\IFYUNVDJ\1909.html; C:\Users\isido\Zotero\storage\P49LVVAB\Chen en Wan - 2019 - Deep Neural Network Framework Based on Backward St.pdf","","deep learning; BSDE; option pricing","","","","","","","","","","","","","","","","","","","","arXiv:1909.11532","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2P8WYJ75","preprint","2020","Huré, Côme; Pham, Huyên; Warin, Xavier","Deep backward schemes for high-dimensional nonlinear PDEs","","","","","http://arxiv.org/abs/1902.01599","We propose new machine learning schemes for solving high dimensional nonlinear partial differential equations (PDEs). Relying on the classical backward stochastic differential equation (BSDE) representation of PDEs, our algorithms estimate simultaneously the solution and its gradient by deep neural networks. These approximations are performed at each time step from the minimization of loss functions defined recursively by backward induction. The methodology is extended to variational inequalities arising in optimal stopping problems. We analyze the convergence of the deep learning schemes and provide error estimates in terms of the universal approximation of neural networks. Numerical results show that our algorithms give very good results till dimension 50 (and certainly above), for both PDEs and variational inequalities problems. For the PDEs resolution, our results are very similar to those obtained by the recent method in \cite{weinan2017deep} when the latter converges to the right solution or does not diverge. Numerical tests indicate that the proposed methods are not stuck in poor local minimaas it can be the case with the algorithm designed in \cite{weinan2017deep}, and no divergence is experienced. The only limitation seems to be due to the inability of the considered deep neural networks to represent a solution with a too complex structure in high dimension.","2020-06-05","2024-04-06 21:58:29","2024-05-17 06:46:39","2024-04-06 21:58:28","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.01599 [cs, math, stat]","Comment: 34 pages","C:\Users\isido\Zotero\storage\23AHB6QC\1902.html; C:\Users\isido\Zotero\storage\CYZBAHZ4\Huré e.a. - 2020 - Deep backward schemes for high-dimensional nonline.pdf","","PDE; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:1902.01599","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8D5CAJQ4","preprint","2023","Murray, Riley; Demmel, James; Mahoney, Michael W.; Erichson, N. Benjamin; Melnichenko, Maksim; Malik, Osman Asif; Grigori, Laura; Luszczek, Piotr; Dereziński, Michał; Lopes, Miles E.; Liang, Tianyu; Luo, Hengrui; Dongarra, Jack","Randomized Numerical Linear Algebra : A Perspective on the Field With an Eye to Software","","","","","http://arxiv.org/abs/2302.11474","Randomized numerical linear algebra - RandNLA, for short - concerns the use of randomization as a resource to develop improved algorithms for large-scale linear algebra computations. The origins of contemporary RandNLA lay in theoretical computer science, where it blossomed from a simple idea: randomization provides an avenue for computing approximate solutions to linear algebra problems more efficiently than deterministic algorithms. This idea proved fruitful in the development of scalable algorithms for machine learning and statistical data analysis applications. However, RandNLA's true potential only came into focus upon integration with the fields of numerical analysis and ""classical"" numerical linear algebra. Through the efforts of many individuals, randomized algorithms have been developed that provide full control over the accuracy of their solutions and that can be every bit as reliable as algorithms that might be found in libraries such as LAPACK. Recent years have even seen the incorporation of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA's cuSOLVER, and SciKit-Learn. For all its success, we believe that RandNLA has yet to realize its full potential. In particular, we believe the scientific community stands to benefit significantly from suitably defined ""RandBLAS"" and ""RandLAPACK"" libraries, to serve as standards conceptually analogous to BLAS and LAPACK. This 200-page monograph represents a step toward defining such standards. In it, we cover topics spanning basic sketching, least squares and optimization, low-rank approximation, full matrix decompositions, leverage score sampling, and sketching data with tensor product structures (among others). Much of the provided pseudo-code has been tested via publicly available MATLAB and Python implementations.","2023-04-12","2024-04-09 15:21:18","2024-05-17 06:45:47","2024-04-09 15:21:18","","","","","","","Randomized Numerical Linear Algebra","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.11474 [cs, math]","Comment: v1: this is the first arXiv release of LAPACK Working Note 299. v2: complete rewrite of the subsection on trace estimation, among other changes. See frontmatter page ii (pdf page 5) for revision history","C:\Users\isido\Zotero\storage\IW5826S9\2302.html; C:\Users\isido\Zotero\storage\24P857HE\Murray e.a. - 2023 - Randomized Numerical Linear Algebra  A Perspectiv.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","arXiv:2302.11474","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VY7S4NA","preprint","2019","Sirignano, Justin; Spiliopoulos, Konstantinos","Stochastic Gradient Descent in Continuous Time: A Central Limit Theorem","","","","","http://arxiv.org/abs/1710.04273","Stochastic gradient descent in continuous time (SGDCT) provides a computationally efficient method for the statistical learning of continuous-time models, which are widely used in science, engineering, and finance. The SGDCT algorithm follows a (noisy) descent direction along a continuous stream of data. The parameter updates occur in continuous time and satisfy a stochastic differential equation. This paper analyzes the asymptotic convergence rate of the SGDCT algorithm by proving a central limit theorem (CLT) for strongly convex objective functions and, under slightly stronger conditions, for non-convex objective functions as well. An $L^{p}$ convergence rate is also proven for the algorithm in the strongly convex case. The mathematical analysis lies at the intersection of stochastic analysis and statistical learning.","2019-06-17","2024-04-09 17:12:39","2024-05-17 07:01:09","2024-04-09 17:12:39","","","","","","","Stochastic Gradient Descent in Continuous Time","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1710.04273 [math, q-fin, stat]","","C:\Users\isido\Zotero\storage\VNYHISGR\1710.html; C:\Users\isido\Zotero\storage\ELRUZ9H8\Sirignano en Spiliopoulos - 2019 - Stochastic Gradient Descent in Continuous Time A .pdf","","gradient descent; SGD","","","","","","","","","","","","","","","","","","","","arXiv:1710.04273","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XTLJG6M","preprint","2019","Fujii, Masaaki; Takahashi, Akihiko; Takahashi, Masayuki","Asymptotic Expansion as Prior Knowledge in Deep Learning Method for high dimensional BSDEs","","","","","http://arxiv.org/abs/1710.07030","We demonstrate that the use of asymptotic expansion as prior knowledge in the ""deep BSDE solver"", which is a deep learning method for high dimensional BSDEs proposed by Weinan E, Han & Jentzen (2017), drastically reduces the loss function and accelerates the speed of convergence. We illustrate the technique and its implications by using Bergman's model with different lending and borrowing rates as a typical model for FVA as well as a class of solvable BSDEs with quadratic growth drivers. We also present an extension of the deep BSDE solver for reflected BSDEs representing American option prices.","2019-03-05","2024-04-09 17:22:13","2024-05-17 06:45:33","2024-04-09 17:22:13","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1710.07030 [q-fin]","Comment: Forthcoming in APFM","C:\Users\isido\Zotero\storage\2MJ7BW8Q\1710.html; C:\Users\isido\Zotero\storage\HYU366T2\Fujii e.a. - 2019 - Asymptotic Expansion as Prior Knowledge in Deep Le.pdf","","deep learning; BSDE","","","","","","","","","","","","","","","","","","","","arXiv:1710.07030","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXGBVB9A","journalArticle","2021","Liang, Jian; Xu, Zhe; Li, Peter","Deep learning-based least squares forward-backward stochastic differential equation solver for high-dimensional derivative pricing","Quantitative Finance","","1469-7688, 1469-7696","10.1080/14697688.2021.1881149","https://www.tandfonline.com/doi/full/10.1080/14697688.2021.1881149","We propose a new forward-backward stochastic differential equation solver for high-dimensional derivative pricing problems by combining a deep learning solver with a least squares regression technique widely used in the least squares Monte Carlo method for the valuation of American options. Our numerical experiments demonstrate the accuracy of our least squares backward deep neural network solver and its capability to produce accurate prices for complex early exercisable derivatives, such as callable yield notes. Our method can serve as a generic numerical solver for pricing derivatives across various asset groups, in particular, as an accurate means for pricing high-dimensional derivatives with early exercise features.","2021-08-03","2024-04-09 17:34:16","2024-05-17 06:46:06","2024-04-09 17:34:16","1309-1323","","8","21","","Quantitative Finance","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\IMQ7W4IR\Liang e.a. - 2021 - Deep learning-based least squares forward-backward.pdf","","deep learning; BSDE; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFLXDSEY","journalArticle","2023","Na, Andrew; Wan, Justin","Efficient Pricing and Hedging of High Dimensional American Options Using Recurrent Networks","Quantitative Finance","","1469-7688, 1469-7696","10.1080/14697688.2023.2167666","http://arxiv.org/abs/2301.08232","We propose a deep Recurrent neural network (RNN) framework for computing prices and deltas of American options in high dimensions. Our proposed framework uses two deep RNNs, where one network learns the price and the other learns the delta of the option for each timestep. Our proposed framework yields prices and deltas for the entire spacetime, not only at a given point (e.g. t = 0). The computational cost of the proposed approach is linear in time, which improves on the quadratic time seen for feedforward networks that price American options. The computational memory cost of our method is constant in memory, which is an improvement over the linear memory costs seen in feedforward networks. Our numerical simulations demonstrate these contributions, and show that the proposed deep RNN framework is computationally more efficient than traditional feedforward neural network frameworks in time and memory.","2023-04-03","2024-04-09 17:37:05","2024-05-17 06:44:51","2024-04-09 17:37:05","631-651","","4","23","","Quantitative Finance","","","","","","","","","","","","","arXiv.org","","arXiv:2301.08232 [cs, q-fin]","","C:\Users\isido\Zotero\storage\EAT2PC49\2301.html; C:\Users\isido\Zotero\storage\3DVRHA48\Na en Wan - 2023 - Efficient Pricing and Hedging of High Dimensional .pdf","","deep learning; american option; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JYQPMP2R","preprint","2024","Woo, Gerald; Liu, Chenghao; Kumar, Akshat; Xiong, Caiming; Savarese, Silvio; Sahoo, Doyen","Unified Training of Universal Time Series Forecasting Transformers","","","","","http://arxiv.org/abs/2402.02592","Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.","2024-02-04","2024-04-14 12:00:11","2024-04-14 12:00:11","2024-04-14 12:00:11","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.02592 [cs]","","C:\Users\isido\Zotero\storage\726J92WH\2402.html; C:\Users\isido\Zotero\storage\TCGD7UC3\Woo e.a. - 2024 - Unified Training of Universal Time Series Forecast.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2402.02592","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZHBA8PX","preprint","2018","Zheng, Xun; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.","DAGs with NO TEARS: Continuous Optimization for Structure Learning","","","","","http://arxiv.org/abs/1803.01422","Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely \emph{continuous} optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.","2018-11-02","2024-04-15 16:22:11","2024-05-17 06:44:18","2024-04-15 16:22:11","","","","","","","DAGs with NO TEARS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1803.01422 [cs, stat]","Comment: 22 pages, 8 figures, accepted to NIPS 2018","C:\Users\isido\Zotero\storage\HD9EAE6Q\1803.html; C:\Users\isido\Zotero\storage\T8MYTJJJ\Zheng e.a. - 2018 - DAGs with NO TEARS Continuous Optimization for St.pdf","","causal ml; causal discovery","","","","","","","","","","","","","","","","","","","","arXiv:1803.01422","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKDTXCJQ","preprint","2020","Zheng, Xun; Dan, Chen; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.","Learning Sparse Nonparametric DAGs","","","","","http://arxiv.org/abs/1909.13189","We develop a framework for learning sparse nonparametric directed acyclic graphs (DAGs) from data. Our approach is based on a recent algebraic characterization of DAGs that led to a fully continuous program for score-based learning of DAG models parametrized by a linear structural equation model (SEM). We extend this algebraic characterization to nonparametric SEM by leveraging nonparametric sparsity based on partial derivatives, resulting in a continuous optimization problem that can be applied to a variety of nonparametric and semiparametric models including GLMs, additive noise models, and index models as special cases. Unlike existing approaches that require specific modeling choices, loss functions, or algorithms, we present a completely general framework that can be applied to general nonlinear models (e.g. without additive noise), general differentiable loss functions, and generic black-box optimization routines. The code is available at https://github.com/xunzheng/notears.","2020-03-23","2024-04-15 17:27:45","2024-05-17 06:44:09","2024-04-15 17:27:45","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.13189 [cs, stat]","Comment: To appear in AISTATS 2020","C:\Users\isido\Zotero\storage\UTLPUYBL\1909.html; C:\Users\isido\Zotero\storage\VTUVRQWL\Zheng e.a. - 2020 - Learning Sparse Nonparametric DAGs.pdf","","causal ml; causal discovery","","","","","","","","","","","","","","","","","","","","arXiv:1909.13189","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKU3BMWX","preprint","2023","Bello, Kevin; Aragam, Bryon; Ravikumar, Pradeep","DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization","","","","","http://arxiv.org/abs/2209.08037","The combinatorial problem of learning directed acyclic graphs (DAGs) from data was recently framed as a purely continuous optimization problem by leveraging a differentiable acyclicity characterization of DAGs based on the trace of a matrix exponential function. Existing acyclicity characterizations are based on the idea that powers of an adjacency matrix contain information about walks and cycles. In this work, we propose a new acyclicity characterization based on the log-determinant (log-det) function, which leverages the nilpotency property of DAGs. To deal with the inherent asymmetries of a DAG, we relate the domain of our log-det characterization to the set of $\textit{M-matrices}$, which is a key difference to the classical log-det function defined over the cone of positive definite matrices. Similar to acyclicity functions previously proposed, our characterization is also exact and differentiable. However, when compared to existing characterizations, our log-det function: (1) Is better at detecting large cycles; (2) Has better-behaved gradients; and (3) Its runtime is in practice about an order of magnitude faster. From the optimization side, we drop the typically used augmented Lagrangian scheme and propose DAGMA ($\textit{DAGs via M-matrices for Acyclicity}$), a method that resembles the central path for barrier methods. Each point in the central path of DAGMA is a solution to an unconstrained problem regularized by our log-det function, then we show that at the limit of the central path the solution is guaranteed to be a DAG. Finally, we provide extensive experiments for $\textit{linear}$ and $\textit{nonlinear}$ SEMs and show that our approach can reach large speed-ups and smaller structural Hamming distances against state-of-the-art methods. Code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/dagma.","2023-01-15","2024-04-15 17:54:32","2024-05-17 06:43:53","2024-04-15 17:54:32","","","","","","","DAGMA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.08037 [cs, stat]","Comment: 28 pages, 13 figures, published at NeurIPS 2022","C:\Users\isido\Zotero\storage\C2J378N8\2209.html; C:\Users\isido\Zotero\storage\7Z4D62WN\Bello e.a. - 2023 - DAGMA Learning DAGs via M-matrices and a Log-Dete.pdf","","causal ml; causal discovery","","","","","","","","","","","","","","","","","","","","arXiv:2209.08037","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H6XZN67H","journalArticle","2020","Sawhney, Rohan; Crane, Keenan","Monte Carlo geometry processing: a grid-free approach to PDE-based methods on volumetric domains","ACM Transactions on Graphics","","0730-0301","10.1145/3386569.3392374","https://dl.acm.org/doi/10.1145/3386569.3392374","This paper explores how core problems in PDE-based geometry processing can be efficiently and reliably solved via grid-free Monte Carlo methods. Modern geometric algorithms often need to solve Poisson-like equations on geometrically intricate domains. Conventional methods most often mesh the domain, which is both challenging and expensive for geometry with fine details or imperfections (holes, self-intersections, etc.). In contrast, grid-free Monte Carlo methods avoid mesh generation entirely, and instead just evaluate closest point queries. They hence do not discretize space, time, nor even function spaces, and provide the exact solution (in expectation) even on extremely challenging models. More broadly, they share many benefits with Monte Carlo methods from photorealistic rendering: excellent scaling, trivial parallel implementation, view-dependent evaluation, and the ability to work with any kind of geometry (including implicit or procedural descriptions). We develop a complete ""black box"" solver that encompasses integration, variance reduction, and visualization, and explore how it can be used for various geometry processing tasks. In particular, we consider several fundamental linear elliptic PDEs with constant coefficients on solid regions of Rn. Overall we find that Monte Carlo methods significantly broaden the horizons of geometry processing, since they easily handle problems of size and complexity that are essentially hopeless for conventional methods.","2020-08-12","2022-09-17 18:54:45","2024-07-28 20:31:05","2024-04-17 08:53:31","123:123:1–123:123:18","","4","39","","ACM Trans. Graph.","Monte Carlo geometry processing","","","","","","","","","","","","ACM Digital Library","","","","C:\Users\isido\Zotero\storage\BL2GKML3\Sawhney en Crane - 2020 - Monte Carlo geometry processing a grid-free appro.pdf; C:\Users\isido\Zotero\storage\J5MXL5PE\Sawhney en Crane - Monte Carlo Geometry ProcessingA Grid-Free Approa.pdf","","monte carlo; PDE; walk on spheres; rendering; ♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MH96AZXZ","conferencePaper","2023","Li, Zilu; Yang, Guandao; Deng, Xi; De Sa, Christopher; Hariharan, Bharath; Marschner, Steve","Neural Caches for Monte Carlo Partial Differential Equation Solvers","SIGGRAPH Asia 2023 Conference Papers","9798400703157","","10.1145/3610548.3618141","https://dl.acm.org/doi/10.1145/3610548.3618141","This paper presents a method that uses neural networks as a caching mechanism to reduce the variance of Monte Carlo Partial Differential Equation solvers, such as the Walk-on-Spheres algorithm [Sawhney and Crane 2020]. While these Monte Carlo PDE solvers have the merits of being unbiased and discretization-free, their high variance often hinders real-time applications. On the other hand, neural networks can approximate the PDE solution, and evaluating these networks at inference time can be very fast. However, neural-network-based solutions may suffer from convergence difficulties and high bias. Our hybrid system aims to combine these two potentially complementary solutions by training a neural field to approximate the PDE solution using supervision from a WoS solver. This neural field is then used as a cache in the WoS solver to reduce variance during inference. We demonstrate that our neural field training procedure is better than the commonly used self-supervised objectives in the literature. We also show that our hybrid solver exhibits lower variance than WoS with the same computational budget: it is significantly better for small compute budgets and rovides smaller improvements for larger budgets, reaching the same performance as WoS in the limit.","2023-12-10","2024-04-17 09:15:24","2024-05-17 06:43:40","2024-04-17 09:15:24","1-10","","","","","","","","","","","ACM","Sydney NSW Australia","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\WY67JZDR\Li e.a. - 2023 - Neural Caches for Monte Carlo Partial Differential.pdf","","monte carlo; walk on spheres; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SA '23: SIGGRAPH Asia 2023","","","","","","","","","","","","","","",""
"TFP59J3D","book","2013","Sato, Ken-iti","Lévy processes and infinitely divisible distributions","","978-0-521-55302-5 978-1-107-65649-9","","","","Stochastic processes are mathematical models of random phenomena in time evolution. Lévy processes are stochastic processes whose increments in nonoverlapping time intervals are independent and whose increments are stationary in time. Further we assume a weak continuity called stochastic continuity. They constitute a fundamental class of stochastic processes. Brownian motion, Poisson processes, and stable processes are typical Lévy processes. After Paul Lévy's characterization in the 1930s of all processes in this class, many researches have revealed properties of their distributions and behaviors of their sample functions. However, Lévy processes are rich mathematical objects, still furnishing attractive problems of their own. On the other hand, important classes of stochastic processes are obtained as generalizations of the class of Lévy processes. One of them is the class of Markov processes; another is the class of semimartingales. The study of Lévy processes serves as the foundation for the study of stochastic processes. Dropping the stationarity requirement of increments for Lévy processes, we get the class of additive processes. The distributions of Lévy and additive processes at any time are infinitely divisible, that is, they have the nth roots in the convolution sense for any n. When a time is fixed, the class of Lévy processes is in one-to-one correspondence with the class of infinitely divisible distributions. Additive processes are described by systems of infinitely divisible distributions. This book is intended to provide comprehensive basic knowledge of Lévy processes, additive processes, and infinitely divisible distributions with detailed proofs and, at the same time, to serve as an introduction to stochastic processes. As we deal with the simplest stochastic processes, we do not assume any knowledge of stochastic processes with a continuous parameter. Prerequisites for this book are of the level of the textbook of Billingsley [27] or that of Chung [70]. Making an additional assumption of selfsixnilarity or some extensions of it on Lévy or additive processes, we get certain important processes. Such are stable processes, semi-stable processes, and selfsimilar additive processes. We give them systematic study. Correspondingly, stable, semistable, and selfdecomposable distributions are treated. On the other hand PREFACE the class of Levy processes contains processes quite different from selfsimilar, and intriguing time evolution in distributional properties appears. There are ten chapters in this book. They can be divided into three parts. Chapters 1 and 2 constitute the basic part. Essential examples and a major tool for the analysis are given in Chapter 1. The tool is to consider Fourier transforms of probability measures, called characteristic functions. Then, in Chapter 2, characterization of all infinitely divisible distributions is given. They give description of all Lévy processes and also of all additive processes. Chapters 3, 4, and 5 are the second part. They develop fundamental results on which subsequent chapters rely. Chapter 3 introduces selfsimilarity and other structures. Chapter 4 deals with decomposition of sample functions into jumps and continuous motions. Chapter 5 is on distributional properties. The third part ranges from Chapter 6 to Chapter 10. They are nearly independent of each other and treat major topics on Lévy processes such as subordination and density transformation, recurrence and transience, potential theory, Wiener-Hopf factorizations, and unimodality and multimodality. We do not touch extensions of Lévy processes and infinitely divisible distributions connected with Lie groups, hypergroups, and generalized convolutions. There are many applications of Lévy processes to stochastic integrals, branching processes, and measure-valued processes, but they are not included in this book. Risk theory, queueing theory, and stochastic finance are active fields where Lévy processes often appear. The original version of this book is Kahou katei written in Japanese, published by Kinokuniya at the end of 1990. The book is enlarged and material is rewritten. Many recent advances are included and a new chapter on potential theory is added. Exercises are now given to each chapter and their solutions are at the end of the volume. For many years I have been happy in collaborating with Makoto Yamazato and Toshiro Watanabe. I was encouraged by Takeyuki Hida and Hiroshi Kunita to write the original Japanese book and the present book. Frank Knight and Toshiro Watanabe read through the manuscript and gave me numerous suggestions for correction of errors and improvement of presentation. Kazuyuki Inoue, Mamoru Kanda, Makoto Maejima, Yumiko Sato, Masaaki Tsuchiya, and Makoto Yamazato pointed out many inaccuracies to be eliminated. Part of the book was presented in lectures at the University of Zurich [405] as arranged by Masao Nagasawa. The preparation of this book was made in AMSLaTeX; Shinta Sato assisted me with the computer. My heartfelt thanks go to all of them.","2013","2024-04-19 17:23:26","2024-05-17 06:42:45","","","521","","","","","","Cambridge studies in advanced mathematics","68","","","Cambridge Univ. Press","Cambridge","en","","","","","K10plus ISBN","","","","C:\Users\isido\Zotero\storage\IJHG775C\Sato - 2013 - Lévy processes and infinitely divisible distributi.PDF","","","","","","","","","","","","","","","","","","","","","","","Revisted edition, corrected paperback edition","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2YA3HRL","bookSection","2002","Embrechts, Paul; McNeil, Alexander J.; Straumann, Daniel","Correlation and Dependence in Risk Management: Properties and Pitfalls","Risk Management","978-0-521-78180-0 978-0-511-61533-7 978-0-521-16963-9","","","https://www.cambridge.org/core/product/identifier/CBO9780511615337A013/type/book_part","Modern risk management calls for an understanding of stochastic dependence going beyond simple linear correlation. This paper deals with the static (non-time-dependent) case and emphasizes the copula representation of dependence for a random vector. Linear correlation is a natural dependence measure for multivariate normally and, more generally, elliptically distributed risks but other dependence concepts like comonotonicity and rank correlation should also be understood by the risk management practitioner. Using counterexamples the falsity of some commonly held views on correlation is demonstrated; in general, these fallacies arise from the naive assumption that dependence properties of the elliptical world also hold in the non-elliptical world. In particular, the problem of ﬁnding multivariate models which are consistent with prespeciﬁed marginal distributions and correlations is addressed. Pitfalls are highlighted and simulation algorithms avoiding these problems are constructed.","2002-01-10","2024-04-21 17:01:24","2024-05-17 06:41:50","2024-04-21 17:01:24","176-223","","","","","","Correlation and Dependence in Risk Management","","","","","Cambridge University Press","","en","https://www.cambridge.org/core/terms","","","","DOI.org (Crossref)","","DOI: 10.1017/CBO9780511615337.008","","C:\Users\isido\Zotero\storage\YP8KJDET\Embrechts e.a. - 2002 - Correlation and Dependence in Risk Management Pro.pdf","","risk management","","Dempster, M. A. H.","","","","","","","","","","","","","","","","","","","1","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2MTB9LH","preprint","2024","Tian, Keyu; Jiang, Yi; Yuan, Zehuan; Peng, Bingyue; Wang, Liwei","Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","","","","","http://arxiv.org/abs/2404.02905","We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine ""next-scale prediction"" or ""next-resolution prediction"", diverging from the standard raster-scan ""next-token prediction"". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.","2024-04-03","2024-04-22 07:42:16","2024-05-17 06:41:38","2024-04-22 07:42:16","","","","","","","Visual Autoregressive Modeling","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2404.02905 [cs]","","C:\Users\isido\Zotero\storage\YUM2Y2CH\2404.html; C:\Users\isido\Zotero\storage\H95KN5WT\Tian e.a. - 2024 - Visual Autoregressive Modeling Scalable Image Gen.pdf","","machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2404.02905","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S7YBWIGX","journalArticle","2021","Müller, Thomas; Rousselle, Fabrice; Novák, Jan; Keller, Alexander","Real-time neural radiance caching for path tracing","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3450626.3459812","https://dl.acm.org/doi/10.1145/3450626.3459812","We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve               generalization via adaptation               , i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead---about 2.6ms on full HD resolution---thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.","2021-08-31","2024-04-22 15:42:22","2024-08-08 12:54:19","2024-04-22 15:42:22","1-16","","4","40","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\CX5ED8I9\Müller e.a. - 2021 - Real-time neural radiance caching for path tracing.pdf","","monte carlo; rendering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4FRN2JC","preprint","2020","Sitzmann, Vincent; Martel, Julien N. P.; Bergman, Alexander W.; Lindell, David B.; Wetzstein, Gordon","Implicit Neural Representations with Periodic Activation Functions","","","","","http://arxiv.org/abs/2006.09661","Implicitly deﬁned, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible beneﬁts over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with ﬁne detail, and fail to represent a signal’s spatial and temporal derivatives, despite the fact that these are essential to many physical signals deﬁned implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, waveﬁelds, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.","2020-06-17","2024-04-22 15:56:40","2024-05-17 06:41:20","2024-04-22 15:56:40","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2006.09661 [cs, eess]","Comment: Project website: https://vsitzmann.github.io/siren/ Project video: https://youtu.be/Q2fLWGBeaiI","C:\Users\isido\Zotero\storage\S3KC8JWP\Sitzmann e.a. - 2020 - Implicit Neural Representations with Periodic Acti.pdf","","rendering; machine learning; deep learning","","","","","","","","","","","","","","","","","","","","arXiv:2006.09661","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95FZXDVG","journalArticle","2024","Xu, Zhihao; Zhou, Tiankuang; Ma, Muzhou; Deng, ChenChen; Dai, Qionghai; Fang, Lu","Large-scale photonic chiplet Taichi empowers 160-TOPS/W artificial general intelligence","Science","","0036-8075, 1095-9203","10.1126/science.adl1203","https://www.science.org/doi/10.1126/science.adl1203","The pursuit of artificial general intelligence (AGI) continuously demands higher computing performance. Despite the superior processing speed and efficiency of integrated photonic circuits, their capacity and scalability are restricted by unavoidable errors, such that only simple tasks and shallow models are realized. To support modern AGIs, we designed Taichi—large-scale photonic chiplets based on an integrated diffractive-interference hybrid design and a general distributed computing architecture that has millions-of-neurons capability with 160–tera-operations per second per watt (TOPS/W) energy efficiency. Taichi experimentally achieved on-chip 1000-category–level classification (testing at 91.89% accuracy in the 1623-category Omniglot dataset) and high-fidelity artificial intelligence–generated content with up to two orders of magnitude of improvement in efficiency. Taichi paves the way for large-scale photonic computing and advanced tasks, further exploiting the flexibility and potential of photonics for modern AGI.           ,              Editor’s summary                            Rapid advances in artificial general intelligence (AGI) come with increased performance and energy efficiency requirements for next-generation computing. Photonic computing has the potential to achieve these goals, but despite attracting attention, current photonic integrated circuits, specifically optical neural networks (ONNs), have limited scale and computing capabilities, barely supporting modern AGI tasks. Xu               et al               . explored a distributed diffractive-interference hybrid photonic computing architecture to effectively increase the scale of the ONN to the million-neuron level. They experimentally realized an on-chip 13.96-million-neuron ONN for complex, thousand-category-level classification and AI-generated content tasks. The present work is a promising step toward real-world photonic computing, supporting various applications in AI. —Yury Suleymanov","2024-04-12","2024-04-30 08:49:23","2024-05-17 06:41:07","2024-04-30 08:49:23","202-209","","6692","384","","Science","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\85JL8JH6\Xu e.a. - 2024 - Large-scale photonic chiplet Taichi empowers 160-T.pdf","","analog","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RMH6TPG9","journalArticle","2006","Drineas, Petros; Kannan, Ravi; Mahoney, Michael W.","Fast Monte Carlo Algorithms for Matrices I: Approximating Matrix Multiplication","SIAM Journal on Computing","","0097-5397, 1095-7111","10.1137/S0097539704442684","http://epubs.siam.org/doi/10.1137/S0097539704442684","","2006-01","2024-05-01 13:13:39","2024-05-17 06:40:57","2024-05-01 13:13:39","132-157","","1","36","","SIAM J. Comput.","Fast Monte Carlo Algorithms for Matrices I","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\V75MTWVA\matrix1_SICOMP.pdf","","monte carlo; random linear algebra","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2CQKABI6","journalArticle","2024","Derezinski, Michał; Mahoney, Michael W","Recent and Upcoming Developments in Randomized Numerical Linear Algebra for ML","","","","","","","2024-06-17","2024-05-01 13:40:19","2024-08-12 13:32:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\BQDA9DE9\Derezinski en Mahoney - Recent and Upcoming Developments in Randomized Num.pdf","","random linear algebra","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6CND27U","preprint","2009","Candes, Emmanuel J.; Li, Xiaodong; Ma, Yi; Wright, John","Robust Principal Component Analysis?","","","","","http://arxiv.org/abs/0912.3599","This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the 1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it oﬀers a principled way of removing shadows and specularities in images of faces.","2009-12-18","2024-05-07 20:32:09","2024-05-17 06:40:27","2024-05-07 20:32:09","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:0912.3599 [cs, math]","","C:\Users\isido\Zotero\storage\KAGLBYK9\Candes e.a. - 2009 - Robust Principal Component Analysis.pdf","","machine learning; dimension reduction","","","","","","","","","","","","","","","","","","","","arXiv:0912.3599","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XLVCZ36","journalArticle","2014","Moro, Sérgio; Cortez, Paulo; Rita, Paulo","A data-driven approach to predict the success of bank telemarketing","Decision Support Systems","","01679236","10.1016/j.dss.2014.03.001","https://linkinghub.elsevier.com/retrieve/pii/S016792361400061X","We propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits. A Portuguese retail bank was addressed, with data collected from 2008 to 2013, thus including the eﬀects of the recent ﬁnancial crisis. We analyzed a large set of 150 features related with bank client, product and social-economic attributes. A semi-automatic feature selection was explored in the modeling phase, performed with the data prior to July 2012 and that allowed to select a reduced set of 22 features. We also compared four DM models: logistic regression, decision trees (DT), neural network (NN) and support vector machine. Using two metrics, area of the receiver operating characteristic curve (AUC) and area of the LIFT cumulative curve (ALIFT), the four models were tested on an evaluation phase, using the most recent data (after July 2012) and a rolling windows scheme. The NN presented the best results (AUC=0.8 and ALIFT=0.7), allowing to reach 79% of the subscribers by selecting the half better classiﬁed clients. Also, two knowledge extraction methods, a sensitivity analysis and a DT, were applied to the NN model and revealed several key attributes (e.g., Euribor rate, direction of the call and bank agent experience). Such knowledge extraction conﬁrmed the obtained model as credible and valuable for telemarketing campaign managers.","2014-06","2024-05-08 11:56:40","2024-05-08 11:56:40","2024-05-08 11:56:40","22-31","","","62","","Decision Support Systems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\FSG5JIKQ\Moro e.a. - 2014 - A data-driven approach to predict the success of b.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G24V6C23","preprint","2022","Cheng, Lu; Guo, Ruocheng; Moraffah, Raha; Sheth, Paras; Candan, K. Selcuk; Liu, Huan","Evaluation Methods and Measures for Causal Learning Algorithms","","","","","http://arxiv.org/abs/2202.02896","The convenient access to copious multi-faceted data has encouraged machine learning researchers to reconsider correlation-based learning and embrace the opportunity of causality-based learning, i.e., causal machine learning (causal learning). Recent years have therefore witnessed great effort in developing causal learning algorithms aiming to help AI achieve human-level intelligence. Due to the lack-of ground-truth data, one of the biggest challenges in current causal learning research is algorithm evaluations. This largely impedes the cross-pollination of AI and causal inference, and hinders the two ﬁelds to beneﬁt from the advances of the other. To bridge from conventional causal inference (i.e., based on statistical methods) to causal learning with big data (i.e., the intersection of causal inference and machine learning), in this survey, we review commonly-used datasets, evaluation methods, and measures for causal learning using an evaluation pipeline similar to conventional machine learning. We focus on the two fundamental causal-inference tasks and causality-aware machine learning tasks. Limitations of current evaluation procedures are also discussed. We then examine popular causal inference tools/packages and conclude with primary challenges and opportunities for benchmarking causal learning algorithms in the era of big data. The survey seeks to bring to the forefront the urgency of developing publicly available benchmarks and consensus-building standards for causal learning evaluation with observational data. In doing so, we hope to broaden the discussions and facilitate collaboration to advance the innovation and application of causal learning.","2022-02-06","2024-05-12 07:02:11","2024-05-17 06:40:06","2024-05-12 07:02:11","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2202.02896 [cs, stat]","Comment: 21 pages. Accepted to IEEE TAI","C:\Users\isido\Zotero\storage\2YQBN7BY\Cheng e.a. - 2022 - Evaluation Methods and Measures for Causal Learnin.pdf","","causal ml","","","","","","","","","","","","","","","","","","","","arXiv:2202.02896","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44FQDJ2Y","preprint","2022","Langen, Henrika; Huber, Martin","How causal machine learning can leverage marketing strategies: Assessing and improving the performance of a coupon campaign","","","","","http://arxiv.org/abs/2204.10820","We apply causal machine learning algorithms to assess the causal eﬀect of a marketing intervention, namely a coupon campaign, on the sales of a retailer. Besides assessing the average impacts of diﬀerent types of coupons, we also investigate the heterogeneity of causal eﬀects across diﬀerent subgroups of customers, e.g., between clients with relatively high vs. low prior purchases. Finally, we use optimal policy learning to determine (in a data-driven way) which customer groups should be targeted by the coupon campaign in order to maximize the marketing intervention’s eﬀectiveness in terms of sales. We ﬁnd that only two out of the ﬁve coupon categories examined, namely coupons applicable to the product categories of drugstore items and other food, have a statistically signiﬁcant positive eﬀect on retailer sales. The assessment of group average treatment eﬀects reveals substantial diﬀerences in the impact of coupon provision across customer groups, particularly across customer groups as deﬁned by prior purchases at the store, with drugstore coupons being particularly eﬀective among customers with high prior purchases and other food coupons among customers with low prior purchases. Our study provides a use case for the application of causal machine learning in business analytics to evaluate the causal impact of speciﬁc ﬁrm policies (like marketing campaigns) for decision support.","2022-06-22","2024-05-12 07:18:20","2024-05-17 06:40:00","2024-05-12 07:18:20","","","","","","","How causal machine learning can leverage marketing strategies","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2204.10820 [econ, q-fin, stat]","","C:\Users\isido\Zotero\storage\2RKN2RKS\Langen en Huber - 2022 - How causal machine learning can leverage marketing.pdf","","causal ml","","","","","","","","","","","","","","","","","","","","arXiv:2204.10820","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AWLNQJ9","preprint","2016","Lopez-Paz, David","From Dependence to Causation","","","","","http://arxiv.org/abs/1607.03300","Machine learning is the science of discovering statistical dependencies in data, and the use of those dependencies to perform predictions. During the last decade, machine learning has made spectacular progress, surpassing human performance in complex tasks such as object recognition, car driving, and computer gaming. However, the central role of prediction in machine learning avoids progress towards general-purpose artiﬁcial intelligence. As one way forward, we argue that causal inference is a fundamental component of human intelligence, yet ignored by learning algorithms.","2016-07-12","2024-05-12 08:18:03","2024-05-17 06:39:46","2024-05-12 08:18:03","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1607.03300 [stat]","Comment: PhD thesis","C:\Users\isido\Zotero\storage\9DHMDX9A\Lopez-Paz - 2016 - From Dependence to Causation.pdf","","causal ml; density estimation","","","","","","","","","","","","","","","","","","","","arXiv:1607.03300","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKV3DWBP","preprint","2018","Goudet, Olivier; Kalainathan, Diviyan; Caillou, Philippe; Guyon, Isabelle; Lopez-Paz, David; Sebag, Michèle","Causal Generative Neural Networks","","","","","http://arxiv.org/abs/1711.08936","We present Causal Generative Neural Networks (CGNNs) to learn functional causal models from observational data. CGNNs leverage conditional independencies and distributional asymmetries to discover bivariate and multivariate causal structures. CGNNs make no assumption regarding the lack of confounders, and learn a differentiable generative model of the data by using backpropagation. Extensive experiments show their good performances comparatively to the state of the art in observational causal discovery on both simulated and real data, with respect to cause-effect inference, v-structure identiﬁcation, and multivariate causal discovery.","2018-02-05","2024-05-12 11:01:30","2024-05-17 06:39:37","2024-05-12 11:01:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1711.08936 [stat]","","C:\Users\isido\Zotero\storage\HXIJG5DG\Goudet e.a. - 2018 - Causal Generative Neural Networks.pdf","","deep learning; causal ml","","","","","","","","","","","","","","","","","","","","arXiv:1711.08936","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I3TQLX8E","preprint","2024","Zabërgja, Guri; Kadra, Arlind; Grabocka, Josif","Tabular Data: Is Attention All You Need?","","","","","http://arxiv.org/abs/2402.03970","Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradientboosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.","2024-02-06","2024-05-13 08:23:03","2024-05-17 06:39:22","2024-05-13 08:23:03","","","","","","","Tabular Data","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2402.03970 [cs]","","C:\Users\isido\Zotero\storage\Y7KLBCMN\Zabërgja e.a. - 2024 - Tabular Data Is Attention All You Need.pdf","","machine learning; deep learning; transformer","","","","","","","","","","","","","","","","","","","","arXiv:2402.03970","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P4JSJHKF","book","2006","Wasserman, Larry","All of nonparametric statistics","","978-0-387-25145-5","","","","There are many books on various aspects of nonparametric inference such as density estimation, nonparametric regression, bootstrapping, and wavelets methods. But it is hard to find all these topics covered in one place. The goal of this text is to provide readers with a single book where they can find a brief account of many of the modern topics in nonparametric inference. The book is aimed at master’s-level or Ph.D.-level statistics and computer science students. It is also suitable for researchers in statistics, machine learning and data mining who want to get up to speed quickly on modern nonparametric methods. My goal is to quickly acquaint the reader with the basic concepts in many areas rather than tackling any one topic in great detail. In the interest of covering a wide range of topics, while keeping the book short, I have opted to omit most proofs. Bibliographic remarks point the reader to references that contain further details. Of course, I have had to choose topics to include and to omit, the title notwithstanding. For the most part, I decided to omit topics that are too big to cover in one chapter. For example, I do not cover classification or nonparametric Bayesian inference. The book developed from my lecture notes for a half-semester (20 hours) course populated mainly by master’s-level students. For Ph.D.-level students, the instructor may want to cover some of the material in more depth and require the students to fill in proofs of some of the theorems. Throughout, I have attempted to follow one basic principle: never give an estimator without giving a confidence set viii Preface The book has a mixture of methods and theory. The material is meant to complement more method-oriented texts such as Hastie et al. (2001) and Ruppert et al. (2003). After the Introduction in Chapter 1, Chapters 2 and 3 cover topics related to the empirical cdf such as the nonparametric delta method and the bootstrap. Chapters4to6coverbasicsmoothingmethods.Chapters7to9haveahigher theoretical content and are more demanding. The theory in Chapter 7 lays the foundation for the orthogonal function methods in Chapters 8 and 9. Chapter 10 surveys some of the omitted topics. I assume that the reader has had a course in mathematical statistics such as Casella and Berger (2002) or Wasserman (2004). In particular, I assume that the following concepts are familiar to the reader: distribution functions, convergence in probability, convergence in distribution, almost sure convergence, likelihood functions, maximum likelihood, confidence intervals, the delta method, bias, mean squared error, and Bayes estimators. These background concepts are reviewed briefly in Chapter 1. Data sets and code can be found at: www.stat.cmu.edu/∼larry/all-of-nonpar I need to make some disclaimers. First, the topics in this book fall under the rubric of “modern nonparametrics.” The omission of traditional methods such as rank tests and so on is not intended to belittle their importance. Second, I make heavy use of large-sample methods. This is partly because I think that statistics is, largely, most successful and useful in large-sample situations, and partly because it is often easier to construct large-sample, nonparametric methods. The reader should be aware that large-sample methods can, of course, go awry when used without appropriate caution. I would like to thank the following people for providing feedback and suggestions: Larry Brown, Ed George, John Lafferty, Feng Liang, Catherine Loader, Jiayang Sun, and Rob Tibshirani. Special thanks to some readers who provided very detailed comments: Taeryon Choi, Nils Hjort, Woncheol Jang, Chris Jones, Javier Rojo, David Scott, and one anonymous reader. Thanks also go to my colleague Chris Genovese for lots of advice and for writing the LATEX macros for the layout of the book. I am indebted to John Kimmel, who has been supportive and helpful and did not rebel against the crazy title. Finally, thanks to my wife Isabella Verdinelli for suggestions that improved the book and for her love and support.","2006","2024-05-13 08:44:39","2024-05-17 06:39:04","","","276","","","","","","Springer texts in statistics","","","","Springer","New York","en","","","","","Library of Congress ISBN","QA278.8 .W37 2006","","","C:\Users\isido\Zotero\storage\CQMTEZXK\Wasserman - 2006 - All of nonparametric statistics.pdf","","statistics; nonparametric statistics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BH4IJBQ","preprint","2023","Neykov, Matey; Wasserman, Larry; Kim, Ilmun; Balakrishnan, Sivaraman","Nearly Minimax Optimal Wasserstein Conditional Independence Testing","","","","","http://arxiv.org/abs/2308.08672","This paper is concerned with minimax conditional independence testing. In contrast to some previous works on the topic, which use the total variation distance to separate the null from the alternative, here we use the Wasserstein distance. In addition, we impose Wasserstein smoothness conditions which on bounded domains are weaker than the corresponding total variation smoothness imposed, for instance, by Neykov et al. [2021]. This added flexibility expands the distributions which are allowed under the null and the alternative to include distributions which may contain point masses for instance. We characterize the optimal rate of the critical radius of testing up to logarithmic factors. Our test statistic which nearly achieves the optimal critical radius is novel, and can be thought of as a weighted multi-resolution version of the U -statistic studied by Neykov et al. [2021].","2023-08-16","2024-05-13 10:00:59","2024-05-17 06:37:36","2024-05-13 10:00:59","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2308.08672 [math, stat]","Comment: 24 pages, 1 figure, ordering of the last three authors is random","C:\Users\isido\Zotero\storage\8FF2D47Y\2308.pdf","","statistics; indpendence testing","","","","","","","","","","","","","","","","","","","","arXiv:2308.08672","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KF7YT9GS","preprint","2023","Park, Beomjo; Balakrishnan, Sivaraman; Wasserman, Larry","Robust Universal Inference","","","","","http://arxiv.org/abs/2307.04034","In statistical inference, it is rarely realistic that the hypothesized statistical model is well-specified, and consequently it is important to understand the effects of misspecification on inferential procedures. When the hypothesized statistical model is misspecified, the natural target of inference is a projection of the data generating distribution onto the model. We present a general method for constructing valid confidence sets for such projections, under weak regularity conditions, despite possible model misspecification. Our method builds upon the universal inference method of Wasserman et al. [41] and is based on inverting a family of split-sample tests of relative fit. We study settings in which our methods yield either exact or approximate, finite-sample valid confidence sets for various projection distributions. We study rates at which the resulting confidence sets shrink around the target of inference and complement these results with a simulation study.","2023-07-08","2024-05-13 10:43:49","2024-05-17 06:37:16","2024-05-13 10:43:49","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2307.04034 [stat]","Comment: 37 pages, 11 figures","C:\Users\isido\Zotero\storage\XVFL34LT\Park e.a. - 2023 - Robust Universal Inference.pdf","","statistics; robust statistics","","","","","","","","","","","","","","","","","","","","arXiv:2307.04034","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ZRQU53T","preprint","2022","Kim, Ilmun; Neykov, Matey; Balakrishnan, Sivaraman; Wasserman, Larry","Local permutation tests for conditional independence","","","","","http://arxiv.org/abs/2112.11666","In this paper, we investigate local permutation tests for testing conditional independence between two random vectors X and Y given Z. The local permutation test determines the signiﬁcance of a test statistic by locally shuﬄing samples which share similar values of the conditioning variables Z, and it forms a natural extension of the usual permutation approach for unconditional independence testing. Despite its simplicity and empirical support, the theoretical underpinnings of the local permutation test remain unclear. Motivated by this gap, this paper aims to establish theoretical foundations of local permutation tests with a particular focus on binning-based statistics. We start by revisiting the hardness of conditional independence testing and provide an upper bound for the power of any valid conditional independence test, which holds when the probability of observing “collisions” in Z is small. This negative result naturally motivates us to impose additional restrictions on the possible distributions under the null and alternate. To this end, we focus our attention on certain classes of smooth distributions and identify provably tight conditions under which the local permutation method is universally valid, i.e. it is valid when applied to any (binning-based) test statistic. To complement this result on type I error control, we also show that in some cases, a binning-based statistic calibrated via the local permutation method can achieve minimax optimal power. We also introduce a double-binning permutation strategy, which yields a valid test over less smooth null distributions than the typical single-binning method without compromising much power. Finally, we present simulation results to support our theoretical ﬁndings.","2022-01-06","2024-05-13 10:47:35","2024-05-17 06:36:56","2024-05-13 10:47:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2112.11666 [math, stat]","Comment: A few important references (missed before) added","C:\Users\isido\Zotero\storage\7BT5CGXH\Kim e.a. - 2022 - Local permutation tests for conditional independen.pdf","","statistics; indpendence testing","","","","","","","","","","","","","","","","","","","","arXiv:2112.11666","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PULLRUSN","preprint","2021","Kennedy, Edward H.; Balakrishnan, Sivaraman; Wasserman, Larry","Semiparametric counterfactual density estimation","","","","","http://arxiv.org/abs/2102.12034","Causal eﬀects are often characterized with averages, which can give an incomplete picture of the underlying counterfactual distributions. Here we consider estimating the entire counterfactual density and generic functionals thereof. We focus on two kinds of target parameters. The ﬁrst is a density approximation, deﬁned by a projection onto a ﬁnite-dimensional model using a generalized distance metric, which includes f -divergences as well as Lp norms. The second is the distance between counterfactual densities, which can be used as a more nuanced eﬀect measure than the mean diﬀerence, and as a tool for model selection. We study nonparametric eﬃciency bounds for these targets, giving results for smooth but otherwise generic models and distances. Importantly, we show how these bounds connect to means of particular non-trivial functions of counterfactuals, linking the problems of density and mean estimation. We go on to propose doubly robuststyle estimators for the density approximations and distances, and study their rates of convergence, showing they can be optimally eﬃcient in large nonparametric models. We also give analogous methods for model selection and aggregation, when many models may be available and of interest. Our results all hold for generic models and distances, but throughout we highlight what happens for particular choices, such as L2 projections on linear models, and KL projections on exponential families. Finally we illustrate by estimating the density of CD4 count among patients with HIV, had all been treated with combination therapy versus zidovudine alone, as well as a density eﬀect. Our results suggest combination therapy may have increased CD4 count most for high-risk patients. Our methods are implemented in the freely available R package npcausal on GitHub.","2021-02-23","2024-05-13 10:53:47","2024-05-17 06:36:42","2024-05-13 10:53:47","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2102.12034 [math, stat]","","C:\Users\isido\Zotero\storage\2QMCFSZW\Kennedy e.a. - 2021 - Semiparametric counterfactual density estimation.pdf","","causal ml; density estimation","","","","","","","","","","","","","","","","","","","","arXiv:2102.12034","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9YRFF9J","book","1998","Higham, Nicholas J.","Handbook of writing for the mathematical sciences","","978-0-89871-420-3","","","","In this book I aim to describe most of what a scientist needs to know about mathematical writing. Although the emphasis is strongly on mathematical writing, many of the points and issues I discuss are relevant to scientific writing in general. My main target audience is graduate students. They often have little experience or knowledge of technical writing and are daunted by the task of writing a report or thesis. The advice given here reflects what I have learned in the ten years since I wrote my first research report as a graduate student and describes what I would have liked to know as I started to write that first report. I hope that as well as being a valuable resource for graduate students, this book will also be of use to practising scientists. The book has grown out of notes for a short lecture course on mathematical writing that I gave at the University of Manchester in May 1992. As I prepared the course I realized that, although several excellent articles and books on mathematical writing are available (notably those by Halmos (1970) [121], Gillman (1987) [104] and Knuth, Larrabee and Roberts (1989) [164]), none functions as a comprehensive handbook that can be both read sequentially and used as a reference when questions about mathematical writing arise. I have attempted to provide such a handbook. (I hope that the comment of one journal referee, ""This paper fills a much needed gap in the literature"", is not applicable to this book!) As well as covering standard topics such as English usage, the anatomy of a research paper, and revising a draft. I examine in detail four topics that are usually discussed only briefly, if at all, in books on technical writing. • The whole publication process, from submission of a manuscript to its appearance as a paper in a journal. • Writing when English is a foreign language. • How to write slides for a talk. • The use of computers in writing and research. In particular, I discuss modern practices such as computerized typesetting in T^X, the use of computer tools for indexing and checking spelling and style, and electronic mail and ftp (file transfer protocol). x xvi PREFACE TO THE FIRST EDITION An important feature of the book is that many examples are given to illustrate the ideas and principles discussed. In particular, Chapter 7 contains a collection of extracts from the mathematics and computer science literature, with detailed comments on how each extract can be improved. In writing the book I have been helped and influenced by many people. Several people read the entire manuscript at one or more of its various stages, offered constructive suggestions, encouragement and advice, and made sure I said what I meant and meant what I said. They are Ian Gladwell, Des Higham, Doris Higham, Nil Mackey, Fred Schneider, Pete (G. W.) Stewart and Nick Trefethen. Other people who have read portions of the book and have given help, suggestions or advice are Carl de Boor, David Carlisle, Valerie Fraysse, Paul Halmos, Bo Kagstrom, Philip Knight, Sven Leyffer, Steve Mackey, Volker Mehrmann, June O'Brien, Pythagoras Papadimitriou, Beresford Parlett, Nigel Ray, Stephan Rudolfer. Bob Sandling, Zdenek Strakos, Gil Strang, Charlie Van Loan, Rossana Vermiglio, Joan Walsh, Barry White, and Yuanjing Xu. I thank all these people, together with many others who have answered my questions and made suggestions. In researching the contents of the book I was inspired by many of the references listed in the bibliography, and learned a lot from them. I acknowledge the Nuffield Foundation for the support of a Nuffield Science Research Fellowship, during the tenure of which I wrote much of the book. Last, but not least, I thank the SIAM staff for their help and advice in the production of the book—in particular, Susan Ciambrano, Beth Gallagher and Tricia Manning. I would be happy to receive comments, notification of errors, and suggestions for improvement, which I will collect for inclusion in a possible second edition. This book was typeset in I^T^X using the book document style with G. W. Stewart's jeep style option. I prepared the references with BlBTEX and the index with Makelndex. I used text editors Qedit (Semware) and GNU Emacs (Free Software Foundation), and checked spelling with PCWrite (Quicksoft).","1998","2024-05-13 16:15:34","2024-05-17 06:36:18","","","302","","","","","","Other titles in applied mathematics","63","","","SIAM","Philadelphia","en","","","","","K10plus ISBN","","","","C:\Users\isido\Zotero\storage\CDFIP8SY\Higham - 1998 - Handbook of writing for the mathematical sciences.pdf","","","","","","","","","","","","","","","","","","","","","","","2. ed","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKH8V6VR","journalArticle","2002","Mäkelä, Marko","Survey of Bundle Methods for Nonsmooth Optimization","Optimization Methods and Software","","1055-6788, 1029-4937","10.1080/10556780290027828","https://www.tandfonline.com/doi/full/10.1080/10556780290027828","Bundle methods are at the moment the most efficient and promising methods for nonsmooth optimization. They have been successfully used in many practical applications, for example, in economics, mechanics, engineering and optimal control. The aim of this paper is to give an overview of the development and history of the bundle methods from the seventies to the present. For simplicity, we first concentrate on the convex unconstrained case with a single objective function. The methods are later extended to nonconvex, constrained and multicriteria cases.","2002-01","2024-05-14 18:15:06","2024-05-17 06:35:19","2024-05-14 18:15:06","1-29","","1","17","","Optimization Methods and Software","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\KWY4CZLE\Mäkelä - 2002 - Survey of Bundle Methods for Nonsmooth Optimizatio.pdf","","nonsmooth optimization; bundle method","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6QIFIHF","preprint","2018","Burke, James V.; Curtis, Frank E.; Lewis, Adrian S.; Overton, Michael L.; Simões, Lucas E. A.","Gradient Sampling Methods for Nonsmooth Optimization","","","","","http://arxiv.org/abs/1804.11003","This paper reviews the gradient sampling methodology for solving nonsmooth, nonconvex optimization problems. An intuitively straightforward gradient sampling algorithm is stated and its convergence properties are summarized. Throughout this discussion, we emphasize the simplicity of gradient sampling as an extension of the steepest descent method for minimizing smooth objectives. We then provide overviews of various enhancements that have been proposed to improve practical performance, as well as of several extensions that have been made in the literature, such as to solve constrained problems. The paper also includes clariﬁcation of certain technical aspects of the analysis of gradient sampling algorithms, most notably related to the assumptions one needs to make about the set of points at which the objective is continuously diﬀerentiable. Finally, we discuss possible future research directions.","2018-04-29","2024-05-14 18:52:43","2024-05-17 06:34:44","2024-05-14 18:52:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1804.11003 [math]","Comment: Submitted to: Special Methods for Nonsmooth Optimization (Springer, 2018), edited by A. Bagirov, M. Gaudioso, N. Karmitsa and M. M\""akel\""a","C:\Users\isido\Zotero\storage\GNATPVYJ\Burke e.a. - 2018 - Gradient Sampling Methods for Nonsmooth Optimizati.pdf","","nonsmooth optimization","","","","","","","","","","","","","","","","","","","","arXiv:1804.11003","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZIAMCZT","journalArticle","2018","Elgindy, Kareem T.","Optimization via Chebyshev Polynomials","Journal of Applied Mathematics and Computing","","1598-5865, 1865-2085","10.1007/s12190-016-1076-x","http://arxiv.org/abs/1603.00863","This paper presents for the ﬁrst time a robust exact line-search method based on a full pseudospectral (PS) numerical scheme employing orthogonal polynomials. The proposed method takes on an adaptive search procedure and combines the superior accuracy of Chebyshev PS approximations with the high-order approximations obtained through Chebyshev PS differentiation matrices (CPSDMs). In addition, the method exhibits quadratic convergence rate by enforcing an adaptive Newton search iterative scheme. A rigorous error analysis of the proposed method is presented along with a detailed set of pseudocodes for the established computational algorithms. Several numerical experiments are conducted on one- and multi-dimensional optimization test problems to illustrate the advantages of the proposed strategy.","2018-02","2024-05-14 19:39:39","2024-05-17 06:34:30","2024-05-14 19:39:39","317-349","","1-2","56","","J. Appl. Math. Comput.","","","","","","","","en","","","","","arXiv.org","","arXiv:1603.00863 [math]","Comment: 26 pages, 6 figures, 2 tables","C:\Users\isido\Zotero\storage\X9SG6B7E\Elgindy - 2018 - Optimization via Chebyshev Polynomials.pdf","","optimization; chebychev","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCUREMAK","journalArticle","2022","Gaudioso, Manlio; Giallombardo, Giovanni; Miglionico, Giovanna","Essentials of numerical nonsmooth optimization","Annals of Operations Research","","0254-5330, 1572-9338","10.1007/s10479-021-04498-y","https://link.springer.com/10.1007/s10479-021-04498-y","Approximately sixty years ago two seminal ﬁndings, the cutting plane and the subgradient methods, radically changed the landscape of mathematical programming. They provided, for the ﬁrst time, the practical chance to optimize real functions of several variables characterized by kinks, namely by discontinuities in their derivatives. Convex functions, for which a superb body of theoretical research was growing in parallel, naturally became the main application ﬁeld of choice. The aim of the paper is to give a concise survey of the key ideas underlying successive development of the area, which took the name of numerical nonsmooth optimization. The focus will be, in particular, on the research mainstreams generated under the impulse of the two initial discoveries.","2022-07","2024-05-14 20:30:18","2024-05-17 06:34:18","2024-05-14 20:30:18","213-253","","1","314","","Ann Oper Res","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\BU8LS4TC\Gaudioso e.a. - 2022 - Essentials of numerical nonsmooth optimization.pdf","","nonsmooth optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V989LRXC","preprint","2024","Czekanski, Michael; Faber, Benjamin; Fairborn, Margaret; Wright, Adelle; Bindel, David","Walking on Spheres and Talking to Neighbors: Variance Reduction for Laplace's Equation","","","","","http://arxiv.org/abs/2404.17692","Walk on Spheres algorithms leverage properties of Brownian Motion to create Monte Carlo estimates of solutions to a class of elliptic partial differential equations. Until recently, estimates were constructed pointwise and did not utilize the relationship between solutions at nearby points within a domain. We propose a new caching strategy which leverages the continuity of paths of Brownian Motion. In the case of Laplace’s equation with Dirichlet boundary conditions, our novel algorithm has improved asymptotic runtime compared to previous approaches. This is achieved by passing information from a cache of constant size. We also provide bounds on the performance of our algorithm and demonstrate its performance on an example problem.","2024-04-26","2024-05-16 13:39:58","2024-05-17 06:34:06","2024-05-16 13:39:58","","","","","","","Walking on Spheres and Talking to Neighbors","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.17692 [physics]","Comment: 21 pages, 7 figures","C:\Users\isido\Zotero\storage\MFVZSLW6\Czekanski e.a. - 2024 - Walking on Spheres and Talking to Neighbors Varia.pdf","","monte carlo; PDE; walk on spheres","","","","","","","","","","","","","","","","","","","","arXiv:2404.17692","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGYQ76K8","webpage","2024","van Dijk, N.M","On uniformization for nonhomogeneous Markov chains","","","","","https://research.vu.nl/ws/portalfiles/portal/73611171/Scanjob+199100006","The Standard method of uniformization for continuous-time Markov chains is shown to be generalizable to time-inhomogeneous Markov chains. A finite grid approximation is also provided.","2024-05-18","2024-05-18 06:27:35","2024-08-12 14:41:08","2024-05-18 06:27:03","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\W36BR9D9\Scanjob+199100006.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKI6XREK","journalArticle","2016","Schwarz, Justus Arne; Selinka, Gregor; Stolletz, Raik","Performance analysis of time-dependent queueing systems: Survey and classification","Omega","","03050483","10.1016/j.omega.2015.10.013","https://linkinghub.elsevier.com/retrieve/pii/S0305048315002170","Many queueing systems are subject to time-dependent changes in system parameters, such as the arrival rate or number of servers. Examples include time-dependent call volumes and agents at inbound call centers, time-varying air trafﬁc at airports, time-dependent truck arrival rates at seaports, and cyclic message volumes in computer systems.","2016-09","2024-05-18 09:09:36","2024-05-18 09:09:36","2024-05-18 09:09:36","170-189","","","63","","Omega","Performance analysis of time-dependent queueing systems","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\7ZVQQSGZ\Schwarz e.a. - 2016 - Performance analysis of time-dependent queueing sy.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U65269HP","journalArticle","2018","Themelis, Andreas; Stella, Lorenzo; Patrinos, Panagiotis","Forward-backward envelope for the sum of two nonconvex functions: Further properties and nonmonotone line-search algorithms","SIAM Journal on Optimization","","1052-6234, 1095-7189","10.1137/16M1080240","http://arxiv.org/abs/1606.06256","We propose ZeroFPR, a nonmonotone linesearch algorithm for minimizing the sum of two nonconvex functions, one of which is smooth and the other possibly nonsmooth. ZeroFPR is the ﬁrst algorithm that, despite being ﬁt for fully nonconvex problems and requiring only the black-box oracle of forward-backward splitting (FBS) — namely evaluations of the gradient of the smooth term and of the proximity operator of the nonsmooth one — achieves superlinear convergence rates under mild assumptions at the limit point when the linesearch directions satisfy a Dennis-Moré condition, and we show that this is the case for quasi-Newton directions. Our approach is based on the forward-backward envelope (FBE), an exact and strictly continuous penalty function for the original cost. Extending previous results we show that, despite being nonsmooth for fully nonconvex problems, the FBE still enjoys favorable ﬁrst- and second-order properties which are key for the convergence results of ZeroFPR. Our theoretical results are backed up by promising numerical simulations. On large-scale problems, by computing linesearch directions using limited-memory quasi-Newton updates our algorithm greatly outperforms FBS and its accelerated variant (AFBS).","2018-01","2024-05-21 19:31:18","2024-05-21 19:31:19","2024-05-21 19:31:18","2274-2303","","3","28","","SIAM J. Optim.","Forward-backward envelope for the sum of two nonconvex functions","","","","","","","en","","","","","arXiv.org","","arXiv:1606.06256 [math]","","C:\Users\isido\Zotero\storage\WCS9K2JZ\Themelis e.a. - 2018 - Forward-backward envelope for the sum of two nonco.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBYYA6VC","journalArticle","2001","Haugh, Martin B; Kogan, Leonid","Pricing American Options: A Duality Approach","","","","","","We develop a new method for pricing American options. The main practical contribution of this paper is a general algorithm for constructing upper and lower bounds on the true price of the option using any approximation to the option price. We show that our bounds are tight, so that if the initial approximation is close to the true price of the option, the bounds are also guaranteed to be close. We also explicitly characterize the worst-case performance of the pricing bounds. The computation of the lower bound is straightforward and relies on simulating the suboptimal exercise strategy implied by the approximate option price. The upper bound is also computed using Monte Carlo simulation. This is made feasible by the representation of the American option price as a solution of a properly deﬁned dual minimization problem, which is the main theoretical result of this paper. Our algorithm proves to be accurate on a set of sample problems where we price call options on the maximum and the geometric mean of a collection of stocks. These numerical results suggest that our pricing method can be successfully applied to problems of practical interest.","2001-12","2024-05-22 13:53:40","2024-08-12 14:24:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\9XGC5NE7\Haugh en Kogan - Pricing American Options A Duality Approach.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZR3HGPFV","preprint","2015","He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian","Deep Residual Learning for Image Recognition","","","","","http://arxiv.org/abs/1512.03385","Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.","2015-12-10","2024-05-22 17:31:36","2024-05-22 17:31:36","2024-05-22 17:31:36","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1512.03385 [cs]","Comment: Tech report","C:\Users\isido\Zotero\storage\MLFYSTUJ\He e.a. - 2015 - Deep Residual Learning for Image Recognition.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1512.03385","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EX8ANNEC","preprint","2015","Ioffe, Sergey; Szegedy, Christian","Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","","","","","http://arxiv.org/abs/1502.03167","Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.","2015-03-02","2024-05-22 17:57:48","2024-05-22 17:57:49","2024-05-22 17:57:48","","","","","","","Batch Normalization","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1502.03167 [cs]","","C:\Users\isido\Zotero\storage\HN3SNX9E\Ioffe en Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1502.03167","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXSK4DBE","preprint","2024","Miller, Bailey; Sawhney, Rohan; Crane, Keenan; Gkioulekas, Ioannis","Differential Walk on Spheres","","","","","http://arxiv.org/abs/2405.12964","We introduce a Monte Carlo method for evaluating derivatives of the solution to a partial differential equation (PDE) with respect to problem parameters (such as domain geometry or boundary conditions). Derivatives can be evaluated at arbitrary points without performing a global solve, or constructing a volumetric grid or mesh. The method is hence well-suited to inverse problems with complex geometry, such as PDE-constrained shape optimization. Like other walk on spheres (WoS) algorithms, our method is trivial to parallelize, and is agnostic to boundary representation (meshes, splines, implicit surfaces etc.), supporting large topological changes. We focus in particular on screened Poisson equations, which model diverse problems from scientific and geometric computing. As in differentiable rendering, we jointly estimate derivatives with respect to all parameters -- hence, cost does not grow significantly with parameter count. In practice, even noisy derivative estimates exhibit fast, stable convergence for stochastic gradient-based optimization -- as we show via examples from thermal design, shape from diffusion, and computer graphics.","2024-05-21","2024-05-23 12:29:30","2024-05-23 12:29:32","2024-05-23 12:29:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.12964 [cs]","Comment: 17 pages","C:\Users\isido\Zotero\storage\DDLAAM5A\Miller e.a. - 2024 - Differential Walk on Spheres.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2405.12964","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JQBNKVJ","preprint","2018","Pfeiffer, Gustavo T.; Sato, Yoichi","On stochastic optimization methods for Monte Carlo least-squares problems","","","","","http://arxiv.org/abs/1804.10079","This work presents stochastic optimization methods targeted at least-squares problems involving Monte Carlo integration. While the most common approach to solving these problems is to apply stochastic gradient descent (SGD) or similar methods such as AdaGrad [5] and Adam [12], which involve estimating a stochastic gradient from a small number of Monte Carlo samples computed at each iteration, we show that for this category of problems it is possible to achieve faster asymptotic convergence rates using an increasing number of samples per iteration instead, a strategy we call increasing precision (IP). We then improve pre-asymptotic convergence by introducing a hybrid approach that combines the qualities of increasing precision and otherwise “constant” precision, resulting in methods such as the IP-SGD hybrid and IP-AdaGrad hybrid, essentially by modifying their gradient estimators to have an equivalent eﬀect to increasing precision. Finally, we observe that, in some problems, incorporating a Gauss-Newton preconditioner to the IP-SGD hybrid method can provide much better convergence than employing a Quasi-Newton approach or covariance-preconditioning as in AdaGrad or Adam.","2018-04-26","2024-05-23 13:17:23","2024-05-23 13:17:23","2024-05-23 13:17:23","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1804.10079 [math]","","C:\Users\isido\Zotero\storage\I99TZJHY\Pfeiffer en Sato - 2018 - On stochastic optimization methods for Monte Carlo.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1804.10079","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADUWY4DF","journalArticle","2017","Zheng, Jing; Lin, Zhengyan; Tong, Changqing; Ye, Rendao","New methods of simulating Lévy processes","Physica A: Statistical Mechanics and its Applications","","03784371","10.1016/j.physa.2017.01.031","https://linkinghub.elsevier.com/retrieve/pii/S0378437117300201","Two new methods of simulating Lévy processes are presented in this paper. The first one is to simulate the small jump part of a Lévy process by the classical rejection method, which can generate the exact sampling. The second is to simulate the Lévy process by the compound Poisson process with corrected density at zero, it is efficient, robust, and can be used in multivariate setting.","2017-05","2024-05-27 10:26:12","2024-05-27 10:26:13","2024-05-27 10:26:12","461-466","","","473","","Physica A: Statistical Mechanics and its Applications","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\EPLHWVRY\Zheng e.a. - 2017 - New methods of simulating Lévy processes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5I4QYQWM","videoRecording","2018","Institute for Mathematical Sciences","Geometrically convergent simulation of the extrema of Levy processes","","","","","https://www.youtube.com/watch?v=QFZGc4N_tIA","Aleksandar Mijatović University of Warwick & The Alan Turing Institute, UK","2018-10-08","2024-05-27 13:31:22","2024-05-27 13:31:22","2024-05-27 13:31:22","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","55:50","","","","","","","","","","","","","","","","","","","","","","","","",""
"RA2NXY64","journalArticle","2022","Cázares, Jorge Ignacio González; Mijatović, Aleksandar; Bravo, Gerónimo Uribe","Geometrically Convergent Simulation of the Extrema of L\'{e}vy Processes","Mathematics of Operations Research","","0364-765X, 1526-5471","10.1287/moor.2021.1163","http://arxiv.org/abs/1810.11039","We develop a novel approximate simulation algorithm for the joint law of the position, the running supremum and the time of the supremum of a general Lévy process at an arbitrary ﬁnite time. We identify the law of the error in simple terms. We prove that the error decays geometrically in Lp (for any p ≥ 1) as a function of the computational cost, in contrast with the polynomial decay for the approximations available in the literature. We establish a central limit theorem and construct non-asymptotic and asymptotic conﬁdence intervals for the corresponding Monte Carlo estimator. We prove that the multilevel Monte Carlo estimator has optimal computational complexity (i.e. of order ǫ−2 if the mean squared error is at most ǫ2) for locally Lipschitz and barrier-type functions of the triplet and develop an unbiased version of the estimator. We illustrate the performance of the algorithm with numerical examples.","2022-05","2024-05-27 14:27:18","2024-05-27 14:27:19","2024-05-27 14:27:18","1141-1168","","2","47","","Mathematics of OR","","","","","","","","en","","","","","arXiv.org","","arXiv:1810.11039 [math, q-fin, stat]","Comment: Minor revision: reintroduction of the result on the scaling limits. 37 pages and 5 figures. Short presentation on: https://youtu.be/P3vHmJUCFbU","C:\Users\isido\Zotero\storage\U4PWW37E\Cázares e.a. - 2022 - Geometrically Convergent Simulation of the Extrema.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MF7RL7EI","preprint","2024","Zhang, Qinzi; Cutkosky, Ashok","Random Scaling and Momentum for Non-smooth Non-convex Optimization","","","","","http://arxiv.org/abs/2405.09742","Training neural networks requires optimizing a loss function that may be highly irregular, and in particular neither convex nor smooth. Popular training algorithms are based on stochastic gradient descent with momentum (SGDM), for which classical analysis applies only if the loss is either convex or smooth. We show that a very small modification to SGDM closes this gap: simply scale the update at each time point by an exponentially distributed random scalar. The resulting algorithm achieves optimal convergence guarantees. Intriguingly, this result is not derived by a specific analysis of SGDM: instead, it falls naturally out of a more general framework for converting online convex optimization algorithms to non-convex optimization algorithms.","2024-05-15","2024-05-27 17:32:37","2024-05-27 17:32:38","2024-05-27 17:32:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.09742 [cs, math]","","C:\Users\isido\Zotero\storage\9BBQ7AY3\Zhang en Cutkosky - 2024 - Random Scaling and Momentum for Non-smooth Non-con.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2405.09742","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6RUIB8P","preprint","2018","Oord, Aaron van den; Vinyals, Oriol; Kavukcuoglu, Koray","Neural Discrete Representation Learning","","","","","http://arxiv.org/abs/1711.00937","Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.","2018-05-30","2024-05-28 07:55:25","2024-05-28 07:55:25","2024-05-28 07:55:25","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1711.00937 [cs]","","C:\Users\isido\Zotero\storage\3CYS3THN\Oord e.a. - 2018 - Neural Discrete Representation Learning.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1711.00937","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJ6EIZHD","preprint","2024","Ju, Zeqian; Wang, Yuancheng; Shen, Kai; Tan, Xu; Xin, Detai; Yang, Dongchao; Liu, Yanqing; Leng, Yichong; Song, Kaitao; Tang, Siliang; Wu, Zhizheng; Qin, Tao; Li, Xiang-Yang; Ye, Wei; Zhang, Shikun; Bian, Jiang; He, Lei; Li, Jinyu; Zhao, Sheng","NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models","","","","","http://arxiv.org/abs/2403.03100","While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.","2024-04-23","2024-05-28 08:00:05","2024-05-28 08:00:06","2024-05-28 08:00:05","","","","","","","NaturalSpeech 3","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.03100 [cs, eess]","Comment: Achieving human-level quality and naturalness on multi-speaker datasets (e.g., LibriSpeech) in a zero-shot way","C:\Users\isido\Zotero\storage\YXEE49T5\Ju e.a. - 2024 - NaturalSpeech 3 Zero-Shot Speech Synthesis with F.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2403.03100","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNBV5THB","preprint","2022","Défossez, Alexandre; Copet, Jade; Synnaeve, Gabriel; Adi, Yossi","High Fidelity Neural Audio Compression","","","","","http://arxiv.org/abs/2210.13438","We introduce a state-of-the-art real-time, high-ﬁdelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that eﬃciently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now deﬁnes the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.","2022-10-24","2024-05-28 08:47:41","2024-05-28 08:47:42","2024-05-28 08:47:41","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.13438 [cs, eess, stat]","Comment: Preprint","C:\Users\isido\Zotero\storage\V4YCF7LL\Défossez e.a. - 2022 - High Fidelity Neural Audio Compression.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2210.13438","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBZ58WEL","preprint","2023","Cutkosky, Ashok; Mehta, Harsh; Orabona, Francesco","Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion","","","","","http://arxiv.org/abs/2302.03775","We present new algorithms for optimizing non-smooth, non-convex stochastic objectives based on a novel analysis technique. This improves the current best-known complexity for ﬁnding a (δ, ǫ)-stationary point from O(ǫ−4δ−1) stochastic gradient queries to O(ǫ−3δ−1), which we also show to be optimal. Our primary technique is a reduction from non-smooth non-convex optimization to online learning, after which our results follow from standard regret bounds in online learning. For deterministic and second-order smooth objectives, applying more advanced optimistic online learning techniques enables a new complexity of O(ǫ−1.5δ−0.5). Our techniques also recover all optimal or best-known results for ﬁnding ǫ stationary points of smooth or secondorder smooth objectives in both stochastic and deterministic settings.","2023-02-11","2024-05-28 09:12:27","2024-05-28 09:12:27","2024-05-28 09:12:27","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.03775 [cs, math, stat]","","C:\Users\isido\Zotero\storage\P5TBF5XJ\Cutkosky e.a. - 2023 - Optimal Stochastic Non-smooth Non-convex Optimizat.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.03775","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HITRS5T","videoRecording","2020","Xaxufulu","Parameter-Free Optimization Part 1","","","","","https://www.youtube.com/watch?v=UT_ziU3nIwU","ICML 2020 tutorial on parameter-free algorithms by Francesco Orabona and Ashok Cutkosky. https://parameterfree.com/icml-tutorial/","2020-07-25","2024-05-28 15:18:12","2024-05-28 15:18:12","2024-05-28 15:18:12","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","30:55","","","","","","","","","","","","","","","","","","","","","","","","",""
"WWAKZYVZ","preprint","2017","Orabona, Francesco; Tommasi, Tatiana","Training Deep Networks without Learning Rates Through Coin Betting","","","","","http://arxiv.org/abs/1705.07795","Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a signiﬁcant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.","2017-11-04","2024-05-28 15:57:29","2024-05-28 15:57:29","2024-05-28 15:57:29","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1705.07795 [cs, math, stat]","Comment: Camera-ready version for NIPS 2017","C:\Users\isido\Zotero\storage\KKAWTNZ2\1705.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1705.07795","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTTUBFGD","preprint","2020","Cutkosky, Ashok; Orabona, Francesco","Momentum-Based Variance Reduction in Non-Convex SGD","","","","","http://arxiv.org/abs/1905.10018","Variance reduction has emerged in recent years as a strong competitor to stochastic gradient descent in non-convex problems, providing the ﬁrst algorithms to improve upon the converge rate of stochastic gradient descent for ﬁnding ﬁrst-order critical points. However, variance reduction techniques typically require carefully tuned learning rates and willingness to use excessively large “mega-batches” in order to achieve their improved results. We present a new algorithm, Storm, that does not require any batches and makes use of adaptive learning rates, enabling simpler implementation and less hyperparameter tuning. Our technique for removing the batches uses a variant of momentum to achieve variance redu√ction in non-convex optimization. On smooth losses F , Storm ﬁnds a point x with E[ ∇F (x) ] ≤ O(1/ T + σ1/3/T 1/3) in T iterations with σ2 variance in the gradients, matching the optimal rate and without requiring knowledge of σ.","2020-04-21","2024-05-28 17:39:55","2024-05-28 17:39:56","2024-05-28 17:39:55","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1905.10018 [cs, math, stat]","Comment: Added Ack","C:\Users\isido\Zotero\storage\TW4FHDX9\1905.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1905.10018","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84THX4DC","blogPost","2020","bremen79","Neural Networks (Maybe) Evolved to Make Adam The Best Optimizer","Parameter-free Learning and Optimization Algorithms","","","","https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/","EDIT 4/25/23This blog post went viral in 2020 and this idea is now widely accepted by the deep learning community. In fact, this is not only the most read post on my blog, but I might say that this…","2020-12-06","2024-05-31 17:11:33","2024-05-31 17:11:33","2024-05-31 17:11:33","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\EITUNCLX\neural-network-maybe-evolved-to-make-adam-the-best-optimizer.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMK8ZW8I","preprint","2024","Carmon, Yair; Hinder, Oliver","Making SGD Parameter-Free","","","","","http://arxiv.org/abs/2205.02160","We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameterfree SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.","2024-03-01","2024-05-31 16:46:48","2024-05-31 16:46:49","2024-05-31 16:46:48","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2205.02160 [cs, math, stat]","","C:\Users\isido\Zotero\storage\7KJL64DM\Carmon en Hinder - 2024 - Making SGD Parameter-Free.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.02160","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CRNKXPBR","preprint","2022","Kornowski, Guy; Shamir, Ohad","Oracle Complexity in Nonsmooth Nonconvex Optimization","","","","","http://arxiv.org/abs/2104.06763","It is well-known that given a smooth, bounded-from-below, and possibly nonconvex function, standard gradient-based methods can ﬁnd -stationary points (with gradient norm less than ) in O(1/ 2) iterations. However, many important nonconvex optimization problems, such as those associated with training modern neural networks, are inherently not smooth, making these results inapplicable. In this paper, we study nonsmooth nonconvex optimization from an oracle complexity viewpoint, where the algorithm is assumed to be given access only to local information about the function at various points. We provide two main results: First, we consider the problem of getting near -stationary points. This is perhaps the most natural relaxation of ﬁnding -stationary points, which is impossible in the nonsmooth nonconvex case. We prove that this relaxed goal cannot be achieved efﬁciently, for any distance and smaller than some constants. Our second result deals with the possibility of tackling nonsmooth nonconvex optimization by reduction to smooth optimization: Namely, applying smooth optimization methods on a smooth approximation of the objective function. For this approach, we prove under a mild assumption an inherent trade-off between oracle complexity and smoothness: On the one hand, smoothing a nonsmooth nonconvex function can be done very efﬁciently (e.g., by randomized smoothing), but with dimension-dependent factors in the smoothness parameter, which can strongly affect iteration complexity when plugging into standard smooth optimization methods. On the other hand, these dimension factors can be eliminated with suitable smoothing methods, but only by making the oracle complexity of the smoothing process exponentially large.","2022-10-27","2024-06-01 07:34:27","2024-06-01 07:34:27","2024-06-01 07:34:27","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2104.06763 [cs, math]","Comment: Accepted to Journal of Machine Learning Research (JMLR); some minor edits following reviews","C:\Users\isido\Zotero\storage\B5WCZP2G\2104.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2104.06763","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ZYHLRF8","preprint","2023","Orabona, Francesco","A Modern Introduction to Online Learning","","","","","http://arxiv.org/abs/1912.13213","In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the included proofs have been carefully chosen to be as simple and as short as possible.","2023-05-27","2024-06-01 07:43:37","2024-06-01 07:43:38","2024-06-01 07:43:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1912.13213 [cs, math, stat]","Comment: Major update: Two new chapters (saddle-point optimization and universal portfolio); added missing proofs; fixed issues due to empty interior simplex; fixed a lot of typos","C:\Users\isido\Zotero\storage\6EQQWESW\1912.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1912.13213","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZRUND88","journalArticle","2017","Stella, Lorenzo; Themelis, Andreas; Patrinos, Panagiotis","Forward–backward quasi-Newton methods for nonsmooth optimization problems","Computational Optimization and Applications","","0926-6003, 1573-2894","10.1007/s10589-017-9912-y","http://link.springer.com/10.1007/s10589-017-9912-y","","2017-07","2024-06-02 17:49:09","2024-06-02 17:49:09","2024-06-02 17:49:09","443-487","","3","67","","Comput Optim Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\C9U54MTB\Stella e.a. - 2017 - Forward–backward quasi-Newton methods for nonsmoot.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8Z6UE5G8","videoRecording","2022","Foundations of Probability Panel Discussions","Francesco Orabona -- Convex Optimization and Confidence Sequences with Coin-Betting","","","","","https://www.youtube.com/watch?v=hSHCLW_Ioi0","Abstract: Consider the following two problems. The first one is calculating valid and numerically sharp confidence sequences for the unknown expectation of a bounded random variable. The second problem is optimizing an arbitrary convex function with the smallest number of accesses to its stochastic gradients. Surprisingly, we will show that both problems can be solved through a reduction to a simple gambling game: betting money on the outcomes of a coin. First, we will explain that the problem of betting money on a coin can be solved with optimal algorithms from the universal compression/gambling literature. These algorithms guarantee an exponential growth rate of the wealth of the gambler, even without stochastic assumptions on the coin. In turn, this exponential wealth will allow us to design a reduction to obtain state-of-the-art valid confidence sequences for the expectation of bounded random variables. In particular, our confidence sequences are never vacuous, even with a single sample. Moreover, another reduction will allow us to convert the same betting algorithm into an optimal online convex optimization algorithm. Emphasis will be given to the history of these ideas in the fields of information theory, game-theoretic probability, and online learning.","2022-11-12","2024-06-04 13:18:04","2024-06-04 13:18:04","2024-06-04 13:18:04","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","01:03:24","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZBXNYPT","book","2020","","Numerical Nonsmooth Optimization: State of the Art Algorithms","","978-3-030-34909-7 978-3-030-34910-3","","","http://link.springer.com/10.1007/978-3-030-34910-3","","2020","2024-06-09 18:44:00","2024-06-09 18:44:00","2024-06-09 18:44:00","","","","","","","Numerical Nonsmooth Optimization","","","","","Springer International Publishing","Cham","en","http://www.springer.com/tdm","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-030-34910-3","","C:\Users\isido\Zotero\storage\EZVYESP2\Bagirov e.a. - 2020 - Numerical Nonsmooth Optimization State of the Art.pdf","","","","Bagirov, Adil M.; Gaudioso, Manlio; Karmitsa, Napsu; Mäkelä, Marko M.; Taheri, Sona","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MF4RVAEA","preprint","2024","Qian, Hongjin; Liu, Zheng; Zhang, Peitian; Mao, Kelong; Zhou, Yujia; Chen, Xu; Dou, Zhicheng","Are Long-LLMs A Necessity For Long-Context Tasks?","","","","","http://arxiv.org/abs/2405.15318","The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks’ inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.","2024-05-24","2024-06-10 16:17:04","2024-06-10 16:17:05","2024-06-10 16:17:04","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.15318 [cs]","Comment: 18 pages","C:\Users\isido\Zotero\storage\HQD9S6HW\Qian e.a. - 2024 - Are Long-LLMs A Necessity For Long-Context Tasks.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2405.15318","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64T9E8GR","journalArticle","2023","Wang, Yu-Chen; Wyman, Chris; Wu, Lifan; Zhao, Shuang","Amortizing Samples in Physics-Based Inverse Rendering Using ReSTIR","ACM Transactions on Graphics","","0730-0301, 1557-7368","10.1145/3618331","https://dl.acm.org/doi/10.1145/3618331","Recently, great progress has been made in physics-based differentiable rendering. Existing differentiable rendering techniques typically focus on               static               scenes, but during inverse rendering---a key application for differentiable rendering---the scene is updated               dynamically               by each gradient step. In this paper, we take a first step to leverage temporal data in the context of inverse direct illumination. By adopting reservoir-based spatiotemporal resampled importance resampling (ReSTIR), we introduce new Monte Carlo estimators for both interior and boundary components of differential direct illumination integrals. We also integrate ReSTIR with antithetic sampling to further improve its effectiveness. At equal frame time, our methods produce gradient estimates with up to 100× lower relative error than baseline methods. Additionally, we propose an inverse-rendering pipeline that incorporates these estimators and provides reconstructions with up to 20× lower error.","2023-12-05","2024-06-10 18:13:36","2024-06-10 18:13:36","2024-06-10 18:13:36","1-17","","6","42","","ACM Trans. Graph.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\WNLAD7GI\Wang e.a. - 2023 - Amortizing Samples in Physics-Based Inverse Render.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NJ27VTS3","journalArticle","2021","Hewamalage, Hansika; Bergmeir, Christoph; Bandara, Kasun","Recurrent Neural Networks for Time Series Forecasting: Current status and future directions","International Journal of Forecasting","","01692070","10.1016/j.ijforecast.2020.06.008","https://linkinghub.elsevier.com/retrieve/pii/S0169207020300996","Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.","2021-01","2024-06-11 17:36:53","2024-06-11 17:36:53","2024-06-11 17:36:53","388-427","","1","37","","International Journal of Forecasting","Recurrent Neural Networks for Time Series Forecasting","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\U5B8C6V7\Hewamalage e.a. - 2021 - Recurrent Neural Networks for Time Series Forecast.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVW73D62","preprint","2023","Sharrock, Louis; Nemeth, Christopher","Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates","","","","","http://arxiv.org/abs/2301.11294","In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.","2023-06-01","2024-06-11 18:02:50","2024-06-11 18:02:51","2024-06-11 18:02:50","","","","","","","Coin Sampling","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.11294 [cs, stat]","Comment: ICML 2023","C:\Users\isido\Zotero\storage\KZUGXC84\2301.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2301.11294","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIW5XFMD","journalArticle","2021","Chen, Yangang; Wan, Justin W. L.","Deep neural network framework based on backward stochastic differential equations for pricing and hedging American options in high dimensions","Quantitative Finance","","1469-7688, 1469-7696","10.1080/14697688.2020.1788219","https://www.tandfonline.com/doi/full/10.1080/14697688.2020.1788219","","2021-01-02","2024-06-12 15:50:18","2024-06-12 15:50:18","2024-06-12 15:50:18","45-67","","1","21","","Quantitative Finance","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\XTC4EZ68\Chen en Wan - 2021 - Deep neural network framework based on backward st.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ZTRUU4Z","webpage","2024","","Writing better code with pytorch and einops","Writing better code with pytorch and einops","","","","https://arogozhnikov.github.io/einops/pytorch-examples.html","Learning by example: rewriting and fixing popular code fragments","2024-06-16","2024-06-16 15:17:50","2024-08-12 15:00:04","2024-06-16 15:17:50","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\Z5U5BZA8\pytorch-examples.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DETIKC85","preprint","2022","Gnoatto, Alessandro; Patacca, Marco; Picarelli, Athena","A deep solver for BSDEs with jumps","","","","","http://arxiv.org/abs/2211.04349","The aim of this work is to propose an extension of the Deep BSDE solver by Han, E, Jentzen (2017) to the case of FBSDEs with jumps. As in the aforementioned solver, starting from a discretized version of the BSDE and parametrizing the (high dimensional) control processes by means of a family of ANNs, the BSDE is viewed as model-based reinforcement learning problem and the ANN parameters are ﬁtted so as to minimize a prescribed loss function. We take into account both ﬁnite and inﬁnite jump activity by introducing, in the latter case, an approximation with ﬁnitely many jumps of the forward process.","2022-11-08","2024-06-17 20:28:35","2024-06-17 20:28:35","2024-06-17 20:28:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.04349 [cs, math, q-fin]","Comment: 31 pages","C:\Users\isido\Zotero\storage\4S7QNIVC\2211.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2211.04349","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y6TY3XYZ","journalArticle","2020","Becker, Sebastian; Cheridito, Patrick; Jentzen, Arnulf","Pricing and hedging American-style options with deep learning","Journal of Risk and Financial Management","","1911-8074","10.3390/jrfm13070158","http://arxiv.org/abs/1912.11060","In this paper we introduce a deep learning method for pricing and hedging American-style options. It ﬁrst computes a candidate optimal stopping policy. From there it derives a lower bound for the price. Then it calculates an upper bound, a point estimate and conﬁdence intervals. Finally, it constructs an approximate dynamic hedging strategy. We test the approach on diﬀerent speciﬁcations of a Bermudan max-call option. In all cases it produces highly accurate prices and dynamic hedging strategies with small replication errors.","2020-07-19","2024-06-20 06:21:48","2024-06-20 06:21:48","2024-06-20 06:21:47","158","","7","13","","JRFM","","","","","","","","en","","","","","arXiv.org","","arXiv:1912.11060 [q-fin]","","C:\Users\isido\Zotero\storage\Z77N3YQW\Becker e.a. - 2020 - Pricing and hedging American-style options with de.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJ3NPUF2","journalArticle","2022","Cao, Jay; Chen, Jacky; Farghadani, Soroush; Hull, John; Poulos, Zissis; Wang, Zeyu; Yuan, Jun","Gamma and Vega Hedging Using Deep Distributional Reinforcement Learning","","","","","","We show how D4PG can be used in conjunction with quantile regression to develop a hedging strategy for a trader responsible for derivatives that arrive stochastically and depend on a single underlying asset. We assume that the trader makes the portfolio delta neutral at the end of each day by taking a position in the underlying asset. We focus on how trades in the options can be used to manage gamma and vega. The option trades are subject to transaction costs. We consider three different objective functions. We reach conclusions on how the optimal hedging strategy depends on the trader’s objective function, the level of transaction costs, and the maturity of the options used for hedging. We also investigate the robustness of the hedging strategy to the process assumed for the underlying asset.","2022-12","2024-06-19 17:02:46","2024-08-12 13:26:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\RIH85TDA\Cao e.a. - Gamma and Vega Hedging Using Deep Distributional R.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXR57FCT","preprint","2024","Hu, Ruimeng; Laurière, Mathieu","Recent Developments in Machine Learning Methods for Stochastic Control and Games","","","","","http://arxiv.org/abs/2303.10257","Stochastic optimal control and games have a wide range of applications, from finance and economics to social sciences, robotics, and energy management. Many real-world applications involve complex models that have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for solving stochastic control problems and games. In this review, we focus on deep learning methods that have unlocked the possibility of solving such problems, even in high dimensions or when the structure is very complex, beyond what traditional numerical methods can achieve. We consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network-based methods for solving high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough results. This paper provides an introduction to these methods and summarizes the state-of-the-art works at the crossroad of machine learning and stochastic control and games.","2024-03-11","2024-06-24 07:53:51","2024-06-24 07:53:51","2024-06-24 07:53:51","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2303.10257 [cs, math]","","C:\Users\isido\Zotero\storage\IFISWBYT\Hu en Laurière - 2024 - Recent Developments in Machine Learning Methods fo.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.10257","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4NTKB2K","preprint","2024","Lones, Michael A.","How to avoid machine learning pitfalls: a guide for academic researchers","","","","","http://arxiv.org/abs/2108.02497","This document outlines some of the common mistakes that occur when using machine learning, and what can be done to avoid them. Whilst it should be accessible to anyone with a basic understanding of machine learning techniques, it was originally written for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.","2024-01-03","2024-06-26 12:49:46","2024-06-26 12:49:48","2024-06-26 12:49:46","","","","","","","How to avoid machine learning pitfalls","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2108.02497 [cs]","Comment: 28 pages","C:\Users\isido\Zotero\storage\HW5GX47X\Lones - 2024 - How to avoid machine learning pitfalls a guide fo.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2108.02497","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFW2I5VC","book","1999","Fox, Bennett L.","Strategies for Quasi-Monte Carlo","","978-1-4613-7379-7 978-1-4615-5221-5","","","http://link.springer.com/10.1007/978-1-4615-5221-5","","1999","2024-06-29 19:27:30","2024-06-29 19:27:30","2024-06-29 19:27:30","","","","22","","","","International Series in Operations Research & Management Science","","","","Springer US","Boston, MA","en","http://www.springer.com/tdm","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4615-5221-5","","C:\Users\isido\Zotero\storage\D9TH5VLH\Fox - 1999 - Strategies for Quasi-Monte Carlo.pdf","","","","","Hillier, Frederick S.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZFK2HB7","webpage","2021","Randall, Ryan P.","Literature Notes in Dendron","Ryan P. Randall","","","","https://ryanpatrickrandall.com/notes/Note-taking/Dendron/literature-notes-dendron","My most current write-up of using Dendron for literature notes.","2021-06-21","2024-06-27 08:39:04","2024-06-27 08:39:04","2024-06-27 08:39:04","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\isido\Zotero\storage\PT42NE9A\literature-notes-dendron.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UWSK7E95","document","2023","","art owen monte carlo book","","","","","","","2023-01","2024-07-02 13:44:28","2024-08-12 14:44:08","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\L96WXMFM\Ch-intro-merged.pdf","","♥♥♥♥♥; importance sampling; MCMC; monte carlo; quasi monte carlo; variance reduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44QY3AXF","preprint","2024","Li, Yu; Ware, Antony","A weighted multilevel Monte Carlo method","","","","","http://arxiv.org/abs/2405.03453","The Multilevel Monte Carlo (MLMC) method has been applied successfully in a wide range of settings since its first introduction by Giles [12]. When using only two levels, the method can be viewed as a kind of control-variate approach to reduce variance, as earlier proposed by Kebaier [18]. We introduce a generalization of the MLMC formulation by extending this control variate approach to any number of levels and deriving a recursive formula for computing the weights associated with the control variates and the optimal numbers of samples at the various levels.","2024-05-06","2024-07-04 19:10:47","2024-07-04 19:10:47","2024-07-04 19:10:47","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.03453 [q-fin]","Comment: 19 pages","C:\Users\isido\Zotero\storage\Y4ZBM66X\2405.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2405.03453","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJMCEZU6","preprint","2015","Dick, Josef; Kuo, Frances Y.; Gia, Quoc T. Le; Schwab, Christoph","Fast QMC matrix-vector multiplication","","","","","http://arxiv.org/abs/1501.06286","Quasi-Monte Carlo (QMC) rules $1/N \sum_{n=0}^{N-1} f(\boldsymbol{y}_n A)$ can be used to approximate integrals of the form $\int_{[0,1]^s} f(\boldsymbol{y} A) \,\mathrm{d} \boldsymbol{y}$, where $A$ is a matrix and $\boldsymbol{y}$ is row vector. This type of integral arises for example from the simulation of a normal distribution with a general covariance matrix, from the approximation of the expectation value of solutions of PDEs with random coefficients, or from applications from statistics. In this paper we design QMC quadrature points $\boldsymbol{y}_0, ..., \boldsymbol{y}_{N-1} \in [0,1]^s$ such that for the matrix $Y = (\boldsymbol{y}_{0}^\top, ..., \boldsymbol{y}_{N-1}^\top)^\top$ whose rows are the quadrature points, one can use the fast Fourier transform to compute the matrix-vector product $Y \boldsymbol{a}^\top$, $\boldsymbol{a} \in \mathbb{R}^s$, in $\mathcal{O}(N \log N)$ operations and at most $s-1$ extra additions. The proposed method can be applied to lattice rules, polynomial lattice rules and a certain type of Korobov $p$-set. The approach is illustrated computationally by three numerical experiments. The first test considers the generation of points with normal distribution and general covariance matrix, the second test applies QMC to high-dimensional, affine-parametric, elliptic partial differential equations with uniformly distributed random coefficients, and the third test addresses Finite-Element discretizations of elliptic partial differential equations with high-dimensional, log-normal random input data. All numerical tests show a significant speed-up of the computation times of the fast QMC matrix method compared to a conventional implementation as the dimension becomes large.","2015-01-26","2024-07-04 20:28:57","2024-08-11 15:41:28","2024-07-04 20:28:57","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1501.06286 [math]","","C:\Users\isido\Zotero\storage\CUW5Y7TQ\Dick e.a. - 2015 - Fast QMC matrix-vector multiplication.pdf","","monte carlo; ♥♥; quasi monte carlo; matrix multiplication; FFT","","","","","","","","","","","","","","","","","","","","arXiv:1501.06286","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKS9BI9C","preprint","2021","Rosowski, Andreas","On Fast Computation of a Circulant Matrix-Vector Product","","","","","http://arxiv.org/abs/2103.02605","This paper deals with circulant matrices. It is shown that a circulant matrix can be multiplied by a vector in time O(n log(n)) in a ring with roots of unity without making use of an FFT algorithm. With our algorithm we achieve a speedup of a factor of about 2.25 for the multiplication of two polynomials with integer coeﬃcients compared to multiplication by an FFT algorithm. Moreover this paper discusses multiplication of large integers as further application.","2021-03-03","2024-07-04 20:57:13","2024-08-11 15:40:52","2024-07-04 20:57:13","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2103.02605 [cs]","","C:\Users\isido\Zotero\storage\8T3LCQ8C\Rosowski - 2021 - On Fast Computation of a Circulant Matrix-Vector P.pdf","","♥♥; matrix multiplication; FFT","","","","","","","","","","","","","","","","","","","","arXiv:2103.02605","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKBGJPGE","preprint","2022","M., Hariprasad; Venkatapathi, Murugesan","Circulant decomposition of a matrix and the eigenvalues of Toeplitz type matrices","","","","","http://arxiv.org/abs/2105.14805","We begin by showing that any n ˆ n matrix can be decomposed into a sum of n circulant matrices with periodic relaxations on the unit circle. This decomposition is orthogonal with respect to a Frobenius inner product, allowing recursive iterations for these circulant components. It is also shown that the dominance of a few circulant components in the matrix allows sparse similarity transformations using Fast-Fourier-transform (FFT) operations. This enables the evaluation of all eigenvalues of dense Toeplitz, block-Toeplitz, and other periodic or quasi-periodic matrices, to a reasonable approximation in Opn2q arithmetic operations. The utility of the approximate similarity transformation in preconditioning linear solvers is also demonstrated.","2022-09-28","2024-07-04 21:07:30","2024-08-11 15:33:43","2024-07-04 21:07:30","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2105.14805 [cs, math]","","C:\Users\isido\Zotero\storage\WGFU2Z9X\M. en Venkatapathi - 2022 - Circulant decomposition of a matrix and the eigenv.pdf","","♥♥♥♥; matrix multiplication; FFT","","","","","","","","","","","","","","","","","","","","arXiv:2105.14805","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZP3QFH9","journalArticle","2016","Kuo, Frances Y.; Nuyens, Dirk","Application of Quasi-Monte Carlo Methods to Elliptic PDEs with Random Diffusion Coefficients: A Survey of Analysis and Implementation","Foundations of Computational Mathematics","","1615-3375, 1615-3383","10.1007/s10208-016-9329-5","http://link.springer.com/10.1007/s10208-016-9329-5","This article provides a survey of recent research efforts on the application of quasi-Monte Carlo (QMC) methods to elliptic partial differential equations (PDEs) with random diffusion coefﬁcients. It considers and contrasts the uniform case versus the lognormal case, single-level algorithms versus multi-level algorithms, ﬁrst-order QMC rules versus higher-order QMC rules, and deterministic QMC methods versus randomized QMC methods. It gives a summary of the error analysis and proof techniques in a uniﬁed view, and provides a practical guide to the software for constructing and generating QMC points tailored to the PDE problems. The analysis for the uniform case can be generalized to cover a range of afﬁne parametric operator equations.","2016-12","2024-07-04 21:08:44","2024-08-11 15:32:48","2024-07-04 21:08:44","1631-1696","","6","16","","Found Comput Math","Application of Quasi-Monte Carlo Methods to Elliptic PDEs with Random Diffusion Coefficients","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\K8LL5K7R\Kuo en Nuyens - 2016 - Application of Quasi-Monte Carlo Methods to Ellipt.pdf","","monte carlo; PDE; random PDE; ♥♥♥; quasi monte carlo; MLMC; uncertainty quantification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2JEKN88","preprint","2024","Anupindi, Vishnupriya; Kritzer, Peter","Column reduced digital nets","","","","","http://arxiv.org/abs/2406.10850","Digital nets provide an efficient way to generate integration nodes of quasi-Monte Carlo (QMC) rules. For certain applications, as e.g. in Uncertainty Quantification, we are interested in obtaining a speed-up in computing products of a matrix with the vectors corresponding to the nodes of a QMC rule. In the recent paper The fast reduced QMC matrix-vector product (J. Comput. Appl. Math. 440, 115642, 2024), a speed up was obtained by using so-called reduced lattices and row reduced digital nets. In this work, we propose a different multiplication algorithm where we exploit the repetitive structure of column reduced digital nets instead of row reduced digital nets. This method has advantages over the previous one, as it facilitates the error analysis when using the integration nodes in a QMC rule. We also provide an upper bound for the quality parameter of column reduced digital nets, and numerical tests to illustrate the efficiency of the new algorithm.","2024-06-16","2024-07-05 20:27:46","2024-08-11 09:35:42","2024-07-05 20:27:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2406.10850 [cs, math]","","C:\Users\isido\Zotero\storage\DRDU53JA\2406.pdf","","quasi monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2406.10850","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CA7C6RYD","preprint","2024","Liu, Yang","Randomized quasi-Monte Carlo and Owen's boundary growth condition: A spectral analysis","","","","","http://arxiv.org/abs/2405.05181","In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen’s boundary growth condition [Owen, 2006] via spectral analysis. Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol’ sequence, applying the Fourier transform and Walsh–Fourier transform, respectively, for this analysis. Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator’s variance closely aligns with the exponent specified in Owen’s boundary growth condition for both sequence types. We also provide analysis for certain discontinuous integrands.","2024-06-01","2024-07-05 20:32:16","2024-08-11 09:31:00","2024-07-05 20:32:16","","","","","","","Randomized quasi-Monte Carlo and Owen's boundary growth condition","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.05181 [cs, math]","Comment: 46 pages, 8 figures","C:\Users\isido\Zotero\storage\X7L94UD8\Liu - 2024 - Randomized quasi-Monte Carlo and Owen's boundary g.pdf","","quasi monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2405.05181","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPSMBBLV","preprint","2024","Bayer, Christian; Hammouda, Chiheb Ben; Papapantoleon, Antonis; Samet, Michael; Tempone, Raúl","Quasi-Monte Carlo for Efficient Fourier Pricing of Multi-Asset Options","","","","","http://arxiv.org/abs/2403.02832","Eﬃciently pricing multi-asset options poses a signiﬁcant challenge in quantitative ﬁnance. The Monte Carlo (MC) method remains the prevalent choice for pricing engines; however, its slow convergence rate impedes its practical application. Fourier methods leverage the knowledge of the characteristic function to accurately and rapidly value options with up to two assets. Nevertheless, they face hurdles in the high-dimensional settings due to the tensor product (TP) structure of commonly employed quadrature techniques. This work advocates using the randomized quasi-MC (RQMC) quadrature to improve the scalability of Fourier methods with high dimensions. The RQMC technique beneﬁts from the smoothness of the integrand and alleviates the curse of dimensionality while providing practical error estimates. Nonetheless, the applicability of RQMC on the unbounded domain, Rd, requires a domain transformation to [0, 1]d, which may result in singularities of the transformed integrand at the corners of the hypercube, and deteriorate the rate of convergence of RQMC. To circumvent this diﬃculty, we design an eﬃcient domain transformation procedure based on the derived boundary growth conditions of the integrand. This transformation preserves the suﬃcient regularity of the integrand and hence improves the rate of convergence of RQMC. To validate this analysis, we demonstrate the eﬃciency of employing RQMC with an appropriate transformation to evaluate options in the Fourier space for various pricing models, payoﬀs, and dimensions. Finally, we highlight the computational advantage of applying RQMC over MC or TP in the Fourier domain, and over MC in the physical domain for options with up to 15 assets.","2024-03-05","2024-07-05 20:41:15","2024-08-11 08:36:29","2024-07-05 20:41:15","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.02832 [cs, math, q-fin]","","C:\Users\isido\Zotero\storage\JSRA96IP\Bayer e.a. - 2024 - Quasi-Monte Carlo for Efficient Fourier Pricing of.pdf","","monte carlo; ♥♥♥; quadrature; quasi monte carlo; option pricing; fourier pricing","","","","","","","","","","","","","","","","","","","","arXiv:2403.02832","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZI86XTL","preprint","2024","Andral, Charly","Combining Normalizing Flows and Quasi-Monte Carlo","","","","","http://arxiv.org/abs/2401.05934","Recent advances in machine learning have led to the development of new methods for enhancing Monte Carlo methods such as Markov chain Monte Carlo (MCMC) and importance sampling (IS). One such method is normalizing flows, which use a neural network to approximate a distribution by evaluating it pointwise. Normalizing flows have been shown to improve the performance of MCMC and IS. On the other side, (randomized) quasi-Monte Carlo methods are used to perform numerical integration. They replace the random sampling of Monte Carlo by a sequence which cover the hypercube more uniformly, resulting in better convergence rates for the error that plain Monte Carlo. In this work, we combine these two methods by using quasiMonte Carlo to sample the initial distribution that is transported by the flow. We demonstrate through numerical experiments that this combination can lead to an estimator with significantly lower variance than if the flow was sampled with a classic Monte Carlo.","2024-01-11","2024-07-05 21:10:00","2024-08-11 07:05:23","2024-07-05 21:10:00","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2401.05934 [stat]","","C:\Users\isido\Zotero\storage\Y43AXQ36\Andral - 2024 - Combining Normalizing Flows and Quasi-Monte Carlo.pdf","","monte carlo; importance sampling; machine learning; ♥♥; variance reduction; quasi monte carlo; MCMC; flows","","","","","","","","","","","","","","","","","","","","arXiv:2401.05934","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3PLZAY3","preprint","2023","Choi, Sou-Cheng T.; Ding, Yuhan; Hickernell, Fred J.; Rathinavel, Jagadeeswaran; Sorokin, Aleksei G.","Challenges in Developing Great Quasi-Monte Carlo Software","","","","","http://arxiv.org/abs/2311.06162","Quasi-Monte Carlo (QMC) methods have developed over several decades. With the explosion in computational science, there is a need for great software that implements QMC algorithms. We summarize the QMC software that has been developed to date, propose some criteria for developing great QMC software, and suggest some steps toward achieving great software. We illustrate these criteria and steps with the Quasi-Monte Carlo Python library (QMCPy), an open-source community software framework, extensible by design with common programming interfaces to an increasing number of existing or emerging QMC libraries developed by the greater community of QMC researchers.","2023-11-10","2024-07-05 21:18:59","2024-08-11 06:50:57","2024-07-05 21:18:59","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.06162 [cs, math, stat]","","C:\Users\isido\Zotero\storage\7RSUI9E3\Choi e.a. - 2023 - Challenges in Developing Great Quasi-Monte Carlo S.pdf","","♥; quasi monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2311.06162","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"855I32TR","preprint","2020","Mortier, Bert; Robbe, Pieterjan; Baelmans, Martine; Samaey, Giovanni","Multilevel Asymptotic-Preserving Monte Carlo for Particle Simulations","","","","","http://arxiv.org/abs/2004.04071","We develop a novel multilevel asymptotic-preserving Monte Carlo method, called Multilevel Kinetic-Diﬀusion Monte Carlo (ML-KDMC), for simulating the kinetic Boltzmann transport equation with a Bhatnagar–Gross–Krook (BGK) collision operator. This equation occurs, for instance, in mathematical models of the neutral particles in the plasma edge of nuclear fusion reactors. In this context, the Kinetic-Diﬀusion Monte Carlo method is known to maintain accuracy both in the low-collisional and the high-collisional limit, without an exploding simulation cost in the latter. We show that, by situating this method within a Multilevel Monte Carlo (MLMC) framework, using a hierarchy of larger time step sizes, the simulation cost is reduced even further. The diﬀerent levels in our ML-KDMC method are connected via a new and improved recipe for correlating particle trajectories with diﬀerent time step sizes. Furthermore, a new and more general level selection strategy is presented. We illustrate the eﬃciency of our ML-KDMC method by applying it to a one-dimensional test case with nonhomogeneous and anisotropic plasma background. Our method yields signiﬁcant speedups compared to the single-level KDMC scheme, both in the low and high collisional regime. In the high-collisional case, our ML-KDMC outperforms the single-level KDMC method by several orders of magnitude.","2020-10-22","2024-07-07 16:50:05","2024-08-11 06:49:52","2024-07-07 16:50:05","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2004.04071 [cs, math]","","C:\Users\isido\Zotero\storage\PICYUX32\Mortier e.a. - 2020 - Multilevel Asymptotic-Preserving Monte Carlo for P.pdf","","monte carlo; ♥♥♥; kinetic equations; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2004.04071","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GMJGBEJN","preprint","2023","Gander, Martin J.; Palitta, Davide","A new ParaDiag time-parallel time integration method","","","","","http://arxiv.org/abs/2304.12597","Time-parallel time integration has received a lot of attention in the high performance computing community over the past two decades. Indeed, it has been shown that parallel-in-time techniques have the potential to remedy one of the main computational drawbacks of parallel-in-space solvers. In particular, it is well-known that for large-scale evolution problems space parallelization saturates long before all processing cores are effectively used on today’s large scale parallel computers. Among the many approaches for time-parallel time integration, ParaDiag schemes have proven to be a very effective approach. In this framework, the time stepping matrix or an approximation thereof is diagonalized by Fourier techniques, so that computations taking place at different time steps can be indeed carried out in parallel. We propose here a new ParaDiag algorithm combining the Sherman-Morrison-Woodbury formula and Krylov techniques. A panel of diverse numerical examples illustrates the potential of our new solver. In particular, we show that it performs very well compared to different ParaDiag algorithms recently proposed in the literature.","2023-09-23","2024-07-07 17:20:11","2024-08-11 06:48:57","2024-07-07 17:20:11","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2304.12597 [cs, math]","","C:\Users\isido\Zotero\storage\FHA4HMW6\Gander en Palitta - 2023 - A new ParaDiag time-parallel time integration meth.pdf","","PDE; ♥; parallel in time","","","","","","","","","","","","","","","","","","","","arXiv:2304.12597","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2CPVZPB","journalArticle","2007","Maday, Yvon; Rønquist, Einar M.","Parallelization in time through tensor-product space–time solvers","Comptes Rendus. Mathématique","","1778-3569","10.1016/j.crma.2007.09.012","https://comptes-rendus.academie-sciences.fr/mathematique/articles/10.1016/j.crma.2007.09.012/","In this Note, we extend the fast tensor-product algorithm for the simulation of time-dependent partial differential equations. We use the natural tensorization of the space–time domain to propose, after discretization, a set of independent problems, each one with the complexity of a single steady problem. This allows for an efﬁcient parallel implementation that is already interesting on small architectures, but that can also be combined with standard domain-decomposition-based algorithms providing a further direction of parallelism on large computer platforms. Preliminary numerical simulations are presented for a one-dimensional unsteady heat equation. To cite this article: Y. Maday, E.M. Rønquist, C. R. Acad. Sci. Paris, Ser. I 346 (2008).","2007-12-03","2024-07-07 18:01:55","2024-08-10 20:09:04","2024-07-07 18:01:55","113-118","","1-2","346","","","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\DG9QN8MH\Maday en Rønquist - 2007 - Parallelization in time through tensor-product spa.pdf","","PDE; ODE; ♥♥♥; parallel in time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQPNUWDH","bookSection","2016","Gander, Martin J.; Halpern, Laurence; Ryan, Juliet; Tran, Thuy Thi Bich","A Direct Solver for Time Parallelization","Domain Decomposition Methods in Science and Engineering XXII","978-3-319-18826-3 978-3-319-18827-0","","","https://link.springer.com/10.1007/978-3-319-18827-0_50","","2016","2024-07-07 18:06:01","2024-08-10 20:08:16","2024-07-07 18:06:01","491-499","","","104","","","","","","","","Springer International Publishing","Cham","en","http://www.springer.com/tdm","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computational Science and Engineering DOI: 10.1007/978-3-319-18827-0_50","","C:\Users\isido\Zotero\storage\CH4R7II4\Gander e.a. - 2016 - A Direct Solver for Time Parallelization.pdf","","PDE; ODE; ♥♥♥; parallel in time","","Dickopf, Thomas; Gander, Martin J.; Halpern, Laurence; Krause, Rolf; Pavarino, Luca F.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JX7S7ZT","journalArticle","2019","Gander, Martin J.; Halpern, Laurence; Rannou, Johann; Ryan, Juliette","A Direct Time Parallel Solver by Diagonalization for the Wave Equation","SIAM Journal on Scientific Computing","","1064-8275, 1095-7197","10.1137/17M1148347","https://epubs.siam.org/doi/10.1137/17M1148347","With the advent of very large scale parallel computers, it has become more and more important to also use the time direction for parallelization when solving evolution problems. While there are many successful algorithms for diﬀusive problems, only some of them are also eﬀective for hyperbolic problems. We present here a mathematical analysis of a new method based on the diagonalization of the time stepping matrix proposed by Maday and Rønquist in 2007. Like many time-parallelization methods, at ﬁrst this does not seem to be a very promising approach: the matrix is essentially triangular, or, for equidistant time steps, actually a Jordan block, and thus not diagonalizable. If one chooses however diﬀerent time steps, diagonalization is possible, and one has to trade oﬀ between the accuracy due to necessarily having diﬀerent time steps, and numerical errors in the diagonalization process of these almost nondiagonalizable matrices. We present for the ﬁrst time such a diagonalization technique for the Newmark scheme for solving wave equations, and derive a mathematically rigorous optimization strategy for the choice of the parameters in the special case when the Newmark scheme becomes Crank–Nicolson. Our analysis shows that small to medium scale time parallelization is possible with this approach. We illustrate our results with numerical experiments for model wave equations in various dimensions and also an industrial test case for the elasticity equations with variable coeﬃcients.","2019-01","2024-07-07 18:15:15","2024-08-10 20:08:02","2024-07-07 18:15:15","A220-A245","","1","41","","SIAM J. Sci. Comput.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\5U6FEP6Z\Gander e.a. - 2019 - A Direct Time Parallel Solver by Diagonalization f.pdf","","PDE; ODE; ♥♥♥; parallel in time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTJDFH95","bookSection","2014","Leobacher, Gunther","A short introduction to quasi-Monte Carlo option pricing","","","","","http://arxiv.org/abs/1707.04293","One of the main practical applications of quasi-Monte Carlo (QMC) methods is the valuation of ﬁnancial derivatives. We aim to give a short introduction into option pricing and show how it is facilitated using QMC. We give some practical examples for illustration.","2014-12-31","2024-07-08 11:31:31","2024-08-10 20:04:46","2024-07-08 11:31:31","191-222","","","","","","","","","","","","","en","","","","","arXiv.org","","DOI: 10.1515/9783110317930.191 arXiv:1707.04293 [math, q-fin]","","C:\Users\isido\Zotero\storage\26V5H7IE\Leobacher - 2014 - A short introduction to quasi-Monte Carlo option p.pdf","","monte carlo; SDE; ♥♥♥; quasi monte carlo; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JX8TTGB","journalArticle","2009","Dick, Josef; Pillichshammer, Friedrich","Digital Nets and Sequences","","","","","","","2009-11-28","2024-07-08 09:02:15","2024-08-12 14:20:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\HLA6CQYU\Dick and Pillichshammer - Digital Nets and Sequences.pdf","","♥♥♥; quasi monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9TM6NEKG","journalArticle","2011","Chen, Zisheng; Feng, Liming; Lin, Xiong","Simulating Levy Processes from Their Characteristic Functions and Financial Applications","ACM Transactions on Modeling and Computer Simulation","","","10.1145/2331140.2331142","","The simulation of a discrete sample path of a Levy process reduces to simulating from the distribution of a Levy increment. For a general Levy process with exponential moments, the inverse transform method proposed in Glasserman and Liu 2010 [24] is reliable and efficient. The values of the cumulative distribution function (cdf) are computed by inverting the characteristic function and tabulated on a uniform grid. The inverse of the cumulative distribution function is obtained by linear interpolation. In this paper, we apply a Hilbert transform method for the characteristic function inversion. The Hilbert transform representation for the cdf can be discretized using a simple rule highly accurately. Most importantly, the error estimates admit explicit and computable expressions, which allow us to compute the cdf to any desired accuracy. We present an explicit bound for the estimation bias in terms of the range of the grid where probabilities are tabulated, the step size of the grid, and the approximation error for the probabilities. The bound can be computed from the characteristic function directly and allows one to determine the size and fineness of the grid and numerical parameters for evaluating the Hilbert transforms for any given bias tolerance level in one dimensional problems. For multidimensional problems, we present a procedure for selecting the grid and the numerical parameters that is shown to converge theoretically and works well practically. The inverse transform method is attractive not only for Levy processes that are otherwise not easy to simulate, but also for processes with special structures that could be simulated in different ways. The method is very fast and accurate when combined with quasi-Monte Carlo schemes and variance reduction techniques. The main results we derived are not limited to Levy processes and can be applied to simulating from tabulated cumulative distribution functions in general and characteristic functions in our analytic class in particular.","2011-07-30","2024-07-09 05:59:30","2024-08-10 20:02:23","","","","","22","","ACM Transactions on Modeling and Computer Simulation","","","","","","","","","","","","","ResearchGate","","","","; C:\Users\isido\Zotero\storage\TT4JNTF6\Chen e.a. - 2011 - Simulating Levy Processes from Their Characteristi.pdf","https://www.researchgate.net/profile/Liming-Feng/publication/228291452_Simulating_Levy_Processes_from_Their_Characteristic_Functions_and_Financial_Applications/links/09e4150eb8a843de93000000/Simulating-Levy-Processes-from-Their-Characteristic-Functions-and-Financial-Applications.pdf?origin=publication_detail&_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uRG93bmxvYWQiLCJwcmV2aW91c1BhZ2UiOiJwdWJsaWNhdGlvbiJ9fQ","levy processes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2RUY33RL","conferencePaper","2008","L'Ecuyer, Pierre; Parent-Chartier, Jean-Sebastien; Dion, Maxime","Simulation of a Lévy process by PCA sampling to reduce the effective dimension","2008 Winter Simulation Conference","978-1-4244-2707-9","","10.1109/WSC.2008.4736098","https://ieeexplore.ieee.org/document/4736098/","We consider a Le´vy process monitored at s (ﬁxed) observation times. The goal is to estimate the expected value of some function of these s observations by (randomized) quasiMonte Carlo. For the case where the process is a Brownian motion, clever techniques such as Brownian bridge sampling and PCA sampling have been proposed to reduce the effective dimension of the problem. The PCA method uses an eigen-decomposition of the covariance matrix of the vector of observations so that a larger fraction of the variance depends on the ﬁrst few (quasi)random numbers that are generated. We show how this method can be applied to other Le´vy processes, and we examine its effectiveness in improving the quasi-Monte Carlo efﬁciency on some examples. The basic idea is to simulate a Brownian motion at s observation points using PCA, transform its increments into independent uniforms over (0,1), then transform these uniforms again by applying the inverse distribution function of the increments of the Le´vy process. This PCA sampling technique is quite effective in improving the quasi-Monte Carlo performance when the sampled increments of the Le´vy process have a distribution that is not too far from normal, which typically happens when the process is observed at a large time scale, but may turn out to be ineffective in cases where the increments are far from normal.","2008-12","2024-07-09 06:03:38","2024-08-10 20:02:48","2024-07-09 06:03:38","436-443","","","","","","","","","","","IEEE","Miami, FL, USA","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\CA9TI8JF\L'Ecuyer e.a. - 2008 - Simulation of a Lévy process by PCA sampling to re.pdf","","levy processes; quasi monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2008 Winter Simulation Conference (WSC)","","","","","","","","","","","","","","",""
"FEYGQGTP","videoRecording","2024","Symposia at CSAIL","Hot Topics in Computing Prof. Michael Bronstein","","","","","https://www.youtube.com/watch?v=MeJgxYfiaz8","On 06/06/2024 Prof. Michael Bronstein delivered a lecture titled Geometric Deep Learning: From Euclid to Drug Design as part of the Hot Topics in Computing Series at CSAIL. Michael Bronstein MAE FIEEE FBCS is an Israeli computer scientist and entrepreneur. He is a computer science professor at the University of Oxford.","2024-06-26","2024-07-10 13:16:58","2024-08-10 19:58:23","2024-07-10 13:16:58","","","","","","","","","","","","","","","","","","","YouTube","","","","","","machine learning; deep learning; ♥♥♥; GNN","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69GY4EEB","videoRecording","2022","MIT Plasma Science and Fusion Center","Dwarfs of computation in HPC fusion codes","","","","","https://www.youtube.com/watch?v=nEyCBzGYoXk","So the first question that you might have is, what is a dwarf of computation? And before I answer that, it's helpful to recall 2:06 some of the historical context. So in the late 1990s and the early 2000s, 2:12 the US government and industry leaders realized that trends in high performance computing 2:17 were creating technology gaps that could pose a threat to US superiority and national security applications 2:24 and industrial competitiveness. And the most alarming trend was rapid growth of less innovative 2:31 commodity-based computing clusters often at the expense of less innovative-- 2:37 I'm sorry-- the most alarming trend was the rapid growth of the less innovative commodity based computing clusters often 2:43 at the expense of the leading edge or the capability class supercomputers. So this strong market trend meant 2:50 that the few companies that produced the high-end supercomputers had less money to invest in the innovative hardware development 2:56 and the specialized software. So in 2002, DARPA, the US's Defense Advanced Research 3:04 Projects Agency, launched the high productivity computing systems or HPCS program. 3:10 And the goal of the HPCS program was to revitalize supercomputer research with direct support 3:16 from the US government and to ultimately develop a new generation of supercomputers 3:22 for large scale science including the first petaflops system. So at the time, the fastest supercomputer in the world, 3:29 according to the top 500 list, was the Japanese Earth simulator at 35 teraflops. 3:36 So DARPA made initial grants to five key players, IBM, Cray, HP, Silicon Graphics, and Sun Microsystems. 3:44 And the focus was on real rather than peak performance of critical national security applications, 3:50 programmability, software portability, and system robustness. So on the hardware side, the program 3:56 was eventually successful. As in 2008, the IBM Roadrunner system at Los Alamos National lab debuted as the first petaflops 4:04 system. And it actually held the number one position on the top 500 list for only one year 4:11 when it was eclipsed by another HPCS program success, which was the Cray Jag layer system at Oak Ridge National Lab. 4:19 But on the application side back in 2002, there was a sense that pedestal systems were just 4:25 around the corner. But that the world wasn't ready for them yet as few software applications could efficiently 4:31 exploit the present day HPC systems much less petite scale systems. 4:36 So algorithms would need to be rewritten, or in some cases, entirely rethought for petascale performance 4:42 to become a commonplace. So related to this, in 2004, an applied mathematician 4:49 named Phil Colella gave what is now a somewhat famous talk about the DARPA HPCS program. 4:55 And the talk was unassumingly titled Defining Software Requirements for Scientific Computing. 5:01 So who is Phil Colella? Well, Phil, shown here, was, until last year 5:06 when he retired, a mathematician at Lawrence Berkeley National Lab. And he developed many key numerical algorithms 5:13 for adaptive mesh refinement and played a key role in the design of HPC software infrastructure 5:19 for scientific computing. So he was also very active in bridging the gap between applied mathematicians and scientists, 5:27 particularly for fluid and fusion research. So plasma physicists who have been in Fusion 5:32 for a while like myself know Phil very well as a colleague. And the key idea that Colella promoted in his 2004 talk 5:40 is that nearly all high performance computing done in support of scientific research both today 5:47 and in the near future will use one or more of a very small set of key algorithmic kernels that 5:53 would become known as dwarfs. And the idea was that the codes may vary over time. 5:58 But the dwarfs will be important for decades. And the role of defining the dwarfs in the context of the HPCS program from DARPA 6:06 was to focus attention on HPC challenges and the development of libraries and problem-solving 6:13 environments and framework. So a key point is that the dwarfs are not tied to any code or language artifacts 6:21 to simultaneously encourage innovation in algorithms, languages, data structures, and hardware. 6:27 So for example, any new architecture should have good performance across all of the dwarfs 6:32 in order to be suited for a broad range of future applications. 6:38 So what exactly is the definition of a dwarf? Well, a dwarf is an algorithmic method 6:43 that captures a pattern of computation and communication. And Colella identified the Seven key dwarfs of computation 6:51 as structured grids, unstructured grids, dense linear algebra, sparse linear algebra, 6:56 spectral methods, n-body methods, and Monte Carlo. So in case you're wondering about the reference 7:02 to the fairy tale of Snow White and the Seven dwarfs, the idea was that the dwarfs ""mine compute 7:09 cycles"" for ""golden results."" so each of the dwarfs has well-defined targets","2022-09-21","2024-07-10 18:09:03","2024-08-10 19:44:07","2024-07-10 18:09:03","","","","","","","","","","","","","","","","","","","YouTube","","","","","","♥♥♥; FFT","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5FX22K9Z","preprint","2019","Blanchet, Jose H.; Glynn, Peter W.; Pei, Yanan","Unbiased Multilevel Monte Carlo: Stochastic Optimization, Steady-state Simulation, Quantiles, and Other Applications","","","","","http://arxiv.org/abs/1904.09929","We present general principles for the design and analysis of unbiased Monte Carlo estimators in a wide range of settings. Our estimators posses ﬁnite work-normalized variance under mild regularity conditions. We apply our estimators to various settings of interest, including unbiased optimization in Sample Average Approximations, unbiased steady-state simulation of regenerative processes, quantile estimation and nested simulation problems.","2019-04-22","2024-07-11 08:15:13","2024-08-10 19:32:19","2024-07-11 08:15:13","","","","","","","Unbiased Multilevel Monte Carlo","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1904.09929 [math, stat]","Comment: 20 pages, 2 figures","C:\Users\isido\Zotero\storage\KMYTPVC7\1904.pdf","","monte carlo; ♥♥♥♥; debiasing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1904.09929","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"STT6HFK9","videoRecording","2020","Erwin Schrödinger International Institute for Mathematics and Physics (ESI)","Matti Vihola - Unbiased estimators and multilevel Monte Carlo","","","","","https://www.youtube.com/watch?v=oJcHIyCg4No","This talk was part of the of the Online Workshop of the Thematic Programme ""Computational Uncertainty Quantification: Mathematical Foundations, Methodology & Data"" held May 4 - 8, 2020 digitally. Multilevel Monte Carlo (MLMC) and recently proposed debiasing schemes are closely related methods which can be applied in scenarios where exact simulation methods are difficult to implement, but biased estimators are easily available. Unbiasedness is important when multiple estimators are combined. This talk is about a general class of unbiased estimators which admits earlier debiasing schemes as special cases, and which accomodates new lower variance estimators which behave asymptotically like MLMC, both in terms of variance and cost, under general conditions. This suggests that bias can often be eliminated entirely with arbitrarily small extra cost. (The talk is based on: Oper. Res. 2018;  66(2):448-462.)","2020-05-08","2024-07-11 14:56:42","2024-08-10 19:32:09","2024-07-11 14:56:42","","","","","","","","","","","","","","","","","","","YouTube","","","","","","monte carlo; ♥♥♥♥; debiasing; MLMC","","","","","","","","","","","","","","","","","","","","","","39:41","","","","","","","","","","","","","","","","","","","","","","","","",""
"9S3YEDWG","preprint","2022","Wang, Guanyang; Wang, Tianze","Unbiased Multilevel Monte Carlo methods for intractable distributions: MLMC meets MCMC","","","","","http://arxiv.org/abs/2204.04808","Constructing unbiased estimators from Markov chain Monte Carlo (MCMC) outputs is a diﬃcult problem that has recently received a lot of attention in the statistics and machine learning communities. However, the current unbiased MCMC framework only works when the quantity of interest is an expectation, which excludes many practical applications. In this paper, we propose a general method for constructing unbiased estimators for functions of expectations and extend it to construct unbiased estimators for nested expectations. Our approach combines and generalizes the unbiased MCMC and Multilevel Monte Carlo (MLMC) methods. In contrast to traditional sequential methods, our estimator can be implemented on parallel processors. We show that our estimator has a ﬁnite variance and computational complexity and can achieve ε-accuracy within the optimal O(1/ε2) computational cost under mild conditions. Our numerical experiments conﬁrm our theoretical ﬁndings and demonstrate the beneﬁts of unbiased estimators in the massively parallel regime.","2022-12-23","2024-07-11 20:22:27","2024-08-10 19:30:16","2024-07-11 20:22:27","","","","","","","Unbiased Multilevel Monte Carlo methods for intractable distributions","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2204.04808 [cs, math, stat]","Comment: add additional experiments, rewrite part of the introduction, add more references","C:\Users\isido\Zotero\storage\IHFWLKW9\Wang en Wang - 2022 - Unbiased Multilevel Monte Carlo methods for intrac.pdf","","monte carlo; ♥♥♥♥; debiasing; MCMC; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2204.04808","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJ3LC26I","preprint","2024","Wang, Guanyang; Blanchet, Jose; Glynn, Peter W.","When are Unbiased Monte Carlo Estimators More Preferable than Biased Ones?","","","","","http://arxiv.org/abs/2404.01431","Due to the potential benefits of parallelization, designing unbiased Monte Carlo estimators, primarily in the setting of randomized multilevel Monte Carlo, has recently become very popular in operations research and computational statistics. However, existing work primarily substantiates the benefits of unbiased estimators at an intuitive level or using empirical evaluations. The intuition being that unbiased estimators can be replicated in parallel enabling fast estimation in terms of wall-clock time. This intuition ignores that, typically, bias will be introduced due to impatience because most unbiased estimators necesitate random completion times. This paper provides a mathematical framework for comparing these methods under various metrics, such as completion time and overall computational cost. Under practical assumptions, our findings reveal that unbiased methods typically have superior completion times —the degree of superiority being quantifiable through the tail behavior of their running time distribution — but they may not automatically provide substantial savings in overall computational costs. We apply our findings to Markov Chain Monte Carlo and Multilevel Monte Carlo methods to identify the conditions and scenarios where unbiased methods have an advantage, thus assisting practitioners in making informed choices between unbiased and biased methods.","2024-04-01","2024-07-11 20:48:41","2024-08-10 19:28:52","2024-07-11 20:48:41","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.01431 [cs, math, stat]","Comment: 35 pages","C:\Users\isido\Zotero\storage\X7NFGD9S\Wang e.a. - 2024 - When are Unbiased Monte Carlo Estimators More Pref.pdf","","monte carlo; ♥♥♥♥♥; debiasing","","","","","","","","","","","","","","","","","","","","arXiv:2404.01431","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LD3HP5U7","preprint","2024","Beschle, Cedric Aaron; Barth, Andrea","Complexity analysis of quasi continuous level Monte Carlo","","","","","http://arxiv.org/abs/2305.15949","Continuous level Monte Carlo is an unbiased, continuous version of the celebrated multilevel Monte Carlo method. The approximation level is assumed to be continuous resulting in a stochastic process describing the quantity of interest. Continuous level Monte Carlo methods allow naturally for samplewise adaptive mesh refinements, which are indicated by goal-oriented error estimators. The samplewise refinement levels are drawn in the estimator from an exponentially-distributed random variable. Unfortunately in practical examples this results in higher costs due to high variance in the samples. In this paper we propose a variant of continuous level Monte Carlo, where a quasi Monte Carlo sequence is utilized to “sample” the exponential random variable. We provide a complexity theorem for this novel estimator and show that this results theoretically and practically in a variance reduction of the whole estimator.","2024-02-16","2024-07-11 21:08:19","2024-08-10 19:27:51","2024-07-11 21:08:19","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2305.15949 [cs, math]","","C:\Users\isido\Zotero\storage\3TNW6EQ4\Beschle en Barth - 2024 - Complexity analysis of quasi continuous level Mont.pdf","","monte carlo; ♥♥♥; debiasing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2305.15949","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YEM93DDR","preprint","2023","Beschle, Cedric Aaron; Barth, Andrea","Quasi continuous level Monte Carlo for random elliptic PDEs","","","","","http://arxiv.org/abs/2303.08694","This paper provides a framework in which multilevel Monte Carlo and continuous level Monte Carlo can be compared. In continuous level Monte Carlo the level of refinement is determined by an exponentially distributed random variable, which therefore heavily influences the computational complexity. We propose in this paper a variant of the algorithm, where the exponentially distributed random variable is generated by a quasi Monte Carlo sequence, resulting in a significant variance reduction. In the examples presented the quasi continuous level Monte Carlo algorithm outperforms multilevel and continuous level Monte Carlo by a clear margin.","2023-10-12","2024-07-11 21:09:07","2024-08-10 19:18:18","2024-07-11 21:09:07","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2303.08694 [cs, math, stat]","","C:\Users\isido\Zotero\storage\BRLNE3TU\Beschle en Barth - 2023 - Quasi continuous level Monte Carlo for random elli.pdf","","monte carlo; PDE; random PDE; ♥♥; quasi monte carlo; debiasing; MLMC; uncertainty quantification","","","","","","","","","","","","","","","","","","","","arXiv:2303.08694","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4DI242TW","preprint","2024","Giles, Michael B.; Haji-Ali, Abdul-Lateef; Spence, Jonathan","Efficient Risk Estimation for the Credit Valuation Adjustment","","","","","http://arxiv.org/abs/2301.05886","The valuation of over-the-counter derivatives is subject to a series of valuation adjustments known as xVA, which pose additional risks for financial institutions. Associated risk measures, such as the value-at-risk of an underlying valuation adjustment, play an important role in managing these risks. Monte Carlo methods are often regarded as inefficient for computing such measures. As an example, we consider the value-at-risk of the Credit Valuation Adjustment (CVA-VaR), which can be expressed using a triple nested expectation. Traditional Monte Carlo methods are often inefficient at handling several nested expectations. Utilising recent developments in multilevel nested simulation for probabilities, we construct a hierarchical estimator of the CVA-VaR which reduces the computational complexity by 3 orders of magnitude compared to standard Monte Carlo.","2024-05-23","2024-07-11 21:11:24","2024-08-10 19:15:44","2024-07-11 21:11:24","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.05886 [q-fin]","Comment: 35 pages, 2 figures","C:\Users\isido\Zotero\storage\V8MS8843\Giles e.a. - 2024 - Efficient Risk Estimation for the Credit Valuation.pdf","","SDE; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2301.05886","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7E5PSI3","preprint","2022","Sinha, Devang; Chakrabarty, Siddhartha P.","Multilevel Monte Carlo and its Applications in Financial Engineering","","","","","http://arxiv.org/abs/2209.14549","In this article, we present a review of the recent developments on the topic of Multilevel Monte Carlo (MLMC) algorithm, in the paradigm of applications in ﬁnancial engineering. We speciﬁcally focus on the recent studies conducted in two subareas, namely, option pricing and ﬁnancial risk management. For the former, the discussion involves incorporation of the importance sampling algorithm, in conjunction with the MLMC estimator, thereby constructing a hybrid algorithm in order to achieve reduction for the overall variance of the estimator. In case of the latter, we discuss the studies carried out in order to construct an efﬁcient algorithm in order to estimate the risk measures of Value-at-Risk (VaR) and Conditional Var (CVaR), in an efﬁcient manner. In this regard, we brieﬂy discuss the motivation and the construction of an adaptive sampling algorithm with an aim to efﬁciently estimate the nested expectation, which, in general is computationally expensive.","2022-09-29","2024-07-11 21:17:58","2024-08-10 19:14:07","2024-07-11 21:17:58","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.14549 [q-fin]","<div data-schema-version=""8""><p>probably a duplicated mark it to read</p> </div>","C:\Users\isido\Zotero\storage\DP6296HN\2209.pdf","","monte carlo; to read; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2209.14549","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMRSPHQD","preprint","2024","Giles, Michael B.; Haji-Ali, Abdul-Lateef","Multilevel Path Branching for Digital Options","","","","","http://arxiv.org/abs/2209.03017","We propose a new Monte Carlo-based estimator for digital options with assets modelled by a stochastic differential equation (SDE). The new estimator is based on repeated path splitting and relies on the correlation of approximate paths of the underlying SDE that share parts of a Brownian path. Combining this new estimator with Multilevel Monte Carlo (MLMC) leads to an estimator with a computational complexity that is similar to the complexity of a MLMC estimator when applied to options with Lipschitz payoffs. This preprint includes detailed calculations and proofs (in grey colour) which are not peer-reviewed and not included in the published article.","2024-06-18","2024-07-11 21:21:20","2024-08-10 19:12:31","2024-07-11 21:21:20","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2209.03017 [cs, math, q-fin]","","C:\Users\isido\Zotero\storage\IZXF996M\Giles en Haji-Ali - 2024 - Multilevel Path Branching for Digital Options.pdf","","monte carlo; SDE; ♥♥♥; branching; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2209.03017","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VSGCIXUD","preprint","2021","Gerstner, Thomas; Harrach, Bastian; Roth, Daniel; Simon, Martin","Multilevel Monte Carlo learning","","","","","http://arxiv.org/abs/2102.08734","In this work, we study the approximation of expected values of functional quantities on the solution of a stochastic diﬀerential equation (SDE), where we replace the Monte Carlo estimation with the evaluation of a deep neural network. Once the neural network training is done, the evaluation of the resulting approximating function is computationally highly eﬃcient so that using deep neural networks to replace costly Monte Carlo integration is appealing, e.g., for near real-time computations in quantitative ﬁnance. However, the drawback of these nowadays widespread ideas lies in the fact that training a suitable neural network is likely to be prohibitive in terms of computational cost. We address this drawback here by introducing a multilevel approach to the training of deep neural networks. More precisely, we combine the deep learning algorithm introduced by Beck et al. Beck et al. (2018) with the idea of multilevel Monte Carlo path simulation of Giles Giles (2008a). The idea is to train several neural networks, each having a certain approximation quality and computational complexity, with training data computed from so-called level estimators, introduced by Giles Giles (2008a). We show that under certain assumptions, the variance in the training process can be reduced by shifting most of the computational workload to training neural nets at coarse levels where producing the training data sets is comparably cheap, whereas training the neural nets corresponding to the ﬁne levels requires only a limited number of training data sets. We formulate a complexity theorem showing that the multilevel idea can indeed reduce computational complexity.","2021-02-17","2024-07-11 21:32:04","2024-08-10 19:11:08","2024-07-11 21:32:04","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2102.08734 [cs, math]","","C:\Users\isido\Zotero\storage\MY9363I7\Gerstner e.a. - 2021 - Multilevel Monte Carlo learning.pdf","","monte carlo; SDE; parametric; deep learning; ♥♥; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:2102.08734","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L68BT3I8","journalArticle","2021","An, Dong; Linden, Noah; Liu, Jin-Peng; Montanaro, Ashley; Shao, Changpeng; Wang, Jiasu","Quantum-accelerated multilevel Monte Carlo methods for stochastic differential equations in mathematical finance","Quantum","","2521-327X","10.22331/q-2021-06-24-481","http://arxiv.org/abs/2012.06283","Inspired by recent progress in quantum algorithms for ordinary and partial differential equations, we study quantum algorithms for stochastic differential equations (SDEs). Firstly we provide a quantum algorithm that gives a quadratic speed-up for multilevel Monte Carlo methods in a general setting. As applications, we apply it to compute expectation values determined by classical solutions of SDEs, with improved dependence on precision. We demonstrate the use of this algorithm in a variety of applications arising in mathematical finance, such as the Black-Scholes and Local Volatility models, and Greeks. We also provide a quantum algorithm based on sublinear binomial sampling for the binomial option pricing model with the same improvement.","2021-06-24","2024-07-11 21:33:32","2024-08-10 18:45:47","2024-07-11 21:33:32","481","","","5","","Quantum","","","","","","","","en","","","","","arXiv.org","","arXiv:2012.06283 [quant-ph, q-fin]","Comment: 37 pages, 6 figures","C:\Users\isido\Zotero\storage\RBRE3RIU\2012.pdf","","monte carlo; SDE; ♥; quantum computing; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BF9D9ZB9","webpage","2024","","parrallel in time site","","","","","https://parallel-in-time.org/methods/","","2024-07-12","2024-07-12 06:19:50","2024-08-12 14:55:33","2024-07-12 06:19:50","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\QSFVTIE5\methods.html","","parallel in time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZULDWU29","journalArticle","2012","Emmett, Matthew; Minion, Michael","Toward an efficient parallel in time method for partial differential equations","Communications in Applied Mathematics and Computational Science","","2157-5452, 1559-3940","10.2140/camcos.2012.7.105","http://msp.org/camcos/2012/7-1/p04.xhtml","Emmett was supported by the Director, DOE Ofﬁce of Science, Ofﬁce of Advanced Scientiﬁc Computing Research, Ofﬁce of Mathematics, Information, and Computational Sciences, Applied Mathematical Sciences Program, under contract DE-SC0004011. Minion’s work was supported by the Alexander von Humboldt Foundation and the Director, DOE Ofﬁce of Science, Ofﬁce of Advanced Scientiﬁc Computing Research, Ofﬁce of Mathematics, Information, and Computational Sciences, Applied Mathematical Sciences Program, under contract DE-SC0004011 and by the National Science Foundation under contract DMS-0854961. MSC2010: 65M99.","2012-03-28","2024-07-12 06:57:14","2024-08-10 18:42:09","2024-07-12 06:57:14","105-132","","1","7","","CAMCoS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\8VSRA736\Emmett en Minion - 2012 - Toward an efficient parallel in time method for pa.pdf","","PDE; parallel in time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZYKDBHK","conferencePaper","2012","Speck, R.; Ruprecht, D.; Krause, R.; Emmett, M.; Minion, M.; Winkel, M.; Gibbon, P.","A massively space-time parallel N-body solver","2012 International Conference for High Performance Computing, Networking, Storage and Analysis","978-1-4673-0805-2 978-1-4673-0806-9","","10.1109/SC.2012.6","http://ieeexplore.ieee.org/document/6468522/","We present a novel space-time parallel version of the Barnes-Hut tree code PEPC using PFASST, the Parallel Full Approximation Scheme in Space and Time. The naive use of increasingly more processors for a ﬁxed-size N-body problem is prone to saturate as soon as the number of unknowns per core becomes too small. To overcome this intrinsic strongscaling limit, we introduce temporal parallelism on top of PEPC’s existing hybrid MPI/PThreads spatial decomposition. Here, we use PFASST which is based on a combination of the iterations of the parallel-in-time algorithm parareal with the sweeps of spectral deferred correction (SDC) schemes. By combining these sweeps with multiple space-time discretization levels, PFASST relaxes the theoretical bound on parallel efﬁciency in parareal. We present results from runs on up to 262,144 cores on the IBM Blue Gene/P installation JUGENE, demonstrating that the spacetime parallel code provides speedup beyond the saturation of the purely space-parallel approach.","2012-11","2024-07-12 07:13:37","2024-08-10 18:41:48","2024-07-12 07:13:37","1-11","","","","","","","","","","","IEEE","Salt Lake City, UT","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\HPBSA2IL\Speck e.a. - 2012 - A massively space-time parallel N-body solver.pdf","","parallel in time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2012 SC - International Conference for High Performance Computing, Networking, Storage and Analysis","","","","","","","","","","","","","","",""
"JFHNZL3R","preprint","2024","Ibrahim, Abdul Qadir; Götschel, Sebastian; Ruprecht, Daniel","Space-time parallel scaling of Parareal with a Fourier Neural Operator as coarse propagator","","","","","http://arxiv.org/abs/2404.02521","Iterative parallel-in-time algorithms like Parareal can extend scaling beyond the saturation of purely spatial parallelization when solving initial value problems. However, they require the user to build coarse models to handle the inevitably serial transport of information in time. This is a time consuming and difficult process since there is still only limited theoretical insight into what constitutes a good and efficient coarse model. Novel approaches from machine learning to solve differential equations could provide a more generic way to find coarse level models for parallel-in-time algorithms. This paper demonstrates that a physics-informed Fourier Neural Operator (PINO) is an effective coarse model for the parallelization in time of the two-asset Black-Scholes equation using Parareal. We demonstrate that PINO-Parareal converges as fast as a bespoke numerical coarse model and that, in combination with spatial parallelization by domain decomposition, it provides better overall speedup than both purely spatial parallelization and space-time parallelizaton with a numerical coarse propagator.","2024-04-03","2024-07-12 08:21:04","2024-08-10 18:41:14","2024-07-12 08:21:04","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.02521 [cs, math]","Comment: 13 pages, 9 figures","C:\Users\isido\Zotero\storage\H36B86II\Ibrahim e.a. - 2024 - Space-time parallel scaling of Parareal with a Fou.pdf","","parareal; deep learning; parallel in time","","","","","","","","","","","","","","","","","","","","arXiv:2404.02521","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LKBIZAEL","journalArticle","2023","Zhou, Zhengqing; Wang, Guanyang; Blanchet, Jose; Glynn, Peter W.","Unbiased Optimal Stopping via the MUSE","Stochastic Processes and their Applications","","03044149","10.1016/j.spa.2022.12.007","http://arxiv.org/abs/2106.02263","We propose a new unbiased estimator for estimating the utility of the optimal stopping problem. The MUSE, short for ‘Multilevel Unbiased Stopping Estimator’, constructs the unbiased Multilevel Monte Carlo (MLMC) estimator at every stage of the optimal stopping problem in a backward recursive way. In contrast to traditional sequential methods, the MUSE can be implemented in parallel. We prove the MUSE has ﬁnite variance, ﬁnite computational complexity, and achieves ε-accuracy with O(1/ε2) computational cost under mild conditions. We demonstrate MUSE empirically in an option pricing problem involving a high-dimensional input and the use of many parallel processors.","2023-12","2024-07-12 21:23:22","2024-08-10 18:39:00","2024-07-12 21:23:22","104088","","","166","","Stochastic Processes and their Applications","","","","","","","","en","","","","","arXiv.org","","arXiv:2106.02263 [math, q-fin, stat]","Comment: 39 pages, add several numerical experiments and technical results, accepted by Stochastic Processes and their Applications","C:\Users\isido\Zotero\storage\63XJD8D8\2106.pdf","","monte carlo; american option; ♥♥♥♥♥; debiasing; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNM43D5K","book","2020","Glynn, Peter; Peng, Yijie; Fu, Michael; Hu, Jianqiang","Submitted to INFORMS Journal on Computing manuscript (Please, provide the mansucript number!) Computing Sensitivities for Distortion Risk Measures","","","","","","Distortion risk measure, defined by an integral of a distorted tail probability, has been widely used in behavioral economics and risk management as an alternative to expected utility. The sensitivity of the distortion risk measure is a functional of certain distribution sensitivities. We propose a new sensitivity estimator for the distortion risk measure that uses the generalized likelihood ratio estimators in Peng et al. (2020) for distribution sensitivities as input and establish a central limit theorem for the new estimator. The proposed estimator can handle discontinuous sample paths and distortion functions.","2020-08-05","2024-07-12 21:25:02","2024-07-12 21:25:02","","","","","","","","","","","","","","","","","","","","ResearchGate","","DOI: 10.13140/RG.2.2.26179.63525","","","https://www.researchgate.net/profile/Yijie-Peng/publication/343444038_Submitted_to_INFORMS_Journal_on_Computing_manuscript_Please_provide_the_mansucript_number_Computing_Sensitivities_for_Distortion_Risk_Measures/links/5f2a7b4e299bf13404a296cf/Submitted-to-INFORMS-Journal-on-Computing-manuscript-Please-provide-the-mansucript-number-Computing-Sensitivities-for-Distortion-Risk-Measures.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLXLK2M4","preprint","2024","Przybyłowicz, Paweł; Sobieraj, Michał","On the randomized Euler scheme for SDEs with integral-form drift","","","","","http://arxiv.org/abs/2405.20481","In this paper, we investigate the problem of strong approximation of the solution of SDEs in the case when the drift coefficient is given in the integral form. Such drift often appears when analyzing stochastic dynamics of optimization procedures in machine learning problems. We discuss connections of the defined randomized Euler approximation scheme with the perturbed version of the stochastic gradient descent (SGD) algorithm. We investigate its upper error bounds, in terms of the discretization parameter n and the size M of the random sample drawn at each step of the algorithm, in different subclasses of coefficients of the underlying SDE. Finally, the results of numerical experiments performed by using GPU architecture are also reported.","2024-05-30","2024-07-13 19:51:36","2024-08-10 17:37:24","2024-07-13 19:51:36","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.20481 [cs, math]","","C:\Users\isido\Zotero\storage\7MKVPDZ2\Przybyłowicz en Sobieraj - 2024 - On the randomized Euler scheme for SDEs with integ.pdf","","SDE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2405.20481","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJGP5T6Z","preprint","2024","Du, Jiarui; He, Zhijian","Unbiased Markov chain quasi-Monte Carlo for Gibbs samplers","","","","","http://arxiv.org/abs/2403.04407","In statistical analysis, Monte Carlo (MC) stands as a classical numerical integration method. When encountering challenging sample problem, Markov chain Monte Carlo (MCMC) is a commonly employed method. However, the MCMC estimator is biased after a ﬁxed number of iterations. Unbiased MCMC, an advancement achieved through coupling techniques, addresses this bias issue in MCMC. It allows us to run many short chains in parallel. Quasi-Monte Carlo (QMC), known for its high order of convergence, is an alternative of MC. By incorporating the idea of QMC into MCMC, Markov chain quasi-Monte Carlo (MCQMC) eﬀectively reduces the variance of MCMC, especially in Gibbs samplers. This work presents a novel approach that integrates unbiased MCMC with MCQMC, called as an unbiased MCQMC method. This method renders unbiased estimators while improving the rate of convergence signiﬁcantly. Numerical experiments demonstrate that unbiased MCQMC with a sample size of N achieves convergence rates of approximately O(N −1) in moderate dimensions for Gibbs sampling problems. In the setting of parallelization, unbiased MCQMC also performs better than unbiased MCMC, even running with short chains.","2024-03-31","2024-07-13 21:38:41","2024-08-10 17:26:51","2024-07-13 21:38:41","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.04407 [cs, math]","","C:\Users\isido\Zotero\storage\C9B7N2BQ\Du en He - 2024 - Unbiased Markov chain quasi-Monte Carlo for Gibbs .pdf","","monte carlo; quasi monte carlo; to read; debiasing; MCMC","","","","","","","","","","","","","","","","","","","","arXiv:2403.04407","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXNIUWSA","preprint","2023","Bouillon, Arne; Samaey, Giovanni; Meerbergen, Karl","Diagonalization-based preconditioners and generalized convergence bounds for ParaOpt","","","","","http://arxiv.org/abs/2304.09235","The ParaOpt algorithm was recently introduced as a time-parallel solver for optimalcontrol problems with a terminal-cost objective, and convergence results have been presented for the linear diﬀusive case with implicit-Euler time integrators. We reformulate ParaOpt for tracking problems and provide generalized convergence analyses for both objectives. We focus on linear diﬀusive equations and prove convergence bounds that are generic in the time integrators used. For large problem dimensions, ParaOpt’s performance depends crucially on having a good preconditioner to solve the arising linear systems. For the case where ParaOpt’s cheap, coarse-grained propagator is linear, we introduce diagonalization-based preconditioners inspired by recent advances in the ParaDiag family of methods. These preconditioners not only lead to a weakly-scalable ParaOpt version, but are themselves invertible in parallel, making maximal use of available concurrency. They have proven convergence properties in the linear diﬀusive case that are generic in the time discretization used, similarly to our ParaOpt results. Numerical results conﬁrm that the iteration count of the iterative solvers used for ParaOpt’s linear systems becomes constant in the limit of an increasing processor count. The paper is accompanied by a sequential Matlab implementation.","2023-05-08","2024-07-13 21:48:09","2024-08-10 17:21:29","2024-07-13 21:48:09","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2304.09235 [cs, math]","","C:\Users\isido\Zotero\storage\KY6EG6GW\Bouillon e.a. - 2023 - Diagonalization-based preconditioners and generali.pdf","","ODE; ♥; parallel in time","","","","","","","","","","","","","","","","","","","","arXiv:2304.09235","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T36PKKKI","preprint","2024","Maes, Vince; Bossuyt, Ignace; Vandecasteele, Hannes; Dekeyser, Wouter; Koellermeier, Julian; Baelmans, Martine; Samaey, Giovanni","Predicting the statistical error of analog particle tracing Monte Carlo","","","","","http://arxiv.org/abs/2404.00315","Large particle systems are often described by high-dimensional (linear) kinetic equations that are simulated using Monte Carlo methods for which the asymptotic convergence rate is independent of the dimensionality. Even though the asymptotic convergence rate is known, predicting the actual value of the statistical error remains a challenging problem. In this paper, we show how the statistical error of an analog particle tracing Monte Carlo method can be calculated (expensive) and predicted a priori (cheap) when estimating quantities of interest (QoI) on a histogram. We consider two types of QoI estimators: point estimators for which each particle provides one independent contribution to the QoI estimates, and analog estimators for which each particle provides multiple correlated contributions to the QoI estimates. The developed statistical error predictors can be applied to other QoI estimators and nonanalog simulation routines as well. The error analysis is based on interpreting the number of particle visits to a histogram bin as the result of a (correlated) binomial experiment. The resulting expressions can be used to optimize (non)analog particle tracing Monte Carlo methods and hybrid simulation methods involving a Monte Carlo component, as well as to select an optimal particle tracing Monte Carlo method from several available options. Additionally, the cheap statistical error predictors can be used to determine a priori the number of particles N that is needed to reach a desired accuracy. We illustrate the theory using a linear kinetic equation describing neutral particles in the plasma edge of a fusion device and show numerical results. The code used to perform the numerical experiments is openly available.","2024-03-30","2024-07-13 22:00:29","2024-08-10 17:20:10","2024-07-13 22:00:29","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.00315 [physics]","","C:\Users\isido\Zotero\storage\DDSMU87C\Maes e.a. - 2024 - Predicting the statistical error of analog particl.pdf","","monte carlo; ♥; kinetic equations","","","","","","","","","","","","","","","","","","","","arXiv:2404.00315","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3W9WZSZ","preprint","2023","Ouyang, Du; Wang, Xiaoqun; He, Zhijian","Quasi-Monte Carlo for unbounded integrands with importance sampling","","","","","http://arxiv.org/abs/2310.00650","We consider the problem of estimating an expectation E [h(W )] by quasi-Monte Carlo (QMC) methods, where h is an unbounded smooth function on Rd and W is a standard normal distributed random variable. To study rates of convergence for QMC on unbounded integrands, we use a smoothed projection operator to project the output of W to a bounded region, which differs from the strategy of avoiding the singularities along the boundary of the unit cube [0, 1]d in [24]. The error is then bounded by the quadrature error of the transformed integrand and the projection error. If the function h(x) and its mixed partial derivatives do not grow too fast as the Euclidean norm |x| goes to infinity, we obtain an error rate of O(n−1+ε) for QMC and randomized QMC (RQMC) with a sample size n and an arbitrarily small ε > 0. However, the rate turns out to be O(n−1+2M+ε) if the functions grow exponentially with a rate of O(exp{M |x|2}) for a constant M ∈ (0, 1/2). Superisingly, we find that using importance sampling with t distribution as the proposal can improve the root mean squared error of RQMC from O(n−1+2M+ε) to O(n−3/2+ε) for any M ∈ (0, 1/2).","2023-10-08","2024-07-13 22:31:47","2024-08-10 17:18:20","2024-07-13 22:31:47","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.00650 [cs, math]","","C:\Users\isido\Zotero\storage\XGEU7BCF\Ouyang e.a. - 2023 - Quasi-Monte Carlo for unbounded integrands with im.pdf","","monte carlo; importance sampling; ♥♥; variance reduction; quasi monte carlo","","","","","","","","","","","","","","","","","","","","arXiv:2310.00650","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K7QH33AB","journalArticle","2022","Croci, Matteo; Fasi, Massimiliano; Higham, Nicholas J; Mary, Théo; Mikaitis, Mantas","Stochastic Rounding: Implementation, Error Analysis, and Applications","Royal Society Open Science","","","","https://hal.science/hal-03378080","Stochastic rounding randomly maps a real number to one of the two nearest values in a finite precision number system. First proposed for use in computer arithmetic in the 1950s, it is attracting renewed interest. If used in floating-point arithmetic in the computation of the inner product of two vectors of length n, it yields an error bounded by √nu with high probability, where u is the unit roundoff, which is not necessarily the case for round to nearest. A particular attraction of stochastic rounding is that, unlike round to nearest, it is immune to the phenomenon of stagnation, whereby a sequence of tiny updates to a relatively large quantity are lost. We survey stochastic rounding, covering its mathematical properties and probabilistic error analysis, its implementation, and its use in applications, including deep learning and the numerical solution of differential equations.","2022","2024-07-14 14:43:29","2024-08-10 17:16:18","2024-07-14 14:43:29","","","","","","","Stochastic Rounding","","","","","","","","","","","","HAL Archives Ouvertes","","Publisher: The Royal Society","","C:\Users\isido\Zotero\storage\TABFMBEI\Croci e.a. - 2022 - Stochastic Rounding Implementation, Error Analysi.pdf","","monte carlo; ♥; stochastic rounding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6V8T72W","videoRecording","2020","MCQMC 2020","Nikolaus Binder – Solving Integral Equations in real-time","","","","","https://www.youtube.com/watch?v=T4n18HMuqB0","This talk is part of MCQMC 2020, the 14th International Conference in Monte Carlo & Quasi-Monte Carlo Methods in Scientific Computing. (https://mcqmc20.web.ox.ac.uk) It is part of the special session ""Variance Reduction"".","2020-08-21","2024-07-15 10:31:37","2024-08-10 17:13:57","2024-07-15 10:31:37","","","","","","","","","","","","","","","","","","","YouTube","","","","","","integral equations; ♥; variance reduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7I6CKC5Y","videoRecording","2020","MCQMC 2020","François-Xavier Briol – Learning to Reduce Variance Using Stochastic Gradient Descent","","","","","https://www.youtube.com/watch?v=6MheW58gyKA","This talk is part of MCQMC 2020, the 14th International Conference in Monte Carlo & Quasi-Monte Carlo Methods in Scientific Computing. (https://mcqmc20.web.ox.ac.uk) It is part of the minisymposium ""Stein’s Method in Computational Statistics"".","2020-08-14","2024-07-15 19:21:04","2024-08-10 17:12:33","2024-07-15 19:21:04","","","","","","","","","","","","","","","","","","","YouTube","","","","","","monte carlo; SGD; ♥♥♥; variance reduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"REEJZR8E","bookSection","2020","Løvbak, Emil; Mortier, Bert; Samaey, Giovanni; Vandewalle, Stefan","Multilevel Monte Carlo with Improved Correlation for Kinetic Equations in the Diffusive Scaling","Computational Science – ICCS 2020","978-3-030-50432-8 978-3-030-50433-5","","","https://link.springer.com/10.1007/978-3-030-50433-5_29","In many applications, it is necessary to compute the timedependent distribution of an ensemble of particles subject to transport and collision phenomena. Kinetic equations are PDEs that model such particles in a position-velocity phase space. In the low collisional regime, explicit particle-based Monte Carlo methods simulate these high dimensional equations eﬃciently, but, as the collision rate increases, these methods suﬀer from severe time-step constraints. In the high collision regime, asymptotic-preserving particle schemes are able to produce stable results. However, this stability comes at the cost of a bias in the computed results. In earlier work, the multilevel Monte Carlo method was used to reduce this bias by combining simulations with large and small time steps. This multilevel scheme, however, still has large variances when correlating ﬁne and coarse simulations, which leads to suboptimal multilevel performance. In this work, we present an improved correlation approach that decreases the variance when bridging the gap from large time steps to time steps of the order of magnitude of the collision rate. We further demonstrate that this reduced variance results in a sharply reduced simulation cost at the expense of a small bias.","2020","2024-07-16 11:23:54","2024-08-10 17:10:05","2024-07-16 11:23:54","374-388","","","12142","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-50433-5_29","","C:\Users\isido\Zotero\storage\PFBMEPEZ\Løvbak e.a. - 2020 - Multilevel Monte Carlo with Improved Correlation f.pdf","","monte carlo; PDE; ♥♥; kinetic equations; MLMC","","Krzhizhanovskaya, Valeria V.; Závodszky, Gábor; Lees, Michael H.; Dongarra, Jack J.; Sloot, Peter M. A.; Brissos, Sérgio; Teixeira, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P73RPF4Y","preprint","2017","Haji-Ali, Abdul-Lateef; Tempone, Raul","Multilevel and Multi-index Monte Carlo methods for the McKean-Vlasov equation","","","","","http://arxiv.org/abs/1610.09934","We address the approximation of functionals depending on a system of particles, described by stochastic diﬀerential equations (SDEs), in the mean-ﬁeld limit when the number of particles approaches inﬁnity. This problem is equivalent to estimating the weak solution of the limiting McKean-Vlasov SDE. To that end, our approach uses systems with ﬁnite numbers of particles and a time-stepping scheme. In this case, there are two discretization parameters: the number of time steps and the number of particles. Based on these two parameters, we consider diﬀerent variants of the Monte Carlo and Multilevel Monte Carlo (MLMC) methods and show that, in the best case, the optimal work complexity of MLMC, to estimate the functional in one typical setting with an error tolerance of TOL, is O TOL−3 . We also consider a method that uses the recent Multi-index Monte Carlo method and show an improved work complexity in the same typical setting of O TOL−2 log(TOL−1)2 . Our numerical experiments are carried out on the so-called Kuramoto model, a system of coupled oscillators.","2017-05-01","2024-07-16 12:42:45","2024-08-10 17:05:23","2024-07-16 12:42:45","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1610.09934 [math]","<div data-citation-items=""%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10093548%2Fitems%2FP73RPF4Y%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10093548%2Fitems%2FP73RPF4Y%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22We%20address%20the%20approximation%20of%20functionals%20depending%20on%20a%20system%20of%20particles%2C%20described%20by%20stochastic%20di%EF%AC%80erential%20equations%20(SDEs)%2C%20in%20the%20mean-%EF%AC%81eld%20limit%20when%20the%20number%20of%20particles%20approaches%20in%EF%AC%81nity.%20This%20problem%20is%20equivalent%20to%20estimating%20the%20weak%20solution%20of%20the%20limiting%20McKean-Vlasov%20SDE.%20To%20that%20end%2C%20our%20approach%20uses%20systems%20with%20%EF%AC%81nite%20numbers%20of%20particles%20and%20a%20time-stepping%20scheme.%20In%20this%20case%2C%20there%20are%20two%20discretization%20parameters%3A%20the%20number%20of%20time%20steps%20and%20the%20number%20of%20particles.%20Based%20on%20these%20two%20parameters%2C%20we%20consider%20di%EF%AC%80erent%20variants%20of%20the%20Monte%20Carlo%20and%20Multilevel%20Monte%20Carlo%20(MLMC)%20methods%20and%20show%20that%2C%20in%20the%20best%20case%2C%20the%20optimal%20work%20complexity%20of%20MLMC%2C%20to%20estimate%20the%20functional%20in%20one%20typical%20setting%20with%20an%20error%20tolerance%20of%20TOL%2C%20is%20O%20TOL%E2%88%923%20.%20We%20also%20consider%20a%20method%20that%20uses%20the%20recent%20Multi-index%20Monte%20Carlo%20method%20and%20show%20an%20improved%20work%20complexity%20in%20the%20same%20typical%20setting%20of%20O%20TOL%E2%88%922%20log(TOL%E2%88%921)2%20.%20Our%20numerical%20experiments%20are%20carried%20out%20on%20the%20so-called%20Kuramoto%20model%2C%20a%20system%20of%20coupled%20oscillators.%22%2C%22language%22%3A%22en%22%2C%22note%22%3A%22arXiv%3A1610.09934%20%5Bmath%5D%22%2C%22number%22%3A%22arXiv%3A1610.09934%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Multilevel%20and%20Multi-index%20Monte%20Carlo%20methods%20for%20the%20McKean-Vlasov%20equation%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1610.09934%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Haji-Ali%22%2C%22given%22%3A%22Abdul-Lateef%22%7D%2C%7B%22family%22%3A%22Tempone%22%2C%22given%22%3A%22Raul%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222024%22%2C7%2C16%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222017%22%2C5%2C1%5D%5D%7D%7D%7D%5D"" data-schema-version=""8""><p>dont like the <span class=""highlight"" data-annotation=""%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10093548%2Fitems%2F4GACUAE8%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B219.342%2C657.28%2C392.66%2C674.478%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10093548%2Fitems%2FP73RPF4Y%22%5D%2C%22locator%22%3A%221%22%7D%7D"">“McKean-Vlasov equation”</span> equation</p> </div>","C:\Users\isido\Zotero\storage\4GACUAE8\Haji-Ali en Tempone - 2017 - Multilevel and Multi-index Monte Carlo methods for.pdf","","monte carlo; SDE; ♥; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1610.09934","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DG62CS55","webpage","2024","Giles, Mike","MLMC research groups","","","","","https://people.maths.ox.ac.uk/gilesm/mlmc_community.html","In this page I attempt to list the research groups working on multilevel Monte Carlo methods, and the main papers that I am aware of, grouped by topic.","2024-07-17","2024-07-17 10:32:19","2024-08-12 14:19:43","2024-07-17 10:32:19","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\LWHSQPX8\mlmc_community.html","","♥♥♥♥♥; MLMC; monte carlo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NGGZQ3Y","journalArticle","2023","Sinha, Devang; Chakrabarty, Siddhartha P.","A review of efficient Multilevel Monte Carlo algorithms for derivative pricing and risk management","MethodsX","","2215-0161","10.1016/j.mex.2023.102078","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9975692/","•               Brief overview of the Multilevel Monte Carlo estimator.                                         •               Review of the importance sampling algorithm to reduce the overall variance of the MLMC estimator associated with the option pricing problems in financial engineering.                                         •               Review of extension of adaptive sampling algorithm to multilevel paradigm, with the aim of improving the computational complexity while estimating the Value-at-Risk (VaR) and Conditional VaR (CVaR).                                 , In this article, we present a review of the recent developments on the topic of Multilevel Monte Carlo (MLMC) algorithms, in the paradigm of applications in financial engineering. We specifically focus on the recent studies conducted in two subareas, namely, option pricing and financial risk management. For the former, the discussion involves incorporation of the importance sampling algorithm, in conjunction with the MLMC estimator, thereby constructing a hybrid algorithms in order to achieve reduction for the overall variance of the estimator. In case of the latter, we discuss the studies carried out in order to construct an efficient algorithm in order to estimate the risk measures of Value-at-Risk (VaR) and Conditional Var (CVaR). In this regard, we briefly discuss the motivation and the construction of an adaptive sampling algorithm with an aim to efficiently estimate the nested expectation, which, in general is computationally expensive.","2023-02-11","2024-07-17 11:51:27","2024-08-10 17:01:32","2024-07-17 11:51:27","102078","","","10","","MethodsX","","","","","","","","","","","","","PubMed Central","","PMID: 36875343 PMCID: PMC9975692","","C:\Users\isido\Zotero\storage\7FRC7ZTK\Sinha en Chakrabarty - 2023 - A review of efficient Multilevel Monte Carlo algor.pdf; ","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9975692/","monte carlo; ♥; option pricing; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RD525TB7","journalArticle","2020","Ben Hammouda, Chiheb; Ben Rached, Nadhir; Tempone, Raúl","Importance sampling for a robust and efficient multilevel Monte Carlo estimator for stochastic reaction networks","Statistics and Computing","","0960-3174, 1573-1375","10.1007/s11222-020-09965-3","https://link.springer.com/10.1007/s11222-020-09965-3","The multilevel Monte Carlo (MLMC) method for continuous-time Markov chains, ﬁrst introduced by Anderson and Higham (SIAM Multiscal Model Simul 10(1):146–179, 2012), is a highly efﬁcient simulation technique that can be used to estimate various statistical quantities for stochastic reaction networks, in particular for stochastic biological systems. Unfortunately, the robustness and performance of the multilevel method can be affected by the high kurtosis, a phenomenon observed at the deep levels of MLMC, which leads to inaccurate estimates of the sample variance. In this work, we address cases where the highkurtosis phenomenon is due to catastrophic coupling (characteristic of pure jump processes where coupled consecutive paths are identical in most of the simulations, while differences only appear in a tiny proportion) and introduce a pathwise-dependent importance sampling (IS) technique that improves the robustness and efﬁciency of the multilevel method. Our theoretical results, along with the conducted numerical experiments, demonstrate that our proposed method signiﬁcantly reduces the kurtosis of the deep levels of MLMC, and also improves the strong convergence rate from β = 1 for the standard case (without IS), to β = 1+δ, where 0 < δ < 1 is a user-selected parameter in our IS algorithm. Due to the complexity theorem of MLMC, and given a pre-selected tolerance, TOL, this results in an improvement of the complexity from O TOL−2 log(TOL)2 in the standard case to O TOL−2 , which is the optimal complexity of the MLMC estimator. We achieve all these improvements with a negligible additional cost since our IS algorithm is only applied a few times across each simulated path.","2020-11","2024-07-17 11:55:48","2024-08-10 17:00:17","2024-07-17 11:55:48","1665-1689","","6","30","","Stat Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\SG47U3SJ\Ben Hammouda e.a. - 2020 - Importance sampling for a robust and efficient mul.pdf","","monte carlo; importance sampling; ♥♥♥♥; MLMC; stochastic reaction networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R65UF2X8","videoRecording","2017","Centre International de Rencontres Mathématiques","Raúl Tempone: Adaptive strategies for Multilevel Monte Carlo","","","","","https://www.youtube.com/watch?v=CZSESgi34kI","Abstract: We will first recall, for a general audience, the use of Monte Carlo and Multi-level Monte Carlo methods in the context of Uncertainty Quantification. Then we will discuss the recently developed Adaptive Multilevel Monte Carlo (MLMC) Methods for (i) It Stochastic Differential Equations, (ii) Stochastic Reaction Networks modeled by Pure Jump Markov Processes and (iii) Partial Differential Equations with random inputs. In this context, the notion of adaptivity includes several aspects such as mesh refinements based on either a priori or a posteriori error estimates, the local choice of different time stepping methods and the selection of the total number of levels and the number of samples at different levels. Our Adaptive MLMC estimator uses a hierarchy of adaptively refined, non-uniform time discretizations, and, as such, it may be considered a generalization of the uniform discretization MLMC method introduced independently by M. Giles and S. Heinrich. In particular, we show that our adaptive MLMC algorithms are asymptotically accurate and have the correct complexity with an improved control of the multiplicative constant factor in the asymptotic analysis. In this context, we developed novel techniques for estimation of parameters needed in our MLMC algorithms, such as the variance of the difference between consecutive approximations. These techniques take particular care of the deepest levels, where for efficiency reasons only few realizations are available to produce essential estimates. Moreover, we show the asymptotic normality of the statistical error in the MLMC estimator, justifying in this way our error estimate that allows prescribing both the required accuracy and confidence level in the final result. We present several examples to illustrate the above results and the corresponding computational savings. Recording during the CEMRACS Summer school 2017 : ""Numerical Methods for Stochastic Models: Control, Uncertainty Quantification, Mean-field "" the July 20, 2017 at the Centre International de Rencontres Mathématiques (Marseille, France) Filmmaker: Guillaume Hennenfent Find this video and other talks given by worldwide mathematicians on CIRM's Audiovisual Mathematics Library: http://library.cirm-math.fr. And discover all its functionalities:  - Chapter markers and keywords to watch the parts of your choice in the video - Videos enriched with abstracts, bibliographies, Mathematics Subject Classification - Multi-criteria search by author, title, tags, mathematical area","2017-08-11","2024-07-18 09:40:20","2024-08-10 16:57:04","2024-07-18 09:40:20","","","","","","","Raúl Tempone","","","","","","","","","","","","YouTube","","","","","","monte carlo; PDE; SDE; MLMC; stochastic reaction networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N835QJ39","preprint","2015","Haji-Ali, Abdul-Lateef; Nobile, Fabio; Tempone, Raul","Multi-Index Monte Carlo: When Sparsity Meets Sampling","","","","","http://arxiv.org/abs/1405.3757","We propose and analyze a novel Multi-Index Monte Carlo (MIMC) method for weak approximation of stochastic models that are described in terms of diﬀerential equations either driven by random measures or with random coeﬃcients. The MIMC method is both a stochastic version of the combination technique introduced by Zenger, Griebel and collaborators and an extension of the Multilevel Monte Carlo (MLMC) method ﬁrst described by Heinrich and Giles. Inspired by Giles’s seminal work, we use in MIMC high-order mixed diﬀerences instead of using ﬁrst-order diﬀerences as in MLMC to reduce the variance of the hierarchical diﬀerences dramatically. This in turn yields new and improved complexity results, which are natural generalizations of Giles’s MLMC analysis and which increase the domain of the problem parameters for which we achieve the optimal convergence, O(TOL−2). Moreover, in MIMC, the rate of increase of required memory with respect to TOL is independent of the number of directions up to a logarithmic term which allows far more accurate solutions to be calculated for higher dimensions than what is possible when using MLMC.","2015-03-25","2024-07-18 11:33:18","2024-08-10 16:55:48","2024-07-18 11:33:18","","","","","","","Multi-Index Monte Carlo","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1405.3757 [math]","","C:\Users\isido\Zotero\storage\7FE3Y8MV\Haji-Ali e.a. - 2015 - Multi-Index Monte Carlo When Sparsity Meets Sampl.pdf","","monte carlo; PDE; ♥♥♥♥♥; MLMC; uncertainty quantification","","","","","","","","","","","","","","","","","","","","arXiv:1405.3757","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QSMV9JX","preprint","2024","Nam, Hong Chul; Berner, Julius; Anandkumar, Anima","Solving Poisson Equations using Neural Walk-on-Spheres","","","","","http://arxiv.org/abs/2406.03494","We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the efficient solution of high-dimensional Poisson equations. Leveraging stochastic representations and Walk-onSpheres methods, we develop novel losses for neural networks based on the recursive solution of Poisson equations on spheres inside the domain. The resulting method is highly parallelizable and does not require spatial gradients for the loss. We provide a comprehensive comparison against competing methods based on PINNs, the Deep Ritz method, and (backward) stochastic differential equations. In several challenging, high-dimensional numerical examples, we demonstrate the superiority of NWoS in accuracy, speed, and computational costs. Compared to commonly used PINNs, our approach can reduce memory usage and errors by orders of magnitude. Furthermore, we apply NWoS to problems in PDEconstrained optimization and molecular dynamics to show its efficiency in practical applications.","2024-06-05","2024-07-18 12:34:44","2024-08-10 16:54:00","2024-07-18 12:34:44","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2406.03494 [cs, math, stat]","Comment: Accepted at ICML 2024","C:\Users\isido\Zotero\storage\YKYKIM9X\Nam e.a. - 2024 - Solving Poisson Equations using Neural Walk-on-Sph.pdf","","monte carlo; PDE; walk on spheres; deep learning; ♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2406.03494","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5JLEDEU","preprint","2017","Kyprianou, Andreas E.; Osojnik, Ana; Shardlow, Tony","Unbiased `walk-on-spheres' Monte Carlo methods for the fractional Laplacian","","","","","http://arxiv.org/abs/1609.03127","We consider Monte Carlo methods for simulating solutions to the analogue of the Dirichlet boundary-value problem in which the Laplacian is replaced by the fractional Laplacian and boundary conditions are replaced by conditions on the exterior of the domain. Speciﬁcally, we consider the analogue of the so-called ‘walk-on-spheres’ algorithm. In the diﬀusive setting, this entails sampling the path of Brownian motion as it uniformly exits a sequence of spheres maximally inscribed in the domain. As this algorithm would otherwise never end, it is truncated when the ‘walk-on-spheres’ comes within ε > 0 of the boundary. In the setting of the fractional Laplacian, the role of Brownian motion is replaced by an isotropic α-stable process with α ∈ (0, 2). A signiﬁcant diﬀerence to the Brownian setting is that the stable processes will exit spheres by a jump rather than hitting their boundary. This diﬀerence ensures that disconnected domains may be considered and that, unlike the diﬀusive setting, the algorithm ends after an almost surely ﬁnite number of steps.","2017-06-24","2024-07-18 13:46:33","2024-08-10 16:53:50","2024-07-18 13:46:33","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1609.03127 [math]","","C:\Users\isido\Zotero\storage\UTK4NNFC\Kyprianou e.a. - 2017 - Unbiased `walk-on-spheres' Monte Carlo methods for.pdf","","monte carlo; PDE; walk on spheres; ♥","","","","","","","","","","","","","","","","","","","","arXiv:1609.03127","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39YCZB7C","journalArticle","2022","Magalhães, Filipe; Monteiro, José; Acebrón, Juan A.; Herrero, José R.","A distributed Monte Carlo based linear algebra solver applied to the analysis of large complex networks","Future Generation Computer Systems","","0167739X","10.1016/j.future.2021.09.014","https://linkinghub.elsevier.com/retrieve/pii/S0167739X21003605","Methods based on Monte Carlo for solving linear systems have some interesting properties which make them, in many instances, preferable to classic methods. Namely, these statistical methods allow the computation of individual entries of the output, hence being able to handle problems where the size of the resulting matrix would be too large. In this paper, we propose a distributed linear algebra solver based on Monte Carlo. The proposed method is based on an algorithm that uses random walks over the system’s matrix to calculate powers of this matrix, which can then be used to compute a given matrix function. Distributing the matrix over several nodes enables the handling of even larger problem instances, however it entails a communication penalty as walks may need to jump between computational nodes. We have studied diﬀerent buﬀering strategies and provide a solution that minimizes this overhead and maximizes performance. We used our method to compute metrics of complex networks, such as node centrality and resolvent Estrada index. We present results that demonstrate the excellent scalability of our distributed implementation on very large networks, eﬀectively providing a solution to previously unreachable problem instances.","2022-02","2024-07-18 13:59:00","2024-08-10 16:48:45","2024-07-18 13:59:00","320-330","","","127","","Future Generation Computer Systems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\KS544CIU\Magalhães e.a. - 2022 - A distributed Monte Carlo based linear algebra sol.pdf","","monte carlo; random linear algebra; matrix exponential; ♥♥; matrix multiplication","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JB6U246","preprint","2018","Detommaso, Gianluca; Dodwell, Tim; Scheichl, Rob","Continuous Level Monte Carlo and Sample-Adaptive Model Hierarchies","","","","","http://arxiv.org/abs/1802.07539","In this paper, we present a generalisation of the Multilevel Monte Carlo (MLMC) method to a setting where the level parameter is a continuous variable. This Continuous Level Monte Carlo (CLMC) estimator provides a natural framework in PDE applications to adapt the model hierarchy to each sample. In addition, it can be made unbiased with respect to the expected value of the true quantity of interest provided the quantity of interest converges suﬃciently fast. The practical implementation of the CLMC estimator is based on interpolating actual evaluations of the quantity of interest at a ﬁnite number of resolutions. As our new level parameter, we use the logarithm of a goal-oriented ﬁnite element error estimator for the accuracy of the quantity of interest. We prove the unbiasedness, as well as a complexity theorem that shows the same rate of complexity for CLMC as for MLMC. Finally, we provide some numerical evidence to support our theoretical results, by successfully testing CLMC on a standard PDE test problem. The numerical experiments demonstrate clear gains for sample-wise adaptive reﬁnement strategies over uniform reﬁnements.","2018-02-21","2024-07-18 21:12:06","2024-08-10 16:46:04","2024-07-18 21:12:06","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1802.07539 [math]","Comment: 22 pages, 4 figures","C:\Users\isido\Zotero\storage\KVDKD5CQ\Detommaso e.a. - 2018 - Continuous Level Monte Carlo and Sample-Adaptive M.pdf","","monte carlo; ♥♥♥♥; debiasing; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1802.07539","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ER4JZRPX","videoRecording","2022","Erwin Schrödinger International Institute for Mathematics and Physics (ESI)","Juan P. Madrigal Cianci - Multi-level Markov Chain Monte Carlo Methods for Bayesian Inverse Problems","","","","","https://www.youtube.com/watch?v=cdlXlRMbEys","This talk was part of the Workshop on ""Computational Uncertainty Quantification: Mathematical Foundations, Methodology & Data"" held at the ESI May 2 to 6, 2022. We present a novel class of Multi-Level Markov Chain Monte Carlo (ML-MCMC) algorithms and apply them in the context of large-scale Bayesian inverse problems (BIPs), where the likelihood function involves a complex differential model, which is then approximated on a sequence of increasingly accurate discretizations. The key point of this algorithm is to construct highly coupled Markov chains together with the standard Multi-level Monte Carlo argument to obtain a better cost-tolerance complexity than a single-level MCMC algorithm. We present several approaches to generate these highly coupled chains by sampling, e.g., from a maximal coupling of the proposals for each marginal Markov chain. By doing this, we are allowed to create novel ML-MCMC methods for which, contrary to previously used models, the proposals at each iteration can depend on the current state of this chain, while at the same time, creating chains that are highly correlated. The presented methods are tested on an array of both academic and large-scale BIPs, where a clear computational advantage can be observed with respect to their single-level counterpart.","2022-05-04","2024-07-19 08:10:43","2024-08-10 16:45:28","2024-07-19 08:10:43","","","","","","","","","","","","","","","","","","","YouTube","","","","","","♥♥♥; MCMC; MLMC; uncertainty quantification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URJB26G6","journalArticle","2023","Bayer, Christian; Hammouda, Chiheb Ben; Tempone, Raúl","Numerical Smoothing with Hierarchical Adaptive Sparse Grids and Quasi-Monte Carlo Methods for Efficient Option Pricing","Quantitative Finance","","1469-7688, 1469-7696","10.1080/14697688.2022.2135455","http://arxiv.org/abs/2111.01874","When approximating the expectations of a functional of a solution to a stochastic diﬀerential equation, the numerical performance of deterministic quadrature methods, such as sparse grid quadrature and quasi-Monte Carlo (QMC) methods, may critically depend on the regularity of the integrand. To overcome this issue and improve the regularity structure of the problem, we consider cases in which analytic smoothing (bias-free molliﬁcation) cannot be performed and introduce a novel numerical smoothing approach by combining a root-ﬁnding method with a one-dimensional numerical integration with respect to a single well-chosen variable. We prove that, under appropriate conditions, the resulting function of the remaining variables is highly smooth, potentially aﬀording the improved eﬃciency of adaptive sparse grid quadrature (ASGQ) and QMC methods, particularly when combined with hierarchical transformations (i.e., the Brownian bridge and Richardson extrapolation on the weak error). This approach facilitates the eﬀective treatment of high dimensionality. Our study is motivated by option pricing problems, focusing on dynamics where the discretization of the asset price is necessary. Based on our analysis and numerical experiments, we demonstrate the advantages of combining numerical smoothing with the ASGQ and QMC methods over these methods without smoothing and the Monte Carlo approach. Finally, our approach is generic and can be applied to solve a broad class of problems, particularly approximating distribution functions, computing ﬁnancial Greeks, and estimating risk quantities.","2023-02-01","2024-07-19 13:20:01","2024-08-10 16:39:01","2024-07-19 13:20:01","209-227","","2","23","","Quantitative Finance","","","","","","","","en","","","","","arXiv.org","","arXiv:2111.01874 [cs, math, q-fin]","Comment: arXiv admin note: substantial text overlap with arXiv:2003.05708","C:\Users\isido\Zotero\storage\R847GD7K\Bayer e.a. - 2023 - Numerical Smoothing with Hierarchical Adaptive Spa.pdf","","monte carlo; SDE; ♥♥♥♥; option pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8CXU9MB","videoRecording","2022","SNSL Workshop","Chiheb Ben Hammouda, Numerical smoothing and hierarchical approximations for efficient option pricin","","","","","https://www.youtube.com/watch?v=Jk2OJ0mRbEA","Chiheb Ben Hammouda, Numerical smoothing and hierarchical approximations for efficient option pricing and density estimation.","2022-05-26","2024-07-19 18:46:01","2024-08-10 16:39:10","2024-07-19 18:46:00","","","","","","","","","","","","","","","","","","","YouTube","","","","","","monte carlo; SDE; ♥♥♥♥; option pricing","","","","","","","","","","","","","","","","","","","","","","57:10","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8J2ECF7","videoRecording","2024","SNSL Workshop","Mike Giles's talk at the SNSL24","","","","","https://www.youtube.com/watch?v=cwDC_i1hYNQ","Mike Giles - SDE analysis for multilevel function approximation ""Building on the original Multilevel Monte Carlo research of Stefan Heinrich, we consider the approximation of parametric functions defined by the expectation of a quantity of interest (QoI) arising from the approximation of an SDE.  In particular we analyse the behaviour of the variance of the multi-index estimator under the assumption that the coefficients of the SDE are smooth with respect to each of the parameters, and the QoI is either a) a smooth function of the terminal path value, or b) a Lipschitz or discontinuous function. This is joint work with Filippo De Angelis and Christoph Reisinger.""","2024-06-02","2024-07-19 20:22:54","2024-08-10 18:50:51","2024-07-19 20:22:54","","","","","","","","","","","","","","","","","","","YouTube","","","","","","monte carlo; SDE; parametric; ♥♥♥; MLMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8H7DKQQP","videoRecording","2024","Sheafification of G","One second to compute the largest Fibonacci number I can - YouTube","","","","","https://www.youtube.com/watch?v=KzT9I1d-LlQ","Most of us are familiar with the Fibonacci sequence. What's the largest Fibonacci number you can compute in 1 second? I'm not setting any world records, here; I don't own a supercomputer. You can criticise my code here: https://github.com/GSheaf/Fibsonicci Addenda: __________ At 7:59, the e_{01}s in the bottom row are incorrect... [in my defense, the Fibonacci transition matrix is symmetric]. Thanks ‪@andykhang404‬! At 14:44, the formula for the coefficient of the product polynomial is incorrect; every instance of (N-1) should be replaced with an n in the summation on the right hand side... This error repeats at 15:14. Embarrassing! Thanks ‪@henryroyer7691‬! Timestamps: __________ 00:00 - Introduction 01:06 - Recursion 02:35 - Memoisation 03:39 - ""Linear"" nonrecursive algorithm 04:31 - Matrix-based algorithm 05:57 - Things add up 08:20 - Fast exponentiation algorithm 10:07 - Grade-school multiplication 11:45 - Multiplication through division 14:38 - Discreet improvements 17:27 - Fast and Furious (wysi) 19:36 - Golden medallist 22:10 - Thx 4 watching 22:38 - The ugly truth","2024-07-20","2024-07-20 07:22:56","2024-08-12 14:36:56","2024-07-20 07:22:56","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\9JZ3ZRRU\watch.html","","♥♥♥♥; FFT","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54YQABPE","book","2023","Baranek, Marcin; Kałuża, Andrzej; Morkisz, Pawel; Przybyłowicz, Paweł; Sobieraj, Michał","On the randomized Euler algorithm under inexact information","","","","","","This paper focuses on analyzing the error of the randomized Euler algorithm when only noisy information about the coefficients of the underlying stochastic differential equation (SDE) and the driving Wiener process is available. Two classes of disturbed Wiener process are considered, and the dependence of the algorithm's error on the regularity of the disturbing functions is investigated. The paper also presents results from numerical experiments to support the theoretical findings.","2023-07-10","2024-07-20 07:55:00","2024-08-10 16:30:04","","","","","","","","","","","","","","","","","","","","ResearchGate","","","","","https://www.researchgate.net/publication/372248603_On_the_randomized_Euler_algorithm_under_inexact_information/fulltext/64acc8f095bbbe0c6e25a81d/On-the-randomized-Euler-algorithm-under-inexact-information.pdf?origin=publication_detail&_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InByb2ZpbGUiLCJwYWdlIjoicHVibGljYXRpb25Eb3dubG9hZCIsInByZXZpb3VzUGFnZSI6InB1YmxpY2F0aW9uIn19","randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WWBK8ZMK","preprint","2024","Bochacik, Tomasz; Przybyłowicz, Paweł","Convergence and stability of randomized implicit two-stage Runge-Kutta schemes","","","","","http://arxiv.org/abs/2404.19059","We randomize the implicit two-stage Runge-Kutta scheme in order to improve the rate of convergence (with respect to a deterministic scheme) and stability of the approximate solution (with respect to the solution generated by the explicit scheme). For stability analysis, we use Dahlquist’s concept of A-stability, adopted to randomized schemes by considering three notions of stability: asymptotic, mean-square, and in probability. The randomized implicit RK2 scheme proves to be A-stable asymptotically and in probability but not in the mean-square sense.","2024-04-29","2024-07-20 07:59:31","2024-08-10 16:29:51","2024-07-20 07:59:31","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.19059 [cs, math]","","C:\Users\isido\Zotero\storage\BWGT83H4\Bochacik en Przybyłowicz - 2024 - Convergence and stability of randomized implicit t.pdf","","randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2404.19059","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKVR7PGQ","preprint","2024","Difonzo, Fabio V.; Przybyłowicz, Paweł; Wu, Yue; Xie, Xinheng","A Randomized Runge-Kutta Method for time-irregular delay differential equations","","","","","http://arxiv.org/abs/2401.11658","In this paper we investigate the existence, uniqueness and approximation of solutions of delay differential equations (DDEs) with the right-hand side functions f = f (t, x, z) that are Lipschitz continuous with respect to x but only Ho¨lder continuous with respect to (t, z). We give a construction of the randomized two-stage Runge-Kutta scheme for DDEs and investigate its upper error bound in the Lp(Ω)norm for p ∈ [2, +∞). Finally, we report on results of numerical experiments.","2024-01-21","2024-07-20 08:02:35","2024-08-10 16:29:38","2024-07-20 08:02:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2401.11658 [cs, math]","Comment: arXiv admin note: text overlap with arXiv:2204.02016","C:\Users\isido\Zotero\storage\L3ZH7QCE\Difonzo e.a. - 2024 - A Randomized Runge-Kutta Method for time-irregular.pdf","","randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2401.11658","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLLJRQJP","preprint","2023","Przybyłowicz, Paweł; Wu, Yue; Xie, Xinheng","On approximation of solutions of stochastic delay differential equations via randomized Euler scheme","","","","","http://arxiv.org/abs/2306.08926","We investigate existence, uniqueness and approximation of solutions to stochastic delay differential equations (SDDEs) under Carath´eodory-type drift coefficients. Moreover, we also assume that both drift f = f (t, x, z) and diffusion g = g(t, x, z) coefficient are Lipschitz continuous with respect to the space variable x, but only Ho¨lder continuous with respect to the delay variable z. We provide a construction of randomized Euler scheme for approximation of solutions of Carath´eodory SDDEs, and investigate its upper error bound. Finally, we report results of numerical experiments that confirm our theoretical findings.","2023-06-15","2024-07-20 08:03:34","2024-08-10 16:29:34","2024-07-20 08:03:34","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2306.08926 [cs, math]","","C:\Users\isido\Zotero\storage\P98QNSNK\Przybyłowicz e.a. - 2023 - On approximation of solutions of stochastic delay .pdf","","SDE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2306.08926","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSNZKVEI","preprint","2023","Przybyłowicz, Paweł; Schwarz, Verena; Szölgyenyi, Michaela","Randomized Milstein algorithm for approximation of solutions of jump-diffusion SDEs","","","","","http://arxiv.org/abs/2212.00411","We investigate the error of the randomized Milstein algorithm for solving scalar jumpdiffusion stochastic differential equations. We provide a complete error analysis under substantially weaker assumptions than those known in the literature. In case the jumpcommutativity condition is satisfied, we prove optimality of the randomized Milstein algorithm by establishing matching lower bounds. Moreover, we give some insight into the multidimensional case by investigating the optimal convergence rate for the approximation of jump-diffusion type Lévys’ areas. Finally, we report numerical experiments that support our theoretical findings.","2023-12-05","2024-07-20 08:04:43","2024-08-10 16:28:48","2024-07-20 08:04:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2212.00411 [cs, math]","","C:\Users\isido\Zotero\storage\3M2I7K9Q\Przybyłowicz e.a. - 2023 - Randomized Milstein algorithm for approximation of.pdf","","SDE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2212.00411","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKM3EBE6","preprint","2023","Przybyłowicz, Paweł; Schwarz, Verena; Szölgyenyi, Michaela","A higher order approximation method for jump-diffusion SDEs with discontinuous drift coefficient","","","","","http://arxiv.org/abs/2211.08739","We present the ﬁrst higher-order approximation scheme for solutions of jump-diﬀusion stochastic diﬀerential equations with discontinuous drift. For this transformation-based jump-adapted quasi-Milstein scheme we prove Lp-convergence order 3/4. To obtain this result, we prove that under slightly stronger assumptions (but still weaker than anything known before) a related jump-adapted quasi-Milstein scheme has convergence order 3/4 – in a special case even order 1. Order 3/4 is conjectured to be optimal.","2023-12-05","2024-07-20 08:27:35","2024-08-10 16:28:33","2024-07-20 08:27:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.08739 [cs, math]","","C:\Users\isido\Zotero\storage\B3342BZI\Przybyłowicz e.a. - 2023 - A higher order approximation method for jump-diffu.pdf","","SDE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2211.08739","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A37AYKF6","journalArticle","2023","Czyżewska, Natalia; Morkisz, Paweł M.; Przybyłowicz, Paweł","Euler scheme for approximation of solution of nonlinear ODEs under inexact information","Applied Numerical Mathematics","","01689274","10.1016/j.apnum.2023.08.002","http://arxiv.org/abs/2209.07482","We investigate error of the Euler scheme in the case when the right-hand side function of the underlying ODE satisfies nonstandard assumptions such as local one-sided Lipschitz condition and local Hölder continuity. Moreover, we assume two cases in regards to information availability: exact and noisy with respect to the right-hand side function. Optimality analysis of the Euler scheme is also provided. Finally, we present the results of some numerical experiments.","2023-11","2024-07-20 08:29:47","2024-08-10 16:28:21","2024-07-20 08:29:47","226-241","","","193","","Applied Numerical Mathematics","","","","","","","","en","","","","","arXiv.org","","arXiv:2209.07482 [cs, math]","Comment: 18 pages, 9 figures","C:\Users\isido\Zotero\storage\HG49W7YE\Czyżewska e.a. - 2023 - Euler scheme for approximation of solution of nonl.pdf","","monte carlo; ODE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5UJQCAU7","journalArticle","2022","Przybyłowicz, Paweł; Sobieraj, Michał; Stȩpień, Łukasz","Efficient approximation of SDEs driven by countably dimensional Wiener process and Poisson random measure","SIAM Journal on Numerical Analysis","","0036-1429, 1095-7170","10.1137/21M1442747","http://arxiv.org/abs/2108.02394","In this paper we deal with pointwise approximation of solutions of stochastic diﬀerential equations (SDEs) driven by inﬁnite dimensional Wiener process with additional jumps generated by Poisson random measure. The further investigations contain upper error bounds for the proposed truncated dimension randomized Euler scheme. We also establish matching (up to constants) upper and lower bounds for ε-complexity and show that the deﬁned algorithm is optimal in the Information-Based Complexity (IBC) sense. Finally, results of numerical experiments performed by using GPU architecture are also reported.","2022-04","2024-07-20 08:32:38","2024-08-10 16:28:18","2024-07-20 08:32:38","824-855","","2","60","","SIAM J. Numer. Anal.","","","","","","","","en","","","","","arXiv.org","","arXiv:2108.02394 [cs, math]","","C:\Users\isido\Zotero\storage\KB4WUZZJ\Przybyłowicz e.a. - 2022 - Efficient approximation of SDEs driven by countabl.pdf","","SDE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TRDQ8CL9","journalArticle","2021","Bochacik, Tomasz; Goćwin, Maciej; Morkisz, Paweł M.; Przybyłowicz, Paweł","Randomized Runge-Kutta method -- stability and convergence under inexact information","Journal of Complexity","","0885064X","10.1016/j.jco.2021.101554","http://arxiv.org/abs/2006.12131","We deal with optimal approximation of solutions of ODEs under local Lipschitz condition and inexact discrete information about the right-hand side functions. We show that the randomized two-stage Runge-Kutta scheme is the optimal method among all randomized algorithms based on standard noisy information. We perform numerical experiments that conﬁrm our theoretical ﬁndings. Moreover, for the optimal algorithm we rigorously investigate properties of regions of absolute stability.","2021-08","2024-07-20 08:37:17","2024-08-10 16:28:05","2024-07-20 08:37:17","101554","","","65","","Journal of Complexity","","","","","","","","en","","","","","arXiv.org","","arXiv:2006.12131 [cs, math]","","C:\Users\isido\Zotero\storage\S3PFSY3L\Bochacik e.a. - 2021 - Randomized Runge-Kutta method -- stability and con.pdf","","monte carlo; ODE; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NG6HSCS","preprint","2024","Przybyłowicz, Paweł","Foundations of Monte Carlo methods and stochastic simulations -- From Monte Carlo Lebesgue integration to weak approximation of SDEs","","","","","http://arxiv.org/abs/2208.05531","In recent years dynamical systems (of deterministic and stochastic nature), describing many models in mathematics, physics, engineering and finances, become more and more complex. Numerical analysis narrowed only to deterministic algorithms seems to be insufficient for such systems, since, for example, curse of dimensionality affects deterministic methods. Therefore, we can observe increasing popularity of Monte Carlo algorithms and, closely related with them, stochastic simulations based on stochastic differential equations. In these lecture notes we present main ideas concerned with Monte Carlo methods and their theoretical properties. We apply them to such problems as integration and approximation of solutions of deterministic/stochastic differential equations. We also discuss implementation of exemplary algorithms in Python programming language and their application to option pricing. Part of these notes has been used during lectures for PhD students at AGH University of Science and Technology, Krakow, Poland, at summer semesters in the years 2020, 2021, 2023.","2024-02-10","2024-07-20 09:02:37","2024-08-10 16:28:13","2024-07-20 09:02:37","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2208.05531 [cs, math]","","C:\Users\isido\Zotero\storage\DKQHYPML\Przybyłowicz - 2024 - Foundations of Monte Carlo methods and stochastic .pdf","","monte carlo; ODE; SDE; ♥♥♥♥; variance reduction; quadrature; randomized ODE & SDE","","","","","","","","","","","","","","","","","","","","arXiv:2208.05531","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADPR23F7","journalArticle","2017","Kruse, Raphael; Wu, Yue","Error analysis of randomized Runge-Kutta methods for differential equations with time-irregular coefficients","Computational Methods in Applied Mathematics","","1609-9389, 1609-4840","10.1515/cmam-2016-0048","http://arxiv.org/abs/1701.03444","This paper contains an error analysis of two randomized explicit Runge-Kutta schemes for ordinary diﬀerential equations (ODEs) with timeirregular coeﬃcient functions. In particular, the methods are applicable to ODEs of Carath´eodory type, whose coeﬃcient functions are only integrable with respect to the time variable but are not assumed to be continuous. A further ﬁeld of application are ODEs with coeﬃcient functions that contain weak singularities with respect to the time variable.","2017-07-01","2024-07-21 11:12:17","2024-08-07 10:03:00","2024-07-21 11:12:17","479-498","","3","17","","","","","","","","","","en","","","","","arXiv.org","","arXiv:1701.03444 [math]","Comment: 24 pages, 3 figures","C:\Users\isido\Zotero\storage\AKENK2FH\Kruse en Wu - 2017 - Error analysis of randomized Runge-Kutta methods f.pdf","","monte carlo; ODE; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TW6GL26V","webpage","2024","Wu, Yue","About me Yue Wu","Yue Wu","","","","https://yuewu57.github.io/aboutme/","My name is Yue Wu, a lecturer in the Department of Mathematics and Statistics, University of Strathclyde, Glasgow; LMS and EMS member. I am accepting PhD student(s) all year round. My research interest includes: Numerical analysis for stochastic (partial) differential equations (see talk slides given at MCQMC 2018) Random periodic solutions of stochastic (partial) differential equations and its numercs (watch my talk given at GPSD 2021 mannheim) Rough path theory and its applications in machine learning/data science","2024-07-21","2024-07-21 12:01:20","2024-08-12 14:41:16","2024-07-21 12:01:20","","","","","","","","","","","","","","en","","","","","","","","<div data-schema-version=""8""><p><a href=""https://pureportal.strath.ac.uk/en/persons/yue-wu"" rel=""noopener noreferrer nofollow"">https://pureportal.strath.ac.uk/en/persons/yue-wu</a></p> </div>","C:\Users\isido\Zotero\storage\R4IPDWXK\aboutme.html","","MLMC; monte carlo; quadrature; random linear algebra; SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EF9KLQMG","preprint","2020","Wu, Yue; Polydorides, Nick","A Multilevel Monte Carlo Estimator for Matrix Multiplication","","","","","http://arxiv.org/abs/1904.00429","Inspired by recent developments in multilevel Monte Carlo (MLMC) methods and randomised sketching for linear algebra problems we propose a MLMC estimator for real-time processing of matrix structured random data. Our algorithm is particularly eﬀective in handling high-dimensional inner products and matrix multiplication, and ﬁnds applications in computer vision and large-scale supervised learning.","2020-04-29","2024-07-21 17:51:46","2024-08-07 10:00:04","2024-07-21 17:51:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1904.00429 [cs, math]","Comment: 23 pages, 3 figures","C:\Users\isido\Zotero\storage\VNSAJ3XM\Wu en Polydorides - 2020 - A Multilevel Monte Carlo Estimator for Matrix Mult.pdf","","monte carlo; random linear algebra; ♥♥♥; matrix multiplication; MLMC","","","","","","","","","","","","","","","","","","","","arXiv:1904.00429","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3K3YFKGE","preprint","2019","Wu, Yue","A Note on Random Sampling for Matrix Multiplication","","","","","http://arxiv.org/abs/1811.11237","This paper extends the framework of randomised matrix multiplication in [5] to a coarser partition and proposes an algorithm as a complement to BASICMATRIXMULTIPLICATION in [5], especially when the optimal probability distribution of the latter algorithm is closed to uniform. The new algorithm increases the likelihood of getting a small approximation error in 2-norm and has the squared approximation error in Frobenious norm bounded by that from algorithm BASICMATRIXMULTIPLICATION.","2019-05-16","2024-07-21 19:40:51","2024-08-07 09:57:10","2024-07-21 19:40:51","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1811.11237 [cs, math]","","C:\Users\isido\Zotero\storage\Z8CYMJST\Wu - 2019 - A Note on Random Sampling for Matrix Multiplicatio.pdf","","monte carlo; random linear algebra; ♥♥; matrix multiplication","","","","","","","","","","","","","","","","","","","","arXiv:1811.11237","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2WM4BS7W","videoRecording","2020","RoyalStatSoc","RSS Discussion Meeting: Unbiased Markov chain Monte Carlo methods with couplings","","","","","https://www.youtube.com/watch?v=ohu8DJ1qDb0","Pierre E. Jacob and John O’Leary (Harvard University, Cambridge) and Yves F. Atchadé (Boston University) Markov chain Monte Carlo (MCMC) methods provide consistent approximations of integrals as the number of iterations goes to 1. MCMC estimators are generally biased after any fixed number of iterations. We propose to remove this bias by using couplings of Markov chains together with a telescopic sum argument of Glynn and Rhee. The resulting unbiased estimators can be computed independently in parallel. We discuss practical couplings for popular MCMC algorithms. We establish the theoretical validity of the estimators proposed and study their efficiency relative to the underlying MCMC algorithms. Finally, we illustrate the performance and limitations of the method on toy examples, on an Ising model around its critical temperature, on a high dimensional variable-selection problem, and on an approximation of the cut distribution arising in Bayesian inference for models made of multiple modules.","2020-01-03","2024-07-22 22:21:12","2024-08-07 09:48:37","2024-07-22 22:21:12","","","","","","","RSS Discussion Meeting","","","","","","","","","","","","YouTube","","","","","","monte carlo; ♥♥♥♥; debiasing; MCMC","","","","","","","","","","","","","","","","","","","","","","01:30:13","","","","","","","","","","","","","","","","","","","","","","","","",""
"KDZ5BKZC","videoRecording","2024","International Centre for Mathematical Sciences","Neil Chada, Unbiased Kinetic Langevin Monte Carlo","","","","","https://www.youtube.com/watch?v=B-8QMe7p8xE","This talk was recorded as part of a workshop hosted by ICMS. For more of our talk recordings have a look at the other event playlists on our YouTube channel or take a look on our Media Hopper channel for lots of other workshops   https://media.ed.ac.uk/channel/Intern...   Head to the ICMS website for news and information about upcoming workshops and events  https://www.icms.org.uk/  Subtitles have been generated automatically and the following disclaimer should be added to all recordings Please note that the subtitles that accompany this recording are auto-generated by YouTube. ICMS is happy to correct any errors, please email info@icms.org.uk with the details.","2024-06-25","2024-07-23 09:11:14","2024-08-07 09:48:09","2024-07-23 09:11:14","","","","","","","","","","","","","","","","","","","YouTube","","","","","","monte carlo; ♥♥; debiasing; MCMC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUG3VYGJ","preprint","2024","Chada, Neil K.; Leimkuhler, Benedict; Paulin, Daniel; Whalley, Peter A.","Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients","","","","","http://arxiv.org/abs/2311.05025","We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon>0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method's computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that the unbiased algorithm we present can be much more efficient than the ``gold-standard"" randomized Hamiltonian Monte Carlo.","2024-05-23","2024-07-23 09:16:16","2024-08-07 09:47:56","2024-07-23 09:16:16","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.05025 [cs, math, stat]","Comment: 99 Pages, 13 Figures","C:\Users\isido\Zotero\storage\R8JVYENF\2311.pdf","","monte carlo; ♥♥; debiasing; MCMC","","","","","","","","","","","","","","","","","","","","arXiv:2311.05025","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4SWLZXI","preprint","2024","Atchadé, Yves F.; Jacob, Pierre E.","Unbiased Markov Chain Monte Carlo: what, why, and how","","","","","http://arxiv.org/abs/2406.06851","This document presents methods to remove the initialization or burn-in bias from Markov chain Monte Carlo (MCMC) estimates, with consequences on parallel computing, convergence diagnostics and performance assessment. The document is written as an introduction to these methods for MCMC users. Some theoretical results are mentioned, but the focus is on the methodology.","2024-06-10","2024-07-23 09:46:04","2024-08-07 09:47:29","2024-07-23 09:46:04","","","","","","","Unbiased Markov Chain Monte Carlo","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2406.06851 [stat]","Comment: To appear in the second edition of the handbook of MCMC","C:\Users\isido\Zotero\storage\8PRIMMQE\Atchadé en Jacob - 2024 - Unbiased Markov Chain Monte Carlo what, why, and .pdf","","monte carlo; ♥♥♥♥; debiasing; MCMC","","","","","","","","","","","","","","","","","","","","arXiv:2406.06851","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4L9PFRWB","preprint","2020","Heng, Jeremy; Jacob, Pierre E.; Ju, Nianqiao","A simple Markov chain for independent Bernoulli variables conditioned on their sum","","","","","http://arxiv.org/abs/2012.03103","We consider a vector of N independent binary variables, each with a diﬀerent probability of success. The distribution of the vector conditional on its sum is known as the conditional Bernoulli distribution. Assuming that N goes to inﬁnity and that the sum is proportional to N , exact sampling costs order N 2, while a simple Markov chain Monte Carlo algorithm using “swaps” has constant cost per iteration. We provide conditions under which this Markov chain converges in order N log N iterations. Our proof relies on couplings and an auxiliary Markov chain deﬁned on a partition of the space into favorable and unfavorable pairs.","2020-12-05","2024-07-23 09:49:27","2024-08-07 09:45:18","2024-07-23 09:49:27","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2012.03103 [math, stat]","Comment: 16 pages, 3 figures, 1 table","C:\Users\isido\Zotero\storage\ZUF4AX7H\Heng e.a. - 2020 - A simple Markov chain for independent Bernoulli va.pdf","","monte carlo; ♥♥♥; MCMC","","","","","","","","","","","","","","","","","","","","arXiv:2012.03103","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFUR86AG","preprint","2022","Dai, Chenguang; Heng, Jeremy; Jacob, Pierre E.; Whiteley, Nick","An invitation to sequential Monte Carlo samplers","","","","","http://arxiv.org/abs/2007.11936","Statisticians often use Monte Carlo methods to approximate probability distributions, primarily with Markov chain Monte Carlo and importance sampling. Sequential Monte Carlo samplers are a class of algorithms that combine both techniques to approximate distributions of interest and their normalizing constants. These samplers originate from particle ﬁltering for state space models and have become general and scalable sampling techniques. This article describes sequential Monte Carlo samplers and their possible implementations, arguing that they remain under-used in statistics, despite their ability to perform sequential inference and to leverage parallel processing resources among other potential beneﬁts.","2022-06-17","2024-07-23 09:56:56","2024-08-07 09:44:39","2024-07-23 09:56:56","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2007.11936 [stat]","Comment: 37 pages, 8 figures; small typos corrected","C:\Users\isido\Zotero\storage\9Q49CKN2\Dai e.a. - 2022 - An invitation to sequential Monte Carlo samplers.pdf","","monte carlo; ♥","","","","","","","","","","","","","","","","","","","","arXiv:2007.11936","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6FZFNIE","preprint","2010","McLeish, Don","A general method for debiasing a Monte Carlo estimator","","","","","http://arxiv.org/abs/1005.2228","Consider a stochastic process Xn, n = 0, 1, 2, ...such that EXn → x∞ as n → ∞. The sequence {Xn} may be a deterministic one, obtained by using a numerical integration scheme, or obtained from Monte-Carlo methods involving an approximation to an integral, or a Newton-Raphson iteration to approximate the root of an equation but we will assume that we can sample from the distribution of X1, X2, ...Xm for ﬁnite m. We propose a scheme for unbiased estimation of the limiting value x∞, together with estimates of standard error and apply this to examples including numerical integrals, root-ﬁnding and option pricing in a Heston Stochastic Volatility model.","2010-06-16","2024-07-23 10:10:46","2024-08-07 09:47:22","2024-07-23 10:10:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1005.2228 [cs, q-fin, stat]","Comment: 11 pages, 1 figure","C:\Users\isido\Zotero\storage\JYBSZ9AD\McLeish - 2010 - A general method for debiasing a Monte Carlo estim.pdf","","monte carlo; ♥♥♥♥♥; debiasing","","","","","","","","","","","","","","","","","","","","arXiv:1005.2228","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KLRDW5JM","preprint","2012","Rhee, Chang-han; Glynn, Peter W.","A new approach to unbiased estimation for SDE's","","","","","http://arxiv.org/abs/1207.2452","In this paper, we introduce a new approach to constructing unbiased estimators when computing expectations of path functionals associated with stochastic differential equations (SDEs). Our randomization idea is closely related to multi-level Monte Carlo and provides a simple mechanism for constructing a ﬁnite variance unbiased estimator with “square root convergence rate” whenever one has available a scheme that produces strong error of order greater than 1/2 for the path functional under consideration.","2012-07-10","2024-07-23 15:05:00","2024-08-07 09:47:19","2024-07-23 15:05:00","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1207.2452 [math, q-fin]","","C:\Users\isido\Zotero\storage\MBNGSGN9\Rhee en Glynn - 2012 - A new approach to unbiased estimation for SDE's.pdf","","monte carlo; SDE; ♥♥♥; debiasing","","","","","","","","","","","","","","","","","","","","arXiv:1207.2452","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"33IQTVWJ","webpage","2024","","PhD in Efficient Computational Methods for xVA and Financial Risk Management | Utrecht University | LinkedIn","","","","","https://www.linkedin.com/jobs/view/3980335235/?trackingId=I2gpdVE7udhGqwW8DeU3XQ%3D%3D&refId=ByteString%28length%3D16%2Cbytes%3D9ff086e9...cbbecf3d%29&midToken=AQGnVUBeuZxFyw&midSig=0lyfB7w45yTbk1&trk=eml-email_job_alert_digest_01-job_card-0-jobcard_body&trkEmail=eml-email_job_alert_digest_01-job_card-0-jobcard_body-null-e1p8n5~lyyeyxum~5m-null-null&eid=e1p8n5-lyyeyxum-5m&otpToken=MWEwMTE3ZTMxYTJkY2ZjY2I1MjQwNGVkNDQxZmUwYjc4NmNlZDQ0NDkwYWU4YjYxNzljNjAwNjg0YjUyNTlmMGZmZGRkZjk4NzNjNmQ5ZmUwZWFiZWExMDYzYjdlNTBlYTRlYWFhNDJkMjU2MTYyMzFlYTdmMywxLDE%3D","","2024-07-23","2024-07-23 15:29:04","2024-08-12 14:55:10","2024-07-23 15:29:04","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\TMMQB957\3980335235.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3PQRE9M","webpage","2024","","Teaching - Dr. C. (Chiheb) Ben Hammouda - Utrecht University","","","","","https://www.uu.nl/staff/CBenHammouda/Teaching","My research integrates mathematical (stochastic) modeling, numerical analysis, and the development of advanced computational methods to address complex problems in engineering and science. The studied problems often exhibit challenging features such as high dimensionality, complex dynamics, low regularity, and rare events, which can adversely affect the performance of numerical methods in terms of computational cost, accuracy, robustness, and applicability. A central question in my research is how to enhance these numerical methods to achieve optimal performance (i.e., balancing efficiency and interpretability). In this respect, I focus on designing novel strategies based on smoothing techniques, dimensionality and variance reduction, improved sampling (hierarchical/adaptive/importance sampling), and machine learning. The conducted research spans theory, algorithm design, and numerical analysis. My work is application-driven, and my research focuses include: Numerical and machine learning methods in quantitative finance: pricing financial derivatives and risk management. Optimal control and reinforcement learning for (hybrid) power systems management and trading in (renewable) energy markets. Forward and inverse problems in stochastic reaction networks, with applications in biochemical systems and epidemiology. Machine learning and data-driven methods for forecasting rare and extreme events. Hierarchical approximation methods and machine learning for scientific computing. The methodologies employed in my research encompass a range of techniques, including Monte Carlo (MC), multilevel (hierarchical) MC, Quasi-MC, (adaptive) sparse grids' quadrature, Fourier methods, stochastic optimal control, importance sampling, and machine learning.","2024-07-23","2024-07-23 15:29:15","2024-08-12 14:58:30","2024-07-23 15:29:15","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\MY7FSLM7\Teaching.html","","inverse problem; MLMC; monte carlo; option pricing; persons; reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J9VWW2P8","preprint","2021","Bochacik, Tomasz; Przybyłowicz, Paweł","On the randomized Euler schemes for ODEs under inexact information","","","","","http://arxiv.org/abs/2104.15071","We analyse errors of randomized explicit and implicit Euler schemes for approximate solving of ordinary diﬀerential equations (ODEs). We consider classes of ODEs for which the right-hand side functions satisfy Lipschitz condition globally or only locally. Moreover, we assume that only inexact discrete information, corrupted by some noise, about the right-hand side function is available. Optimality and stability of explicit and implicit randomized Euler algorithms are also investigated.","2021-04-30","2024-07-24 13:28:50","2024-08-07 09:32:16","2024-07-24 13:28:50","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2104.15071 [cs, math]","","C:\Users\isido\Zotero\storage\K4QVZKJH\2104.pdf","","monte carlo; ODE; ♥♥♥","","","","","","","","","","","","","","","","","","","","arXiv:2104.15071","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKATV8TJ","journalArticle","2018","Eisenmann, Monika; Kruse, Raphael","Two quadrature rules for stochastic It\^o-integrals with fractional Sobolev regularity","Communications in Mathematical Sciences","","15396746, 19450796","10.4310/CMS.2018.v16.n8.a4","http://arxiv.org/abs/1712.08152","In this paper we study the numerical quadrature of a stochastic integral, where the temporal regularity of the integrand is measured in the fractional Sobolev–Slobodeckij norm in W σ,p(0, T ), σ ∈ (0, 2), p ∈ [2, ∞). We introduce two quadrature rules: The ﬁrst is best suited for the parameter range σ ∈ (0, 1) and consists of a Riemann–Maruyama approximation on a randomly shifted grid. The second quadrature rule considered in this paper applies to the case of a deterministic integrand of fractional Sobolev regularity with σ ∈ (1, 2). In both cases the order of convergence is equal to σ with respect to the Lp-norm. As an application, we consider the stochastic integration of a Poisson process, which has discontinuous sample paths. The theoretical results are accompanied by numerical experiments.","2018","2024-07-24 15:27:47","2024-08-07 09:31:42","2024-07-24 15:27:47","2125-2146","","8","16","","","","","","","","","","en","","","","","arXiv.org","","arXiv:1712.08152 [math]","Comment: 20 pages, 2 figures","C:\Users\isido\Zotero\storage\K7GPAZCS\1712.pdf","","SDE; ♥; quadrature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIP6WAA9","journalArticle","2024","Eisenmann, Monika; Stillfjord, Tony","A randomized operator splitting scheme inspired by stochastic optimization methods","Numerische Mathematik","","0029-599X, 0945-3245","10.1007/s00211-024-01396-w","https://link.springer.com/10.1007/s00211-024-01396-w","In this paper, we combine the operator splitting methodology for abstract evolution equations with that of stochastic methods for large-scale optimization problems. The combination results in a randomized splitting scheme, which in a given time step does not necessarily use all the parts of the split operator. This is in contrast to deterministic splitting schemes which always use every part at least once, and often several times. As a result, the computational cost can be signiﬁcantly decreased in comparison to such methods. We rigorously deﬁne a randomized operator splitting scheme in an abstract setting and provide an error analysis where we prove that the temporal convergence order of the scheme is at least 1/2. We illustrate the theory by numerical experiments on both linear and quasilinear diffusion problems, using a randomized domain decomposition approach. We conclude that choosing the randomization in certain ways may improve the order to 1. This is as accurate as applying e.g. backward (implicit) Euler to the full problem, without splitting.","2024-04","2024-07-24 15:47:07","2024-08-07 09:28:20","2024-07-24 15:47:07","435-461","","2","156","","Numer. Math.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\WTQZ2ML3\Eisenmann en Stillfjord - 2024 - A randomized operator splitting scheme inspired by.pdf","","monte carlo; ODE; ♥♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"74GYRJ95","journalArticle","2003","Moler, Cleve; Van Loan, Charles","Nineteen Dubious Ways to Compute the Exponential of a Matrix, Twenty-Five Years Later","SIAM Review","","0036-1445, 1095-7200","10.1137/S00361445024180","http://epubs.siam.org/doi/10.1137/S00361445024180","In principle, the exponential of a matrix could be computed in many ways. Methods involving approximation theory, diﬀerential equations, the matrix eigenvalues, and the matrix characteristic polynomial have been proposed. In practice, consideration of computational stability and eﬃciency indicates that some of the methods are preferable to others, but that none are completely satisfactory.","2003-01","2024-07-24 16:35:13","2024-08-10 18:38:29","2024-07-24 16:35:13","3-49","","1","45","","SIAM Rev.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\5HL7H7CN\Moler en Van Loan - 2003 - Nineteen Dubious Ways to Compute the Exponential o.pdf","","♥; matrix exponential","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JV8SAIG","journalArticle","2005","Jimenez, J.C.; Carbonell, F.","Rate of convergence of local linearization schemes for initial-value problems","Applied Mathematics and Computation","","00963003","10.1016/j.amc.2005.01.118","https://linkinghub.elsevier.com/retrieve/pii/S0096300305001773","There is a large variety of Local Linearization (LL) schemes for the numerical integration of initial-value problems, which diﬀer with respect to the algorithm that is used in the numerical implementation of the Local Linear Discretization. However, in contrast with the LL Discretization, the order of convergence of the LL schemes have not been studied so far. In this paper, a general result about that matter is given. In addition, a brief survey of the main implementations of the LL method is also presented. Ó 2005 Elsevier Inc. All rights reserved.","2005-12","2024-07-24 16:40:55","2024-08-06 19:25:04","2024-07-24 16:40:55","1282-1295","","2","171","","Applied Mathematics and Computation","","","","","","","","en","https://www.elsevier.com/tdm/userlicense/1.0/","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\7RW94QV4\Jimenez en Carbonell - 2005 - Rate of convergence of local linearization schemes.pdf","","ODE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62PRGEDW","journalArticle","2017","Jimenez, J.C.; Mora, C.; Selva, M.","A weak Local Linearization scheme for stochastic differential equations with multiplicative noise","Journal of Computational and Applied Mathematics","","03770427","10.1016/j.cam.2016.09.013","https://linkinghub.elsevier.com/retrieve/pii/S0377042716304332","In this paper, a weak Local Linearization scheme for Stochastic Differential Equations (SDEs) with multiplicative noise is introduced. First, for a time discretization, the solution of the SDE is locally approximated by the solution of the piecewise linear SDE that results from the Local Linearization strategy. The weak numerical scheme is then defined as a sequence of random vectors whose first moments coincide with those of the piecewise linear SDE on the time discretization. The scheme is explicit, preserves the first two moments of the solution of SDEs with linear drift and diffusion coefficients in state and time, and inherits the mean-square stability or instability that such solution may have. The rate of convergence is derived and numerical simulations are presented for illustrating the performance of the scheme.","2017-03","2024-07-24 17:06:14","2024-08-06 19:24:13","2024-07-24 17:06:14","202-217","","","313","","Journal of Computational and Applied Mathematics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\GGTCWWZ9\Jimenez e.a. - 2017 - A weak Local Linearization scheme for stochastic d.pdf","","SDE; ♥","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3DP3YBX","videoRecording","2024","CaptainLuma","Ever Changing Redstone Maze in Minecraft","","","","","https://www.youtube.com/watch?v=1dZmw1a8HMY","In this video I go over my minecraft redstone ever-changing maze. This maze uses a custom algorithm I created specifically for this purpose. The algorithm can be used to create constantly changing mazes, and it is free for anyone to use. Using this algorithm I was able to make this generator over 3 times smaller than my previous generator. This video took me a long time to make, so please consider leaving a like to show your support, and subscribe so you don't miss future uploads. IMPORTANT: I didn't realize this until after recording this video. The maze will break if you leave the game while it's is running, or if the maze leaves render distance while it's running. If this happens, the origin cell will deactivate and must be reactivated manually. world download (made in 1.20.2): https://drive.google.com/file/d/1R_vG... tutorial video:    • Ever Changing Maze Tutorial   my previous video going over the algorithm:    • New Maze Generating Algorithm (Origin...   Check out some more redstone maze generators: RaPsCaLLioN1138's survival friendly maze:    • Redstone Maze Generator   DqwertyC's maze using recursive division:    • Recursive Division Redstone Maze Gene...   SleeperPin's maze using the hunt and kill algorithm:    • Randomized Spanning Tree Maze Creator...   D_00's maze using recursive backtracking:    • I made a Random Maze Generator with O...   Chapters: 0:00 Intro 0:49 Algorithm 3:19 Modes 4:10 Runtime 5:00 Mechanism 8:39 Closing thoughts Music: Tokyo Music Walker - When the Rain Stops    • Tokyo Music Walker - When the Rain Stops   StreamBeats by Harris Heller - Moving Lights    • Moving Lights   Tokyo Music Walker - Slowly    • Tokyo Music Walker - Slowly   Artificial Music - Faithful Mission    • Faithful Mission","2024-05-09","2024-07-27 08:23:29","2024-08-06 19:23:21","2024-07-27 08:23:29","","","","","","","","","","","","","","","","","","","YouTube","","","","","","♥♥♥♥; MCMC","","","","","","","","","","","","","","","","","","","","","","10:03","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFNHJB63","journalArticle","2017","Brumm, Johannes; Scheidegger, Simon","Using Adaptive Sparse Grids to Solve High-Dimensional Dynamic Models","","","","","","We present a ﬂexible and scalable method for computing global solutions of highdimensional stochastic dynamic models. Within a time iteration or value function iteration setup, we interpolate functions using an adaptive sparse grid algorithm. With increasing dimensions, sparse grids grow much more slowly than standard tensor product grids. Moreover, adaptivity adds a second layer of sparsity, as grid points are added only where they are most needed, for instance in regions with steep gradients or at nondiﬀerentiabilities. To further speed up the solution process, our implementation is fully hybrid parallel, combining distributed and shared memory parallelization paradigms, and thus permits an eﬃcient use of high-performance computing architectures. To demonstrate the broad applicability of our method, we solve two very diﬀerent types of dynamic models: ﬁrst, high-dimensional international real business cycle models with capital adjustment costs and irreversible investment; second, multiproduct menu-cost models with temporary sales and economies of scope in price setting.","2017-05-22","2024-07-28 19:56:53","2024-08-12 13:25:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\isido\Zotero\storage\4B5X7BE9\Brumm en Scheidegger - Using Adaptive Sparse Grids to Solve High-Dimensio.pdf","","♥♥; function approximation; sparse grids","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFSVXVMW","webpage","2024","","Francesco Orabona","","","","","https://francesco.orabona.com/","I am looking for 1-2 post-docs and 1-2 PhD students to work on practical and theoretical aspects of online learning, stochastic optimization, and training of LLMs. The ideal candidate has an exceptional mathematical background and is proficient in coding. If you are interested, send me an email with your CV. Please send me your transcript too if your are applying for a PhD position. Due to the volume of emails and my limited time, I might not answer to everyone: please do not take it personally, you were probably not a good match for my lab. I am currently an Associate Professor at KAUST in the Computer, Electrical and Mathematical Sciences and Engineering Division. Previously, I was at Boston University, Stony Brook University, Yahoo Research NY, the Toyota Technological Institute at Chicago, the University of Milan, the IDIAP Research Institute, and the University of Genoa. My current research interest is parameter-free machine learning. In particular I am interested in online learning, batch/stochastic optimization, and statistical learning theory.","2024-07-29","2024-07-29 11:53:00","2024-08-12 14:47:33","2024-07-29 11:53:00","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\97Z9CKNJ\francesco.orabona.com.html","","online learning; persons; stochastic optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P42YNHYZ","videoRecording","2024","BYUSupercomputing","Debugging and Optimizing Julia Code","","","","","https://www.youtube.com/watch?v=PhhIgy1Vozo","This video briefly covers how to write, debug, and optimize Julia code. More Julia resources: https://byuhpc.github.io/sci-comp-cou... VS Code Julia extension: https://www.julia-vscode.org/ Revise.jl: https://timholy.github.io/Revise.jl/s... Julia REPL: https://docs.julialang.org/en/v1/stdl... Debugger.jl: https://github.com/JuliaDebug/Debugge... Julia optimization: https://docs.julialang.org/en/v1/manu... https://viralinstruction.com/posts/op... Creating Julia packages: https://pkgdocs.julialang.org/v1/crea... Julia unit tests: https://docs.julialang.org/en/v1/stdl...","2024-07-15","2024-07-29 17:01:53","2024-08-06 19:04:49","2024-07-29 17:01:53","","","","","","","","","","","","","","","","","","","YouTube","","","","","","julia; ♥♥♥; programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQN8TPBY","preprint","2023","Jentzen, Arnulf; Kuckuck, Benno; von Wurstemberger, Philippe","Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory","","","","","http://arxiv.org/abs/2310.20360","This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.","2023-10-31","2024-07-30 15:36:54","2024-08-06 19:04:02","2024-07-30 15:36:54","","","","","","","Mathematical Introduction to Deep Learning","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.20360 [cs, math, stat]","Comment: 601 pages, 36 figures, 45 source codes","C:\Users\isido\Zotero\storage\5VQDHAYR\Jentzen e.a. - 2023 - Mathematical Introduction to Deep Learning Method.pdf","","deep learning; to read","","","","","","","","","","","","","","","","","","","","arXiv:2310.20360","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EEENZWY","preprint","2024","Adcock, Ben; Brugiapaglia, Simone; Dexter, Nick; Moraga, Sebastian","Learning smooth functions in high dimensions: from sparse polynomials to deep neural networks","","","","","http://arxiv.org/abs/2404.03761","Learning approximations to smooth target functions of many variables from finite sets of pointwise samples is an important task in scientific computing and its many applications in computational science and engineering. Despite well over half a century of research on highdimensional approximation, this remains a challenging problem. Yet, significant advances have been made in the last decade towards efficient methods for doing this, commencing with so-called sparse polynomial approximation methods and continuing most recently with methods based on Deep Neural Networks (DNNs). In tandem, there have been substantial advances in the relevant approximation theory and analysis of these techniques. In this work, we survey this recent progress. We describe the contemporary motivations for this problem, which stem from parametric models and computational uncertainty quantification; the relevant function classes, namely, classes of infinite-dimensional, Banach-valued, holomorphic functions; fundamental limits of learnability from finite data for these classes; and finally, sparse polynomial and DNN methods for efficiently learning such functions from finite data. For the latter, there is currently a significant gap between the approximation theory of DNNs and the practical performance of deep learning. Aiming to narrow this gap, we develop the topic of practical existence theory, which asserts the existence of dimension-independent DNN architectures and training strategies that achieve provably near-optimal generalization errors in terms of the amount of training data.","2024-04-04","2024-07-30 15:52:05","2024-08-06 19:00:33","2024-07-30 15:52:05","","","","","","","Learning smooth functions in high dimensions","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.03761 [cs, math]","","C:\Users\isido\Zotero\storage\9WW26YIH\Adcock e.a. - 2024 - Learning smooth functions in high dimensions from.pdf","","chebychev; deep learning; ♥♥♥; function approximation","","","","","","","","","","","","","","","","","","","","arXiv:2404.03761","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSC3S9FL","journalArticle","2019","Gopal, Abinand; Trefethen, Lloyd N.","Solving Laplace Problems with Corner Singularities via Rational Functions","SIAM Journal on Numerical Analysis","","0036-1429, 1095-7170","10.1137/19M125947X","https://epubs.siam.org/doi/10.1137/19M125947X","A new method is introduced for solving Laplace problems on two-dimensional regions with corners by approximation of boundary data by the real part of a rational function with ﬁxed poles exponentially clustered near each corner. Greatly extending a result of D. J. Newman in 1964 in approximation theory, we ﬁrst prove that such approximations can achieve root-exponential convergence for a wide range of problems, all the way up to the corner singularities. We then develop a numerical method to compute approximations via linear least-squares ﬁtting on the boundary. Typical problems are solved in < 1s on a desktop to 8-digit accuracy, with the accuracy guaranteed in the interior by the maximum principle. The computed solution is represented globally by a single formula, which can be evaluated in a few microseconds at each point.","2019-01","2024-07-31 20:27:28","2024-08-06 18:57:51","2024-07-31 20:27:28","2074-2094","","5","57","","SIAM J. Numer. Anal.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\8WS282DD\Gopal and Trefethen - 2019 - Solving Laplace Problems with Corner Singularities.pdf","","chebychev; ♥♥♥♥; rational functions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7HYWP77Z","videoRecording","2023","INI Seminar Room 1","Prof. Nick Trefethen | Computing with rational approximations","","","","","https://www.youtube.com/watch?v=7klWg7tc-pc","Speaker(s): Professor Nick Trefethen (University of Oxford) Date: 25 July 2023 - 09:00 to 10:00 Venue: INI Seminar Room 1 Session Title: Computing with rational approximations Event: [CATW04] Complex analysis: techniques, applications and computations - perspectives in 2023 Since the appearance of the AAA algorithm in 2018, we have for the first time what comes close to a black box for near-best rational approximation in the complex plane. The talk will explore applications.","2023-08-18","2024-07-31 21:37:20","2024-08-06 18:56:16","2024-07-31 21:37:20","","","","","","","","","","","","","","","","","","","YouTube","","","","","","♥♥♥♥♥","","","","","","","","","","","","","","","","","","","","","","59:25","","","","","","","","","","","","","","","","","","","","","","","","",""
"EIHL3TEA","preprint","2023","Nakatsukasa, Yuji; Sete, Olivier; Trefethen, Lloyd N.","The first five years of the AAA algorithm","","","","","http://arxiv.org/abs/2312.03565","The AAA algorithm, introduced in 2018, computes best or near-best rational approximations to functions or data on subsets of the real line or the complex plane. It is much faster and more robust than previous algorithms for such problems and has been used in many applications since its appearance, including the numerical solution of Laplace, Poisson, and biharmonic PDE problems in irregular domains. AAA has also been extended in new directions and seems likely to be a tool of lasting importance in the future.","2023-12-06","2024-07-31 21:43:48","2024-08-06 18:56:10","2024-07-31 21:43:48","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2312.03565 [cs, math]","","C:\Users\isido\Zotero\storage\FUQQCZC6\Nakatsukasa et al. - 2023 - The first five years of the AAA algorithm.pdf","","chebychev; ♥♥♥; rational functions","","","","","","","","","","","","","","","","","","","","arXiv:2312.03565","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7M4HQRY","journalArticle","2019","Gopal, Abinand; Trefethen, Lloyd N.","New Laplace and Helmholtz solvers","Proceedings of the National Academy of Sciences","","0027-8424, 1091-6490","10.1073/pnas.1904139116","https://pnas.org/doi/full/10.1073/pnas.1904139116","New numerical algorithms based on rational functions are introduced that can solve certain Laplace and Helmholtz problems on two-dimensional domains with corners faster and more accurately than the standard methods of ﬁnite elements and integral equations. The new algorithms point to a reconsideration of the assumptions underlying existing numerical analysis for partial diﬀerential equations.","2019-05-21","2024-07-31 21:59:29","2024-08-06 18:54:58","2024-07-31 21:59:29","10223-10225","","21","116","","Proc. Natl. Acad. Sci. U.S.A.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\isido\Zotero\storage\W927VEU4\Gopal and Trefethen - 2019 - New Laplace and Helmholtz solvers.pdf","","PDE; chebychev; ♥♥♥♥; rational functions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TW78MU4L","document","2022","Mickel, Annalena","Weak and Strong Approximation of the Log-Heston Model by Euler-Type Methods and Related Topics","","","","","","This thesis deals with the weak and strong numerical approximation of so-called stochastic volatility models. In particular, the focus is on the log-Heston model and its associated Euler methods, for which there have been only a few convergence results with a polynomial rate in the literature so far. The biggest challenge here is the approximation of the CIR process, which models the stochastic variance and whose diffusion coefficient is not Lipschitz continuous. We first study the weak order of convergence of two Euler methods that keep the approximation of the CIR process positive. When the Feller index ν of the CIR process is greater than one, weak convergence of order one is obtained as under standard assumptions. For ν ≤ 1 we obtain a weak order of convergence of ν − ε for ε > 0 arbitrarily small. For the L1-error for a large class of Euler methods, we can recover the order 1/2 obtained under standard assumptions under the condition ν > 1. Moreover, we prove that this is already the optimal L1-convergence order for the log-Heston model. Finally, in the last part of this dissertation we deal with the optimal L2 approximation of more general stochastic volatility models.","2022","2024-08-07 09:28:58","2024-08-12 14:33:56","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\isido\Zotero\storage\YA7GCAIU\Dissertation.pdf","","♥; SDE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""